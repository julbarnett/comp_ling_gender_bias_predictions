{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from re import sub, split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw = pd.read_csv('data/docs_before.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw[\"avg_ranking\"] = ratings_raw.iloc[:,6:10].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_cols = ['hp_id',\n",
    "               'ratemds_id',\n",
    "               'hasorder',\n",
    "               'order_id',\n",
    "               'avg_help', \n",
    "               'avg_know', \n",
    "               'avg_punct', \n",
    "               'avg_staff',\n",
    "               'avg_ranking',\n",
    "               'spec_comb',\n",
    "               'review_corpus'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings_raw[useful_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.drop_duplicates(subset='hp_id',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_info = pd.read_csv('data/genders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_info.drop_duplicates(subset='hp_id', keep='first',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_review = pd.merge(ratings, gender_info, on='hp_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_review['spec_comb'] = doc_review['spec_comb'].map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_review['sentence']= doc_review['review_corpus'].map(lambda x: x.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in doc_review['sentence'][0]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(doc_review['hp_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a datset where there is one row for review not for doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_review.reset_index(inplace= True,drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_reviews = doc_review[doc_review[\"spec_comb\"] == \"Internal Medicine\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new dataframe of doc's review, make sure each record has single review\n",
    "im_review_df = pd.DataFrame()\n",
    "for i, sentence in enumerate(im_reviews['sentence']):\n",
    "    temp_dict = dict(enumerate(sentence))\n",
    "    a = len(list(temp_dict.keys()))\n",
    "    s = str(im_reviews['hp_id'][i])\n",
    "    temp_df = pd.DataFrame.from_dict(data = temp_dict, orient = 'index', columns=['Review'])\n",
    "    temp_df['hp_id'] = [s for i in range(a)]\n",
    "    im_review_df = im_review_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_review_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df['hp_id'] = im_review_df['hp_id'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_review_df = pd.merge(im_review_df, doc_review, on='hp_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df = im_review_df.drop(['review_corpus', 'sentence', 'spec_comb'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df.gender.value_counts()/len(im_review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df.hasorder.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df.hasorder.value_counts()/len(im_review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cols_to_binary(val, col_name):\n",
    "    # col names can be \"gender\" or \"ranking\"\n",
    "    if col_name == \"gender\":\n",
    "        if val == \"F\":\n",
    "            return_var = 1\n",
    "        elif val == \"M\":\n",
    "            return_var = 0\n",
    "        else:\n",
    "            return_var = \"NA\"\n",
    "    elif col_name == \"ranking\":\n",
    "        if val >= 4.0:\n",
    "            return_var = 1\n",
    "        elif val < 4.0:\n",
    "            return_var = 0\n",
    "        else: \n",
    "            return_var = \"NA\"\n",
    "    return return_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df[\"gen_bin\"] = im_review_df[\"gender\"].apply(lambda x: cols_to_binary(x, \"gender\"))\n",
    "im_review_df[\"high_avg_rank\"] = im_review_df[\"avg_ranking\"].apply(lambda x: cols_to_binary(x, \"ranking\"))\n",
    "im_review_df[\"high_avg_help\"] = im_review_df[\"avg_help\"].apply(lambda x: cols_to_binary(x, \"ranking\"))\n",
    "im_review_df[\"high_avg_know\"] = im_review_df[\"avg_know\"].apply(lambda x: cols_to_binary(x, \"ranking\"))\n",
    "im_review_df[\"high_avg_punc\"] = im_review_df[\"avg_punct\"].apply(lambda x: cols_to_binary(x, \"ranking\"))\n",
    "im_review_df[\"high_avg_staf\"] = im_review_df[\"avg_staff\"].apply(lambda x: cols_to_binary(x, \"ranking\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle them\n",
    "df = im_review_df.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eighty_percent = len(df)*.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 80% for train // 20% for test\n",
    "df_train = df[0:eighty_percent]\n",
    "df_test = df[eighty_percent:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop = True)\n",
    "df_test = df_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split x data (literally just the review) from the other potential y variables (which also serve as metadata analysis features)\n",
    "\n",
    "x_train = df_train[\"Review\"]\n",
    "y_train_gen = df_train[\"gen_bin\"]\n",
    "y_train_rank = df_train[\"high_avg_rank\"]\n",
    "y_train_help = df_train[\"high_avg_help\"]\n",
    "y_train_know = df_train[\"high_avg_know\"]\n",
    "y_train_punc = df_train[\"high_avg_punc\"]\n",
    "\n",
    "x_test = df_test[\"Review\"]\n",
    "y_test_gen = df_test[\"gen_bin\"]\n",
    "y_test_rank = df_test[\"high_avg_rank\"]\n",
    "y_test_help = df_test[\"high_avg_help\"]\n",
    "y_test_know = df_test[\"high_avg_know\"]\n",
    "y_test_punc = df_test[\"high_avg_punc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(s):\n",
    "    \n",
    "    # unwanted symbols/punctuations\n",
    "    s = sub(\"%\", \" percent\", s) ##percents\n",
    "    s = sub(\"&amp;\", \"and\", s) ##ampersands\n",
    "    s = sub(\"'s\", \"\", s) ##possessive or contraction\n",
    "    s = sub(\"'re\", \"\", s) ##contraction\n",
    "    s = sub(\"'ll\", \"\", s) ##contraction\n",
    "    s = sub(\"‚Äô\", \"\", s) ##contraction\n",
    "    s = sub(\"'t\", \" not\", s) ##contraction\n",
    "    \n",
    "    # typical text mining things\n",
    "    s = s.lower() ##case sensitivity\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.apply(lambda x: clean_word(x))\n",
    "x_test = x_test.apply(lambda x: clean_word(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most basic model: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize text reviews to numbers\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "x_train_vec = vec.fit_transform(x_train).toarray()\n",
    "x_test_vec = vec.transform(x_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(x_train_vec, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb.score(x_train_vec, y_train_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb.score(x_test_vec, y_test_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = model_nb.predict(x_test_vec)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(pred_val, actual_val):\n",
    "    if pred_val == 1:\n",
    "        if actual_val == 1:\n",
    "            return \"TP\"\n",
    "        elif actual_val == 0:\n",
    "            return \"FP\"\n",
    "    elif pred_val == 0:\n",
    "        if actual_val == 1:\n",
    "            return \"FN\"\n",
    "        elif actual_val == 0:\n",
    "            return \"TN\"\n",
    "    else:\n",
    "        return \"ERROR IN DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conf_df(model, x_df, y_df):\n",
    "    compared_df = pd.DataFrame()\n",
    "    compared_df[\"preds\"] = model.predict(x_df)\n",
    "    compared_df[\"actuals\"] = y_df\n",
    "    compared_df[\"error_type\"] = compared_df.apply(lambda x: conf_matrix(x.preds, x.actuals), axis=1)\n",
    "    compared_df[\"count\"] = 1\n",
    "    return compared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_cost_matrix(model, x_df, y_df):\n",
    "    conf_df = create_conf_df(model, x_df, y_df)\n",
    "    summary_df = conf_df[[\"error_type\", \"count\"]].groupby(by=[\"error_type\"]).sum()\n",
    "    fn = summary_df.iloc[0][0]\n",
    "    fp = summary_df.iloc[1][0]\n",
    "    tn = summary_df.iloc[2][0]\n",
    "    tp = summary_df.iloc[3][0]\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = (2*precision*recall)/(precision+recall)\n",
    "    F0_5score = ((1 + 0.5**2) * precision * recall) / (0.5**2 * precision + recall)\n",
    "    print(summary_df)\n",
    "    print(\"FN:\", fn)\n",
    "    print(\"FP:\", fp)\n",
    "    print(\"TN:\", tn)\n",
    "    print(\"TP:\", tp)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1_score)\n",
    "    print(\"F0.5:\", F0_5score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cost_matrix(model_nb, x_train_vec, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cost_matrix(model_nb, x_test_vec, y_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    " \n",
    "# n_estimators = # of trees\n",
    "rf_model_no_extra = RandomForestClassifier(n_estimators = 501,\n",
    "                                           criterion = 'entropy')\n",
    "                             \n",
    "rf_model_no_extra.fit(x_train_vec, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_model_no_extra.score(x_train_vec, y_train_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_no_extra.score(x_test_vec, y_test_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred_rf_no_extra = rf_model_no_extra.predict(x_test_vec)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cost_matrix(rf_model_no_extra, x_train_vec, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cost_matrix(rf_model_no_extra, x_test_vec, y_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding lemmatization, stemming etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_tokenize(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def lemm_sentence(tokenized_sentence):\n",
    "    new_tokenized_sentence = []\n",
    "    for word in tokenized_sentence:\n",
    "        new_tokenized_sentence.append(lemm.lemmatize(word))\n",
    "    return new_tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_lemmatized = x_train_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "# x_test_lemmatized = x_test_tokenized.apply(lambda x: lemm_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "ps = PorterStemmer()\n",
    " \n",
    "def stem_sentence(lemmatized_sentence):\n",
    "    new_lemmatized_sentence = []\n",
    "    for word in lemmatized_sentence:\n",
    "        new_lemmatized_sentence.append(ps.stem(word))\n",
    "    return new_lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_stemmed = x_train_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "# x_test_stemmed = x_test_lemmatized.apply(lambda x: stem_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_sentence_again(stem_sentence):\n",
    "    new_norm_sentence = \"\"\n",
    "    for word in stem_sentence:\n",
    "        new_norm_sentence = new_norm_sentence + \" \" + word\n",
    "    return new_norm_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_to_vec = x_train_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "# x_test_to_vec = x_test_stemmed.apply(lambda x: norm_sentence_again(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigram_strings(string_sentence):\n",
    "    bigram_list = []\n",
    "    for word in range(0, len(string_sentence.split())-1):\n",
    "        unigram_1 = string_sentence.split()[word]\n",
    "        unigram_2 = string_sentence.split()[word+1]\n",
    "        bigram_list.append(unigram_1 + \"_\" + unigram_2)\n",
    "    bigram_sentence = \"\"\n",
    "    for bigram in bigram_list:\n",
    "        bigram_sentence = bigram_sentence + \" \" + bigram\n",
    "    return bigram_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ratings_to_vectors(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # clean them\n",
    "    inner_x_train = x_train.apply(lambda x: clean_word(x))\n",
    "    inner_x_test = x_test.apply(lambda x: clean_word(x))\n",
    "\n",
    "    # tokenize\n",
    "    inner_x_train_tokenized = inner_x_train.apply(lambda x: word_tokenize(x))\n",
    "    inner_x_test_tokenized = inner_x_test.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "    # lemmatize\n",
    "    lemm = WordNetLemmatizer()\n",
    "    inner_x_train_lemmatized = inner_x_train_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    inner_x_test_lemmatized = inner_x_test_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    \n",
    "    # stem\n",
    "    ps = PorterStemmer()\n",
    "    inner_x_train_stemmed = inner_x_train_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    inner_x_test_stemmed = inner_x_test_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    \n",
    "    # return to non-vectorized-list so we can manipulate\n",
    "    inner_x_train_to_vec = inner_x_train_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "    inner_x_test_to_vec = inner_x_test_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "    \n",
    "    # now count vectorize\n",
    "    cv = CountVectorizer()\n",
    "    inner_x_train_vec = cv.fit_transform(pd.Series(inner_x_train_to_vec)).toarray()\n",
    "    inner_x_test_vec = cv.transform(pd.Series(inner_x_test_to_vec)).toarray()\n",
    "\n",
    "    return inner_x_train_vec, y_train, inner_x_test_vec, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ratings_to_vectors_bigram(x_train, y_train, x_test, y_test):\n",
    "    # clean them\n",
    "    inner_x_train = x_train.apply(lambda x: clean_word(x))\n",
    "    inner_x_test = x_test.apply(lambda x: clean_word(x))\n",
    "\n",
    "    # tokenize\n",
    "    inner_x_train_tokenized = inner_x_train.apply(lambda x: word_tokenize(x))\n",
    "    inner_x_test_tokenized = inner_x_test.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "    # lemmatize\n",
    "    lemm = WordNetLemmatizer()\n",
    "    inner_x_train_lemmatized = inner_x_train_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    inner_x_test_lemmatized = inner_x_test_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    \n",
    "    # stem\n",
    "    ps = PorterStemmer()\n",
    "    inner_x_train_stemmed = inner_x_train_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    inner_x_test_stemmed = inner_x_test_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    \n",
    "    # return to non-vectorized-list so we can manipulate\n",
    "    inner_x_train_to_vec = inner_x_train_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "    inner_x_test_to_vec = inner_x_test_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "    \n",
    "    # convert this guy to a bigrammized sentence\n",
    "    bigrammed_sentence_x_train = inner_x_train_to_vec.apply(lambda x: create_bigram_strings(x))\n",
    "    bigrammed_sentence_x_test  = inner_x_test_to_vec.apply(lambda x: create_bigram_strings(x))\n",
    "    \n",
    "    # make them one big sentence:\n",
    "    comb_inner_x_train_to_vec = inner_x_train_to_vec + bigrammed_sentence_x_train\n",
    "    comb_inner_x_test_to_vec = inner_x_test_to_vec + bigrammed_sentence_x_test\n",
    "    \n",
    "    # now count vectorize\n",
    "    cv = CountVectorizer()\n",
    "    inner_x_train_vec = cv.fit_transform(pd.Series(comb_inner_x_train_to_vec)).toarray()\n",
    "    inner_x_test_vec = cv.transform(pd.Series(comb_inner_x_test_to_vec)).toarray()\n",
    "\n",
    "    return inner_x_train_vec, y_train, inner_x_test_vec, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bi_uni_df = train_ratings_to_vectors_bigram(x_train, y_train_gen, x_test, y_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df = train_ratings_to_vectors(x_train, y_train_gen, x_test, y_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_eval_metrics(trained_model, x_train, y_train, x_test, y_test):\n",
    "   \n",
    "    # vectorized usable versions only of x train\n",
    "    \n",
    "    train_fit = trained_model.score(x_train, y_train)\n",
    "    print(\"Train fit: \", train_fit)\n",
    "    test_fit = trained_model.score(x_test, y_test)\n",
    "    print(\"Test fit: \", test_fit)\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    y_predictions = trained_model.predict(x_test)\n",
    "\n",
    "    four_outputs = prec_reca_f1(y_test, y_predictions)\n",
    "    print(\"Precision: \", four_outputs[0])\n",
    "    print(\"Recall: \", four_outputs[1])\n",
    "    print(\"F1: \", four_outputs[2])\n",
    "    print(\"F0.5: \", four_outputs[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix in the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to redo the tokenization because we want it to be in a different format to compute cosine similarity, we will not use these after we have the dictionary cosine similarity vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def lemm_sentence(tokenized_sentence):\n",
    "    new_tokenized_sentence = []\n",
    "    for word in tokenized_sentence:\n",
    "        new_tokenized_sentence.append(lemm.lemmatize(word))\n",
    "    return new_tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "ps = PorterStemmer()\n",
    " \n",
    "def stem_sentence(lemmatized_sentence):\n",
    "    new_lemmatized_sentence = []\n",
    "    for word in lemmatized_sentence:\n",
    "        new_lemmatized_sentence.append(ps.stem(word))\n",
    "    return new_lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_sentence_again(stem_sentence):\n",
    "    new_norm_sentence = \"\"\n",
    "    for word in stem_sentence:\n",
    "        new_norm_sentence = new_norm_sentence + \" \" + word\n",
    "    return new_norm_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ratings_to_vectors_STEMMED(x_train, y_train, x_test, y_test):\n",
    "    # clean them\n",
    "    inner_x_train = x_train.apply(lambda x: clean_word(x))\n",
    "    inner_x_test = x_test.apply(lambda x: clean_word(x))\n",
    "\n",
    "    # tokenize\n",
    "    inner_x_train_tokenized = inner_x_train.apply(lambda x: word_tokenize(x))\n",
    "    inner_x_test_tokenized = inner_x_test.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "    # lemmatize\n",
    "    lemm = WordNetLemmatizer()\n",
    "    inner_x_train_lemmatized = inner_x_train_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    inner_x_test_lemmatized = inner_x_test_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    \n",
    "    # stem\n",
    "    ps = PorterStemmer()\n",
    "    inner_x_train_stemmed = inner_x_train_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    inner_x_test_stemmed = inner_x_test_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    \n",
    "#     # return to non-vectorized-list so we can manipulate\n",
    "#     inner_x_train_to_vec = inner_x_train_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "#     inner_x_test_to_vec = inner_x_test_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "    \n",
    "#     # now count vectorize\n",
    "#     cv = CountVectorizer(max_features = 1500)\n",
    "#     inner_x_train_vec = cv.fit_transform(pd.Series(inner_x_train_to_vec)).toarray()\n",
    "#     inner_x_test_vec = cv.transform(pd.Series(inner_x_test_to_vec)).toarray()\n",
    "\n",
    "#    return inner_x_train_vec, inner_y_train, inner_x_test_vec, inner_y_test\n",
    "    return inner_x_train_stemmed, y_train, inner_x_test_stemmed, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df_for_str = train_ratings_to_vectors_STEMMED(x_train, y_train_gen, x_test, y_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_to_string(tokenized_sent):\n",
    "    new_sent = \"\"\n",
    "    for word in tokenized_sent:\n",
    "        new_sent = new_sent + \" \" + word\n",
    "    return new_sent[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df_str = (standard_df_for_str[0].apply(tokenized_to_string),\n",
    "                   standard_df_for_str[1],\n",
    "                   standard_df_for_str[2].apply(tokenized_to_string),\n",
    "                   standard_df_for_str[3],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make covariate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_dict_pd = pd.read_excel('data/LIWC2007dictionary_cleaned.xls', sheet_name = \"Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_dict_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mass_dict_pd.columns:\n",
    "    mass_dict[col]  = []\n",
    "    for word in range(0, len(mass_dict_pd[col])):\n",
    "        if pd.notna(mass_dict_pd[col][word]):\n",
    "            clean_word = mass_dict_pd[col][word].replace(\"*\", \"\")\n",
    "            mass_dict[col].append(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Semantic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_as_str = defaultdict(str) \n",
    "dicts_as_nlp = defaultdict(lambda: 'initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_key in mass_dict:\n",
    "    for item in mass_dict[col_key]:\n",
    "        dicts_as_str[col_key] = dicts_as_str[col_key] + item + \" \" \n",
    "    dicts_as_nlp[col_key] = nlp(dicts_as_str[col_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts_as_str = the dictionary as a concatenated string\n",
    "# dicts_as_nlp = the nlp(objects) of these concatenated string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_as_nlp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Cosine Similarity Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_cosine_similarity(string_sentence, dict_col, dict_to_compare = dicts_as_nlp):\n",
    "    ind_dict = dict_to_compare[dict_col]\n",
    "    main_doc = nlp(string_sentence.lower())\n",
    "    \n",
    "    all_scores = set()\n",
    "    \n",
    "    for nlp_word in dicts_as_nlp[dict_col]:\n",
    "        all_scores.add(main_doc.similarity(nlp_word))\n",
    "    \n",
    "    return st.median(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_cols_sem_we(string_sentence, dict_to_compare = dicts_as_nlp):\n",
    "    # must be a string sentence! not a tokenized one\n",
    "    # works best with a concatenated string of tokenized words\n",
    "    new_meta_cols = ['Quant', \n",
    "                     'Numbers', \n",
    "                     'Humans', \n",
    "                     'Affect', \n",
    "                     'Posemo', \n",
    "                     'Negemo', \n",
    "                     'Cause', \n",
    "                     'Health', \n",
    "                     'Money', \n",
    "                     'Death', \n",
    "                     'Ipron', \n",
    "                     'aux_verb', \n",
    "                     'adverbs', \n",
    "                     'Negate', \n",
    "                     'Family', \n",
    "                     'Anx', \n",
    "                     'Anger', \n",
    "                     'Sad', \n",
    "                     'CogMech', \n",
    "                     'Insight', \n",
    "                     'Discrep', \n",
    "                     'Tentat', \n",
    "                     'Certain', \n",
    "                     'Inhib', \n",
    "                     'Incl', \n",
    "                     'Excl', \n",
    "                     'Bio', \n",
    "                     'Body', \n",
    "                     'Sexual', \n",
    "                     'Time', \n",
    "                     'Achiev']\n",
    "    blank_row_to_fill = pd.DataFrame(columns=new_meta_cols)\n",
    "    blank_row_to_fill.loc[0] = 0\n",
    "    \n",
    "    blank_row_to_fill['Length'] = len(string_sentence.split())\n",
    "    \n",
    "    string_doc1 = string_sentence.lower()\n",
    "    \n",
    "    for dict_key in dict_to_compare:\n",
    "        score = median_cosine_similarity(string_doc1, dict_key)        \n",
    "        blank_row_to_fill[dict_key] = score\n",
    "\n",
    "    return blank_row_to_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_dfs_sem_we(train_df_tokenized_strings, test_df_tokenized_strings):\n",
    "    # must be tokenized string rows\n",
    "    new_meta_cols = ['Quant', \n",
    "                     'Numbers', \n",
    "                     'Humans', \n",
    "                     'Affect', \n",
    "                     'Posemo', \n",
    "                     'Negemo', \n",
    "                     'Cause', \n",
    "                     'Health', \n",
    "                     'Money', \n",
    "                     'Death', \n",
    "                     'Ipron', \n",
    "                     'aux_verb', \n",
    "                     'adverbs', \n",
    "                     'Negate', \n",
    "                     'Family', \n",
    "                     'Anx', \n",
    "                     'Anger', \n",
    "                     'Sad', \n",
    "                     'CogMech', \n",
    "                     'Insight', \n",
    "                     'Discrep', \n",
    "                     'Tentat', \n",
    "                     'Certain', \n",
    "                     'Inhib', \n",
    "                     'Incl', \n",
    "                     'Excl', \n",
    "                     'Bio', \n",
    "                     'Body', \n",
    "                     'Sexual', \n",
    "                     'Time', \n",
    "                     'Achiev']\n",
    "    \n",
    "    meta_train = pd.DataFrame(columns=new_meta_cols)\n",
    "    meta_test = pd.DataFrame(columns=new_meta_cols)\n",
    "\n",
    "    for row in train_df_tokenized_strings:\n",
    "        meta_train = meta_train.append(create_meta_cols_sem_we(row)).reset_index(drop = True)\n",
    "\n",
    "    for row in test_df_tokenized_strings:\n",
    "        meta_test = meta_test.append(create_meta_cols_sem_we(row)).reset_index(drop = True)\n",
    "        \n",
    "    return meta_train, meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_meta_data, x_test_meta_data = create_meta_dfs_sem_we(standard_df_str[0], standard_df_str[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab proper ngram data\n",
    "x_train_bi_uni = bi_uni_df[0]\n",
    "y_train_bi_uni = bi_uni_df[1]\n",
    "x_test_bi_uni = bi_uni_df[2]\n",
    "y_test_bi_uni = bi_uni_df[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab proper ngram data\n",
    "x_train_uni = standard_df[0]\n",
    "y_train_uni = standard_df[1]\n",
    "x_test_uni = standard_df[2]\n",
    "y_test_uni = standard_df[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_unibi_train = pd.concat([x_train_meta_data, pd.DataFrame(x_train_bi_uni)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_unibi_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_unibi_test = pd.concat([x_test_meta_data, pd.DataFrame(x_test_bi_uni)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_unibi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_uni_train = pd.concat([x_train_meta_data, pd.DataFrame(x_train_uni)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_uni_test = pd.concat([x_test_meta_data, pd.DataFrame(x_test_uni)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logreg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegressionCV(cv = 5, max_iter = 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_train_scaled = min_max_scaler.fit_transform(meta_plus_unibi_train)\n",
    "df_test_scaled = min_max_scaler.fit_transform(meta_plus_unibi_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(df_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eval_metrics(logreg, df_train_scaled, y_train, df_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models on combined set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "rf_model = RandomForestClassifier(criterion = 'entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold Cross validation\n",
    "np.mean(cross_val_score(rf_model, meta_plus_unibi_train, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_param_grid = {\n",
    "                    'bootstrap': [True],\n",
    "                    'criterion': ['entropy'],\n",
    "                    'max_depth': [20, 25],\n",
    "                    'max_features': ['sqrt'],\n",
    "                    'min_samples_leaf': [1, 2, 3],\n",
    "                    'min_samples_split': [3, 5],\n",
    "                    'n_estimators': [1000]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_model = GridSearchCV(rf_model, intro_param_grid, cv=5)\n",
    "grid_rf_model.fit(meta_plus_unibi_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_unibi_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_rf_model.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_rf_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_eval_metrics(grid_rf_model, meta_plus_unibi_train, y_train, meta_plus_unibi_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fone_scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "fone_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_f1 = RandomForestClassifier(criterion = 'entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "                 'n_estimators': [1000, 1100],\n",
    "                 'max_depth': [15, 20, 25],\n",
    "                 'criterion': [\"entropy\"],\n",
    "                 'max_features': ['auto', 'sqrt'],\n",
    "                 'min_samples_split': [2, 3, 4],\n",
    "                 'min_samples_leaf': [1, 2],\n",
    "                 'bootstrap': [True, False]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_model_f1 = GridSearchCV(rf_model_f1, param_grid1, cv=5, scoring=fone_scorer)\n",
    "grid_rf_model_f1.fit(meta_plus_uni_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf_model_f1.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf_model_f1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_eval_metrics(grid_rf_model_f1, meta_plus_uni_train, y_train, meta_plus_uni_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = grid_rf_model_f1.predict(meta_plus_uni_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf_model_f1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'bootstrap': [False],\n",
    " 'criterion': ['entropy'],\n",
    " 'max_depth': [25],\n",
    " 'max_features': ['sqrt'],\n",
    " 'min_samples_leaf': [1],\n",
    " 'min_samples_split': [2],\n",
    " 'n_estimators': [1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_best = RandomForestClassifier(criterion = 'entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_model_best = GridSearchCV(rf_model_best, best_params, cv=5, scoring=fone_scorer)\n",
    "grid_rf_model_best.fit(meta_plus_uni_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_uni_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eval_metrics(grid_rf_model_best, meta_plus_uni_train, y_train, meta_plus_uni_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_f1_bi = RandomForestClassifier(criterion = 'entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "                 'n_estimators': [1000, 1100],\n",
    "                 'max_depth': [15, 20, 25],\n",
    "                 'criterion': [\"entropy\"],\n",
    "                 'max_features': ['auto', 'sqrt'],\n",
    "                 'min_samples_split': [2, 3, 4],\n",
    "                 'min_samples_leaf': [1, 2],\n",
    "                 'bootstrap': [True, False]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_model_f1_bi = GridSearchCV(rf_model_f1_bi, param_grid1, cv=5, scoring=fone_scorer)\n",
    "grid_rf_model_f1_bi.fit(meta_plus_unibi_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf_model_f1_bi.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf_model_f1_bi.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_eval_metrics(grid_rf_model_f1_bi, meta_plus_unibi_train, y_train, meta_plus_unibi_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = grid_rf_model_f1_bi.predict(meta_plus_unibi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_bi = {'bootstrap': [False],\n",
    " 'criterion': ['entropy'],\n",
    " 'max_depth': [25],\n",
    " 'max_features': ['sqrt'],\n",
    " 'min_samples_leaf': [1],\n",
    " 'min_samples_split': [2],\n",
    " 'n_estimators': [1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_best_bi = RandomForestClassifier(criterion = 'entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_model_best_bi = GridSearchCV(rf_model_best_bi, best_params_bi, cv=5, scoring=fone_scorer)\n",
    "grid_rf_model_best_bi.fit(meta_plus_unibi_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_eval_metrics(grid_rf_model_best, meta_plus_unibi_train, y_train, meta_plus_unibi_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegressionCV(cv = 5, max_iter = 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_train_scaled = min_max_scaler.fit_transform(meta_plus_unibi_train)\n",
    "df_test_scaled = min_max_scaler.fit_transform(meta_plus_unibi_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_logr = {    \n",
    "                     'fit_intercept': [True, False],\n",
    "                     'max_iter': [50000],\n",
    "                     'penalty': ['l2'],\n",
    "                     'refit': [True, False                     \n",
    "                     'scoring': [fone_scorer],\n",
    "                     'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rf_model_regr = GridSearchCV(logreg, param_grid_logr, cv=5, scoring=fone_scorer)\n",
    "grid_rf_model_regr.fit(df_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eval_metrics(grid_rf_model_regr, df_train_scaled, y_train, df_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(pred_val, actual_val):\n",
    "    if pred_val == 1:\n",
    "        if actual_val == 1:\n",
    "            return \"TP\"\n",
    "        elif actual_val == 0:\n",
    "            return \"FP\"\n",
    "    elif pred_val == 0:\n",
    "        if actual_val == 1:\n",
    "            return \"FN\"\n",
    "        elif actual_val == 0:\n",
    "            return \"TN\"\n",
    "    else:\n",
    "        return \"ERROR IN DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conf_df(model, x_df, y_df):\n",
    "    compared_df = pd.DataFrame()\n",
    "    compared_df[\"preds\"] = model.predict(x_df)\n",
    "    compared_df[\"actuals\"] = y_df\n",
    "    compared_df[\"error_type\"] = compared_df.apply(lambda x: conf_matrix(x.preds, x.actuals), axis=1)\n",
    "    compared_df[\"count\"] = 1\n",
    "    return compared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_cost_matrix(model, x_df, y_df):\n",
    "    conf_df = create_conf_df(model, x_df, y_df)\n",
    "    summary_df = conf_df[[\"error_type\", \"count\"]].groupby(by=[\"error_type\"]).sum()\n",
    "    fn = summary_df.iloc[0][0]\n",
    "    fp = summary_df.iloc[1][0]\n",
    "    tn = summary_df.iloc[2][0]\n",
    "    tp = summary_df.iloc[3][0]\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = (2*precision*recall)/(precision+recall)\n",
    "    F0_5score = ((1 + 0.5**2) * precision * recall) / (0.5**2 * precision + recall)\n",
    "    print(summary_df)\n",
    "    print(\"FN:\", fn)\n",
    "    print(\"FP:\", fp)\n",
    "    print(\"TN:\", tn)\n",
    "    print(\"TP:\", tp)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1_score)\n",
    "    print(\"F0.5:\", F0_5score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_check(model, x_train, y_train, x_test, y_test):\n",
    "    train_fit = model.score(x_train, y_train)\n",
    "    print(\"Train fit: \", train_fit)\n",
    "    test_fit = model.score(x_test, y_test)\n",
    "    print(\"Test fit: \", test_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_check(grid_rf_model_regr, df_train_scaled, y_train, df_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cost_matrix(grid_rf_model_regr, df_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
