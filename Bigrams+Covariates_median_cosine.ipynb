{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from re import sub, split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw = pd.read_csv('data/docs_before.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hp_id</th>\n",
       "      <th>ratemds_id</th>\n",
       "      <th>order_id</th>\n",
       "      <th>hasorder</th>\n",
       "      <th>review_count</th>\n",
       "      <th>avg_help</th>\n",
       "      <th>avg_know</th>\n",
       "      <th>avg_punct</th>\n",
       "      <th>avg_staff</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_nonprej</th>\n",
       "      <th>cat_otherprej</th>\n",
       "      <th>cat_restrictlicense</th>\n",
       "      <th>cat_losslicense</th>\n",
       "      <th>cem_strata_alldocs</th>\n",
       "      <th>basecat_badmed</th>\n",
       "      <th>basecat_impaired</th>\n",
       "      <th>basecat_finances</th>\n",
       "      <th>basecat_criminal</th>\n",
       "      <th>basecat_patient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>303282</td>\n",
       "      <td>24616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>427422</td>\n",
       "      <td>66138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3843</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1250536</td>\n",
       "      <td>3065893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6945</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1269847</td>\n",
       "      <td>284377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2072</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    hp_id  ratemds_id  order_id  hasorder  review_count  \\\n",
       "0           1   303282       24616       NaN         0             3   \n",
       "1           2   427422       66138       NaN         0             3   \n",
       "2           3    19713      127113       NaN         0            11   \n",
       "3           4  1250536     3065893       NaN         0             1   \n",
       "4           5  1269847      284377       NaN         0             1   \n",
       "\n",
       "   avg_help  avg_know  avg_punct  avg_staff  ...  cat_nonprej  cat_otherprej  \\\n",
       "0  4.333333  4.333333   4.333333   2.666667  ...          NaN            NaN   \n",
       "1  4.666667  5.000000   4.666667   3.333333  ...          NaN            NaN   \n",
       "2  2.272727  2.636364   3.000000   2.000000  ...          NaN            NaN   \n",
       "3  5.000000  5.000000   5.000000   5.000000  ...          NaN            NaN   \n",
       "4  1.000000  2.000000   1.000000   1.000000  ...          NaN            NaN   \n",
       "\n",
       "   cat_restrictlicense  cat_losslicense  cem_strata_alldocs  basecat_badmed  \\\n",
       "0                  NaN              NaN                7040               0   \n",
       "1                  NaN              NaN                3843               0   \n",
       "2                  NaN              NaN                2310               0   \n",
       "3                  NaN              NaN                6945               0   \n",
       "4                  NaN              NaN                2072               0   \n",
       "\n",
       "   basecat_impaired  basecat_finances  basecat_criminal  basecat_patient  \n",
       "0                 0                 0                 0                0  \n",
       "1                 0                 0                 0                0  \n",
       "2                 0                 0                 0                0  \n",
       "3                 0                 0                 0                0  \n",
       "4                 0                 0                 0                0  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw[\"avg_ranking\"] = ratings_raw.iloc[:,6:10].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'hp_id', 'ratemds_id', 'order_id', 'hasorder',\n",
       "       'review_count', 'avg_help', 'avg_know', 'avg_punct', 'avg_staff',\n",
       "       'var_help', 'var_know', 'var_punct', 'var_staff', 'rem_neg_DONOTUSE',\n",
       "       'rem_pos_DONOTUSE', 'ones', 'twos', 'threes', 'fours', 'fives',\n",
       "       'prop_ones', 'prop_twos', 'prop_threes', 'prop_fours', 'prop_fives',\n",
       "       'review_corpus', 'spec_comb', 'gy_comb', 'HP_ST', 'firstorderdate',\n",
       "       'action_desc_concat', 'action_comment_concat', 'orderbasesconcat',\n",
       "       'cat_nonprej', 'cat_otherprej', 'cat_restrictlicense',\n",
       "       'cat_losslicense', 'cem_strata_alldocs', 'basecat_badmed',\n",
       "       'basecat_impaired', 'basecat_finances', 'basecat_criminal',\n",
       "       'basecat_patient', 'avg_ranking'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_cols = ['hp_id',\n",
    "               'ratemds_id',\n",
    "               'hasorder',\n",
    "               'order_id',\n",
    "               'avg_help', \n",
    "               'avg_know', \n",
    "               'avg_punct', \n",
    "               'avg_staff',\n",
    "               'avg_ranking',\n",
    "               'spec_comb',\n",
    "               'review_corpus'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings_raw[useful_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings.drop_duplicates(subset='hp_id',keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp_id</th>\n",
       "      <th>ratemds_id</th>\n",
       "      <th>hasorder</th>\n",
       "      <th>order_id</th>\n",
       "      <th>avg_help</th>\n",
       "      <th>avg_know</th>\n",
       "      <th>avg_punct</th>\n",
       "      <th>avg_staff</th>\n",
       "      <th>avg_ranking</th>\n",
       "      <th>spec_comb</th>\n",
       "      <th>review_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303282</td>\n",
       "      <td>24616</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>Very clean offices, friendly and informative s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427422</td>\n",
       "      <td>66138</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>If you are lucky enough to get in to see Dr. B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>Internal Medicine</td>\n",
       "      <td>satisfactory but would not recommend coldness,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1250536</td>\n",
       "      <td>3065893</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>The best physician I've been to in my 70 years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1269847</td>\n",
       "      <td>284377</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>I found Dr. Bernstein to be cold and uncaring....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hp_id  ratemds_id  hasorder  order_id  avg_help  avg_know  avg_punct  \\\n",
       "0   303282       24616         0       NaN  4.333333  4.333333   4.333333   \n",
       "1   427422       66138         0       NaN  4.666667  5.000000   4.666667   \n",
       "2    19713      127113         0       NaN  2.272727  2.636364   3.000000   \n",
       "3  1250536     3065893         0       NaN  5.000000  5.000000   5.000000   \n",
       "4  1269847      284377         0       NaN  1.000000  2.000000   1.000000   \n",
       "\n",
       "   avg_staff  avg_ranking                             spec_comb  \\\n",
       "0   2.666667     3.916667  Family Practice                        \n",
       "1   3.333333     4.416667  Family Practice                        \n",
       "2   2.000000     2.477273  Internal Medicine                      \n",
       "3   5.000000     5.000000  Family Practice                        \n",
       "4   1.000000     1.250000  Surgery                                \n",
       "\n",
       "                                       review_corpus  \n",
       "0  Very clean offices, friendly and informative s...  \n",
       "1  If you are lucky enough to get in to see Dr. B...  \n",
       "2  satisfactory but would not recommend coldness,...  \n",
       "3  The best physician I've been to in my 70 years...  \n",
       "4  I found Dr. Bernstein to be cold and uncaring....  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp_id</th>\n",
       "      <th>ratemds_id</th>\n",
       "      <th>hasorder</th>\n",
       "      <th>order_id</th>\n",
       "      <th>avg_help</th>\n",
       "      <th>avg_know</th>\n",
       "      <th>avg_punct</th>\n",
       "      <th>avg_staff</th>\n",
       "      <th>avg_ranking</th>\n",
       "      <th>spec_comb</th>\n",
       "      <th>review_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303282</td>\n",
       "      <td>24616</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>Very clean offices, friendly and informative s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427422</td>\n",
       "      <td>66138</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>If you are lucky enough to get in to see Dr. B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>Internal Medicine</td>\n",
       "      <td>satisfactory but would not recommend coldness,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1250536</td>\n",
       "      <td>3065893</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>The best physician I've been to in my 70 years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1269847</td>\n",
       "      <td>284377</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>I found Dr. Bernstein to be cold and uncaring....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134968</th>\n",
       "      <td>1165896</td>\n",
       "      <td>2474959</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>Very nice but not genuine-seeming. Didn't have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134969</th>\n",
       "      <td>1049044</td>\n",
       "      <td>3205133</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>Internal Medicine</td>\n",
       "      <td>dr. Foster has been excellent, she has hapled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134970</th>\n",
       "      <td>1132315</td>\n",
       "      <td>2880773</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>Internal Medicine</td>\n",
       "      <td>Dr. Davis is a very caring physician. She is n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134971</th>\n",
       "      <td>1308120</td>\n",
       "      <td>152018</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Pediatrics</td>\n",
       "      <td>Two years ago, I was diagnosed with ulcerative...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134972</th>\n",
       "      <td>1321949</td>\n",
       "      <td>548661</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>Obstetrics and Gynecology</td>\n",
       "      <td>Dr. Lyon is a very through doctor who takes a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134968 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          hp_id  ratemds_id  hasorder  order_id  avg_help  avg_know  \\\n",
       "0        303282       24616         0       NaN  4.333333  4.333333   \n",
       "1        427422       66138         0       NaN  4.666667  5.000000   \n",
       "2         19713      127113         0       NaN  2.272727  2.636364   \n",
       "3       1250536     3065893         0       NaN  5.000000  5.000000   \n",
       "4       1269847      284377         0       NaN  1.000000  2.000000   \n",
       "...         ...         ...       ...       ...       ...       ...   \n",
       "134968  1165896     2474959         0       NaN  2.000000  1.000000   \n",
       "134969  1049044     3205133         0       NaN  2.666667  2.666667   \n",
       "134970  1132315     2880773         0       NaN  3.666667  4.333333   \n",
       "134971  1308120      152018         0       NaN  5.000000  5.000000   \n",
       "134972  1321949      548661         0       NaN  2.250000  2.500000   \n",
       "\n",
       "        avg_punct  avg_staff  avg_ranking  \\\n",
       "0        4.333333   2.666667     3.916667   \n",
       "1        4.666667   3.333333     4.416667   \n",
       "2        3.000000   2.000000     2.477273   \n",
       "3        5.000000   5.000000     5.000000   \n",
       "4        1.000000   1.000000     1.250000   \n",
       "...           ...        ...          ...   \n",
       "134968   2.000000   2.000000     1.750000   \n",
       "134969   3.000000   3.000000     2.833333   \n",
       "134970   3.333333   4.333333     3.916667   \n",
       "134971   5.000000   5.000000     5.000000   \n",
       "134972   4.000000   2.500000     2.812500   \n",
       "\n",
       "                                   spec_comb  \\\n",
       "0       Family Practice                        \n",
       "1       Family Practice                        \n",
       "2       Internal Medicine                      \n",
       "3       Family Practice                        \n",
       "4       Surgery                                \n",
       "...                                      ...   \n",
       "134968  Family Practice                        \n",
       "134969  Internal Medicine                      \n",
       "134970  Internal Medicine                      \n",
       "134971  Pediatrics                             \n",
       "134972  Obstetrics and Gynecology              \n",
       "\n",
       "                                            review_corpus  \n",
       "0       Very clean offices, friendly and informative s...  \n",
       "1       If you are lucky enough to get in to see Dr. B...  \n",
       "2       satisfactory but would not recommend coldness,...  \n",
       "3       The best physician I've been to in my 70 years...  \n",
       "4       I found Dr. Bernstein to be cold and uncaring....  \n",
       "...                                                   ...  \n",
       "134968  Very nice but not genuine-seeming. Didn't have...  \n",
       "134969  dr. Foster has been excellent, she has hapled ...  \n",
       "134970  Dr. Davis is a very caring physician. She is n...  \n",
       "134971  Two years ago, I was diagnosed with ulcerative...  \n",
       "134972  Dr. Lyon is a very through doctor who takes a ...  \n",
       "\n",
       "[134968 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_info = pd.read_csv('data/genders.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_info.drop_duplicates(subset='hp_id', keep='first',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_review = pd.merge(ratings, gender_info, on='hp_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_review['spec_comb'] = doc_review['spec_comb'].map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_review['sentence']= doc_review['review_corpus'].map(lambda x: x.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very clean offices, friendly and informative staff \n",
      " I love this doctor! She is very warm and takes her time to answer questions. She's in an office with many doctors, and I am usually glad when she's the doctor on call. \n",
      " She's really nice and helpful. She takes time to listen to what's going on in order to come up with the best solution. I really like her! \n"
     ]
    }
   ],
   "source": [
    "for sentence in doc_review['sentence'][0]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134968"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134968"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_review['hp_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp_id</th>\n",
       "      <th>ratemds_id</th>\n",
       "      <th>hasorder</th>\n",
       "      <th>order_id</th>\n",
       "      <th>avg_help</th>\n",
       "      <th>avg_know</th>\n",
       "      <th>avg_punct</th>\n",
       "      <th>avg_staff</th>\n",
       "      <th>avg_ranking</th>\n",
       "      <th>spec_comb</th>\n",
       "      <th>review_corpus</th>\n",
       "      <th>gender</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303282</td>\n",
       "      <td>24616</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>Very clean offices, friendly and informative s...</td>\n",
       "      <td>F</td>\n",
       "      <td>[Very clean offices, friendly and informative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427422</td>\n",
       "      <td>66138</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>If you are lucky enough to get in to see Dr. B...</td>\n",
       "      <td>M</td>\n",
       "      <td>[If you are lucky enough to get in to see Dr. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>Internal Medicine</td>\n",
       "      <td>satisfactory but would not recommend coldness,...</td>\n",
       "      <td>F</td>\n",
       "      <td>[satisfactory but would not recommend coldness...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1250536</td>\n",
       "      <td>3065893</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>The best physician I've been to in my 70 years...</td>\n",
       "      <td>F</td>\n",
       "      <td>[The best physician I've been to in my 70 year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1269847</td>\n",
       "      <td>284377</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>I found Dr. Bernstein to be cold and uncaring....</td>\n",
       "      <td>F</td>\n",
       "      <td>[I found Dr. Bernstein to be cold and uncaring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134963</th>\n",
       "      <td>1165896</td>\n",
       "      <td>2474959</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>Very nice but not genuine-seeming. Didn't have...</td>\n",
       "      <td>F</td>\n",
       "      <td>[Very nice but not genuine-seeming. Didn't hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134964</th>\n",
       "      <td>1049044</td>\n",
       "      <td>3205133</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>Internal Medicine</td>\n",
       "      <td>dr. Foster has been excellent, she has hapled ...</td>\n",
       "      <td>F</td>\n",
       "      <td>[dr. Foster has been excellent, she has hapled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134965</th>\n",
       "      <td>1132315</td>\n",
       "      <td>2880773</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>Internal Medicine</td>\n",
       "      <td>Dr. Davis is a very caring physician. She is n...</td>\n",
       "      <td>F</td>\n",
       "      <td>[Dr. Davis is a very caring physician. She is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134966</th>\n",
       "      <td>1308120</td>\n",
       "      <td>152018</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Pediatrics</td>\n",
       "      <td>Two years ago, I was diagnosed with ulcerative...</td>\n",
       "      <td>M</td>\n",
       "      <td>[Two years ago, I was diagnosed with ulcerativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134967</th>\n",
       "      <td>1321949</td>\n",
       "      <td>548661</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.812500</td>\n",
       "      <td>Obstetrics and Gynecology</td>\n",
       "      <td>Dr. Lyon is a very through doctor who takes a ...</td>\n",
       "      <td>F</td>\n",
       "      <td>[Dr. Lyon is a very through doctor who takes a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134968 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          hp_id  ratemds_id  hasorder  order_id  avg_help  avg_know  \\\n",
       "0        303282       24616         0       NaN  4.333333  4.333333   \n",
       "1        427422       66138         0       NaN  4.666667  5.000000   \n",
       "2         19713      127113         0       NaN  2.272727  2.636364   \n",
       "3       1250536     3065893         0       NaN  5.000000  5.000000   \n",
       "4       1269847      284377         0       NaN  1.000000  2.000000   \n",
       "...         ...         ...       ...       ...       ...       ...   \n",
       "134963  1165896     2474959         0       NaN  2.000000  1.000000   \n",
       "134964  1049044     3205133         0       NaN  2.666667  2.666667   \n",
       "134965  1132315     2880773         0       NaN  3.666667  4.333333   \n",
       "134966  1308120      152018         0       NaN  5.000000  5.000000   \n",
       "134967  1321949      548661         0       NaN  2.250000  2.500000   \n",
       "\n",
       "        avg_punct  avg_staff  avg_ranking                  spec_comb  \\\n",
       "0        4.333333   2.666667     3.916667            Family Practice   \n",
       "1        4.666667   3.333333     4.416667            Family Practice   \n",
       "2        3.000000   2.000000     2.477273          Internal Medicine   \n",
       "3        5.000000   5.000000     5.000000            Family Practice   \n",
       "4        1.000000   1.000000     1.250000                    Surgery   \n",
       "...           ...        ...          ...                        ...   \n",
       "134963   2.000000   2.000000     1.750000            Family Practice   \n",
       "134964   3.000000   3.000000     2.833333          Internal Medicine   \n",
       "134965   3.333333   4.333333     3.916667          Internal Medicine   \n",
       "134966   5.000000   5.000000     5.000000                 Pediatrics   \n",
       "134967   4.000000   2.500000     2.812500  Obstetrics and Gynecology   \n",
       "\n",
       "                                            review_corpus gender  \\\n",
       "0       Very clean offices, friendly and informative s...      F   \n",
       "1       If you are lucky enough to get in to see Dr. B...      M   \n",
       "2       satisfactory but would not recommend coldness,...      F   \n",
       "3       The best physician I've been to in my 70 years...      F   \n",
       "4       I found Dr. Bernstein to be cold and uncaring....      F   \n",
       "...                                                   ...    ...   \n",
       "134963  Very nice but not genuine-seeming. Didn't have...      F   \n",
       "134964  dr. Foster has been excellent, she has hapled ...      F   \n",
       "134965  Dr. Davis is a very caring physician. She is n...      F   \n",
       "134966  Two years ago, I was diagnosed with ulcerative...      M   \n",
       "134967  Dr. Lyon is a very through doctor who takes a ...      F   \n",
       "\n",
       "                                                 sentence  \n",
       "0       [Very clean offices, friendly and informative ...  \n",
       "1       [If you are lucky enough to get in to see Dr. ...  \n",
       "2       [satisfactory but would not recommend coldness...  \n",
       "3       [The best physician I've been to in my 70 year...  \n",
       "4       [I found Dr. Bernstein to be cold and uncaring...  \n",
       "...                                                   ...  \n",
       "134963  [Very nice but not genuine-seeming. Didn't hav...  \n",
       "134964  [dr. Foster has been excellent, she has hapled...  \n",
       "134965  [Dr. Davis is a very caring physician. She is ...  \n",
       "134966  [Two years ago, I was diagnosed with ulcerativ...  \n",
       "134967  [Dr. Lyon is a very through doctor who takes a...  \n",
       "\n",
       "[134968 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a datset where there is one row for review not for doctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_review.reset_index(inplace= True,drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hp_id</th>\n",
       "      <th>ratemds_id</th>\n",
       "      <th>hasorder</th>\n",
       "      <th>order_id</th>\n",
       "      <th>avg_help</th>\n",
       "      <th>avg_know</th>\n",
       "      <th>avg_punct</th>\n",
       "      <th>avg_staff</th>\n",
       "      <th>avg_ranking</th>\n",
       "      <th>spec_comb</th>\n",
       "      <th>review_corpus</th>\n",
       "      <th>gender</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>303282</td>\n",
       "      <td>24616</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>Very clean offices, friendly and informative s...</td>\n",
       "      <td>F</td>\n",
       "      <td>[Very clean offices, friendly and informative ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>427422</td>\n",
       "      <td>66138</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>If you are lucky enough to get in to see Dr. B...</td>\n",
       "      <td>M</td>\n",
       "      <td>[If you are lucky enough to get in to see Dr. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>Internal Medicine</td>\n",
       "      <td>satisfactory but would not recommend coldness,...</td>\n",
       "      <td>F</td>\n",
       "      <td>[satisfactory but would not recommend coldness...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1250536</td>\n",
       "      <td>3065893</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Family Practice</td>\n",
       "      <td>The best physician I've been to in my 70 years...</td>\n",
       "      <td>F</td>\n",
       "      <td>[The best physician I've been to in my 70 year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1269847</td>\n",
       "      <td>284377</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>Surgery</td>\n",
       "      <td>I found Dr. Bernstein to be cold and uncaring....</td>\n",
       "      <td>F</td>\n",
       "      <td>[I found Dr. Bernstein to be cold and uncaring...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hp_id  ratemds_id  hasorder  order_id  avg_help  avg_know  avg_punct  \\\n",
       "0   303282       24616         0       NaN  4.333333  4.333333   4.333333   \n",
       "1   427422       66138         0       NaN  4.666667  5.000000   4.666667   \n",
       "2    19713      127113         0       NaN  2.272727  2.636364   3.000000   \n",
       "3  1250536     3065893         0       NaN  5.000000  5.000000   5.000000   \n",
       "4  1269847      284377         0       NaN  1.000000  2.000000   1.000000   \n",
       "\n",
       "   avg_staff  avg_ranking          spec_comb  \\\n",
       "0   2.666667     3.916667    Family Practice   \n",
       "1   3.333333     4.416667    Family Practice   \n",
       "2   2.000000     2.477273  Internal Medicine   \n",
       "3   5.000000     5.000000    Family Practice   \n",
       "4   1.000000     1.250000            Surgery   \n",
       "\n",
       "                                       review_corpus gender  \\\n",
       "0  Very clean offices, friendly and informative s...      F   \n",
       "1  If you are lucky enough to get in to see Dr. B...      M   \n",
       "2  satisfactory but would not recommend coldness,...      F   \n",
       "3  The best physician I've been to in my 70 years...      F   \n",
       "4  I found Dr. Bernstein to be cold and uncaring....      F   \n",
       "\n",
       "                                            sentence  \n",
       "0  [Very clean offices, friendly and informative ...  \n",
       "1  [If you are lucky enough to get in to see Dr. ...  \n",
       "2  [satisfactory but would not recommend coldness...  \n",
       "3  [The best physician I've been to in my 70 year...  \n",
       "4  [I found Dr. Bernstein to be cold and uncaring...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_reviews = doc_review[doc_review[\"spec_comb\"] == \"Internal Medicine\"].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33549 entries, 0 to 33548\n",
      "Data columns (total 13 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   hp_id          33549 non-null  int64  \n",
      " 1   ratemds_id     33549 non-null  int64  \n",
      " 2   hasorder       33549 non-null  int64  \n",
      " 3   order_id       207 non-null    float64\n",
      " 4   avg_help       33549 non-null  float64\n",
      " 5   avg_know       33549 non-null  float64\n",
      " 6   avg_punct      33549 non-null  float64\n",
      " 7   avg_staff      31987 non-null  float64\n",
      " 8   avg_ranking    33549 non-null  float64\n",
      " 9   spec_comb      33549 non-null  object \n",
      " 10  review_corpus  33549 non-null  object \n",
      " 11  gender         33549 non-null  object \n",
      " 12  sentence       33549 non-null  object \n",
      "dtypes: float64(6), int64(3), object(4)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "im_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new dataframe of doc's review, make sure each record has single review\n",
    "im_review_df = pd.DataFrame()\n",
    "for i, sentence in enumerate(im_reviews['sentence']):\n",
    "    temp_dict = dict(enumerate(sentence))\n",
    "    a = len(list(temp_dict.keys()))\n",
    "    s = str(im_reviews['hp_id'][i])\n",
    "    temp_df = pd.DataFrame.from_dict(data = temp_dict, orient = 'index', columns=['Review'])\n",
    "    temp_df['hp_id'] = [s for i in range(a)]\n",
    "    im_review_df = im_review_df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89661 entries, 0 to 89660\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  89661 non-null  object\n",
      " 1   hp_id   89661 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "im_review_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>hp_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>satisfactory but would not recommend coldness,...</td>\n",
       "      <td>19713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Although Dr. Merlo does not appear to have an...</td>\n",
       "      <td>19713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>took my daughter for problems, never examined...</td>\n",
       "      <td>19713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes, Dr. Merlot can be rude and arrogant, but...</td>\n",
       "      <td>19713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I called to make and appointment and a woman ...</td>\n",
       "      <td>19713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89656</th>\n",
       "      <td>This doctor is just flaky. Dr. Foster, just s...</td>\n",
       "      <td>1049044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89657</th>\n",
       "      <td>DO NOT GO TO DR. ERNIA FOSTER. She perforated...</td>\n",
       "      <td>1049044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89658</th>\n",
       "      <td>Dr. Davis is a very caring physician. She is n...</td>\n",
       "      <td>1132315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89659</th>\n",
       "      <td>I found it sad that Dr. Charlyce Davis will n...</td>\n",
       "      <td>1132315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89660</th>\n",
       "      <td>The times she is running behind you are promp...</td>\n",
       "      <td>1132315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89661 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review    hp_id\n",
       "0      satisfactory but would not recommend coldness,...    19713\n",
       "1       Although Dr. Merlo does not appear to have an...    19713\n",
       "2       took my daughter for problems, never examined...    19713\n",
       "3       Yes, Dr. Merlot can be rude and arrogant, but...    19713\n",
       "4       I called to make and appointment and a woman ...    19713\n",
       "...                                                  ...      ...\n",
       "89656   This doctor is just flaky. Dr. Foster, just s...  1049044\n",
       "89657   DO NOT GO TO DR. ERNIA FOSTER. She perforated...  1049044\n",
       "89658  Dr. Davis is a very caring physician. She is n...  1132315\n",
       "89659   I found it sad that Dr. Charlyce Davis will n...  1132315\n",
       "89660   The times she is running behind you are promp...  1132315\n",
       "\n",
       "[89661 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df['hp_id'] = im_review_df['hp_id'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 89661 entries, 0 to 89660\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  89661 non-null  object\n",
      " 1   hp_id   89661 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "im_review_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im_review_df = pd.merge(im_review_df, doc_review, on='hp_id', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 89661 entries, 0 to 89660\n",
      "Data columns (total 14 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Review         89661 non-null  object \n",
      " 1   hp_id          89661 non-null  int64  \n",
      " 2   ratemds_id     89661 non-null  int64  \n",
      " 3   hasorder       89661 non-null  int64  \n",
      " 4   order_id       593 non-null    float64\n",
      " 5   avg_help       89661 non-null  float64\n",
      " 6   avg_know       89661 non-null  float64\n",
      " 7   avg_punct      89661 non-null  float64\n",
      " 8   avg_staff      87664 non-null  float64\n",
      " 9   avg_ranking    89661 non-null  float64\n",
      " 10  spec_comb      89661 non-null  object \n",
      " 11  review_corpus  89661 non-null  object \n",
      " 12  gender         89661 non-null  object \n",
      " 13  sentence       89661 non-null  object \n",
      "dtypes: float64(6), int64(3), object(5)\n",
      "memory usage: 10.3+ MB\n"
     ]
    }
   ],
   "source": [
    "im_review_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df = im_review_df.drop(['review_corpus', 'sentence', 'spec_comb'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>hp_id</th>\n",
       "      <th>ratemds_id</th>\n",
       "      <th>hasorder</th>\n",
       "      <th>order_id</th>\n",
       "      <th>avg_help</th>\n",
       "      <th>avg_know</th>\n",
       "      <th>avg_punct</th>\n",
       "      <th>avg_staff</th>\n",
       "      <th>avg_ranking</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>satisfactory but would not recommend coldness,...</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Although Dr. Merlo does not appear to have an...</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>took my daughter for problems, never examined...</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes, Dr. Merlot can be rude and arrogant, but...</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I called to make and appointment and a woman ...</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  hp_id  ratemds_id  \\\n",
       "0  satisfactory but would not recommend coldness,...  19713      127113   \n",
       "1   Although Dr. Merlo does not appear to have an...  19713      127113   \n",
       "2   took my daughter for problems, never examined...  19713      127113   \n",
       "3   Yes, Dr. Merlot can be rude and arrogant, but...  19713      127113   \n",
       "4   I called to make and appointment and a woman ...  19713      127113   \n",
       "\n",
       "   hasorder  order_id  avg_help  avg_know  avg_punct  avg_staff  avg_ranking  \\\n",
       "0         0       NaN  2.272727  2.636364        3.0        2.0     2.477273   \n",
       "1         0       NaN  2.272727  2.636364        3.0        2.0     2.477273   \n",
       "2         0       NaN  2.272727  2.636364        3.0        2.0     2.477273   \n",
       "3         0       NaN  2.272727  2.636364        3.0        2.0     2.477273   \n",
       "4         0       NaN  2.272727  2.636364        3.0        2.0     2.477273   \n",
       "\n",
       "  gender  \n",
       "0      F  \n",
       "1      F  \n",
       "2      F  \n",
       "3      F  \n",
       "4      F  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    63281\n",
       "F    26380\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_review_df.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    0.705781\n",
       "F    0.294219\n",
       "Name: gender, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_review_df.gender.value_counts()/len(im_review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    89068\n",
       "1      593\n",
       "Name: hasorder, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_review_df.hasorder.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.993386\n",
       "1    0.006614\n",
       "Name: hasorder, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_review_df.hasorder.value_counts()/len(im_review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cols_to_binary(val, col_name):\n",
    "    # col names can be \"gender\" or \"ranking\"\n",
    "    if col_name == \"gender\":\n",
    "        if val == \"F\":\n",
    "            return_var = 1\n",
    "        elif val == \"M\":\n",
    "            return_var = 0\n",
    "        else:\n",
    "            return_var = \"NA\"\n",
    "    elif col_name == \"ranking\":\n",
    "        if val >= 4.0:\n",
    "            return_var = 1\n",
    "        elif val < 4.0:\n",
    "            return_var = 0\n",
    "        else: \n",
    "            return_var = \"NA\"\n",
    "    return return_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_review_df[\"gen_bin\"] = im_review_df[\"gender\"].apply(lambda x: cols_to_binary(x, \"gender\"))\n",
    "im_review_df[\"high_avg_rank\"] = im_review_df[\"avg_ranking\"].apply(lambda x: cols_to_binary(x, \"ranking\"))\n",
    "im_review_df[\"high_avg_help\"] = im_review_df[\"avg_help\"].apply(lambda x: cols_to_binary(x, \"ranking\"))\n",
    "im_review_df[\"high_avg_know\"] = im_review_df[\"avg_know\"].apply(lambda x: cols_to_binary(x, \"ranking\"))\n",
    "im_review_df[\"high_avg_punc\"] = im_review_df[\"avg_punct\"].apply(lambda x: cols_to_binary(x, \"ranking\"))\n",
    "im_review_df[\"high_avg_staf\"] = im_review_df[\"avg_staff\"].apply(lambda x: cols_to_binary(x, \"ranking\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>hp_id</th>\n",
       "      <th>ratemds_id</th>\n",
       "      <th>hasorder</th>\n",
       "      <th>order_id</th>\n",
       "      <th>avg_help</th>\n",
       "      <th>avg_know</th>\n",
       "      <th>avg_punct</th>\n",
       "      <th>avg_staff</th>\n",
       "      <th>avg_ranking</th>\n",
       "      <th>gender</th>\n",
       "      <th>gen_bin</th>\n",
       "      <th>high_avg_rank</th>\n",
       "      <th>high_avg_help</th>\n",
       "      <th>high_avg_know</th>\n",
       "      <th>high_avg_punc</th>\n",
       "      <th>high_avg_staf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>satisfactory but would not recommend coldness,...</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Although Dr. Merlo does not appear to have an...</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>took my daughter for problems, never examined...</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes, Dr. Merlot can be rude and arrogant, but...</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I called to make and appointment and a woman ...</td>\n",
       "      <td>19713</td>\n",
       "      <td>127113</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.272727</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.477273</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89656</th>\n",
       "      <td>This doctor is just flaky. Dr. Foster, just s...</td>\n",
       "      <td>1049044</td>\n",
       "      <td>3205133</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89657</th>\n",
       "      <td>DO NOT GO TO DR. ERNIA FOSTER. She perforated...</td>\n",
       "      <td>1049044</td>\n",
       "      <td>3205133</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89658</th>\n",
       "      <td>Dr. Davis is a very caring physician. She is n...</td>\n",
       "      <td>1132315</td>\n",
       "      <td>2880773</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89659</th>\n",
       "      <td>I found it sad that Dr. Charlyce Davis will n...</td>\n",
       "      <td>1132315</td>\n",
       "      <td>2880773</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89660</th>\n",
       "      <td>The times she is running behind you are promp...</td>\n",
       "      <td>1132315</td>\n",
       "      <td>2880773</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89661 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review    hp_id  ratemds_id  \\\n",
       "0      satisfactory but would not recommend coldness,...    19713      127113   \n",
       "1       Although Dr. Merlo does not appear to have an...    19713      127113   \n",
       "2       took my daughter for problems, never examined...    19713      127113   \n",
       "3       Yes, Dr. Merlot can be rude and arrogant, but...    19713      127113   \n",
       "4       I called to make and appointment and a woman ...    19713      127113   \n",
       "...                                                  ...      ...         ...   \n",
       "89656   This doctor is just flaky. Dr. Foster, just s...  1049044     3205133   \n",
       "89657   DO NOT GO TO DR. ERNIA FOSTER. She perforated...  1049044     3205133   \n",
       "89658  Dr. Davis is a very caring physician. She is n...  1132315     2880773   \n",
       "89659   I found it sad that Dr. Charlyce Davis will n...  1132315     2880773   \n",
       "89660   The times she is running behind you are promp...  1132315     2880773   \n",
       "\n",
       "       hasorder  order_id  avg_help  avg_know  avg_punct  avg_staff  \\\n",
       "0             0       NaN  2.272727  2.636364   3.000000   2.000000   \n",
       "1             0       NaN  2.272727  2.636364   3.000000   2.000000   \n",
       "2             0       NaN  2.272727  2.636364   3.000000   2.000000   \n",
       "3             0       NaN  2.272727  2.636364   3.000000   2.000000   \n",
       "4             0       NaN  2.272727  2.636364   3.000000   2.000000   \n",
       "...         ...       ...       ...       ...        ...        ...   \n",
       "89656         0       NaN  2.666667  2.666667   3.000000   3.000000   \n",
       "89657         0       NaN  2.666667  2.666667   3.000000   3.000000   \n",
       "89658         0       NaN  3.666667  4.333333   3.333333   4.333333   \n",
       "89659         0       NaN  3.666667  4.333333   3.333333   4.333333   \n",
       "89660         0       NaN  3.666667  4.333333   3.333333   4.333333   \n",
       "\n",
       "       avg_ranking gender  gen_bin  high_avg_rank  high_avg_help  \\\n",
       "0         2.477273      F        1              0              0   \n",
       "1         2.477273      F        1              0              0   \n",
       "2         2.477273      F        1              0              0   \n",
       "3         2.477273      F        1              0              0   \n",
       "4         2.477273      F        1              0              0   \n",
       "...            ...    ...      ...            ...            ...   \n",
       "89656     2.833333      F        1              0              0   \n",
       "89657     2.833333      F        1              0              0   \n",
       "89658     3.916667      F        1              0              0   \n",
       "89659     3.916667      F        1              0              0   \n",
       "89660     3.916667      F        1              0              0   \n",
       "\n",
       "       high_avg_know  high_avg_punc high_avg_staf  \n",
       "0                  0              0             0  \n",
       "1                  0              0             0  \n",
       "2                  0              0             0  \n",
       "3                  0              0             0  \n",
       "4                  0              0             0  \n",
       "...              ...            ...           ...  \n",
       "89656              0              0             0  \n",
       "89657              0              0             0  \n",
       "89658              1              0             1  \n",
       "89659              1              0             1  \n",
       "89660              1              0             1  \n",
       "\n",
       "[89661 rows x 17 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now prepare the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle them\n",
    "df = im_review_df.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71728.8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eighty_percent = len(df)*.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 80% for train // 20% for test\n",
    "df_train = df[0:eighty_percent]\n",
    "df_test = df[eighty_percent:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop = True)\n",
    "df_test = df_test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>hp_id</th>\n",
       "      <th>ratemds_id</th>\n",
       "      <th>hasorder</th>\n",
       "      <th>order_id</th>\n",
       "      <th>avg_help</th>\n",
       "      <th>avg_know</th>\n",
       "      <th>avg_punct</th>\n",
       "      <th>avg_staff</th>\n",
       "      <th>avg_ranking</th>\n",
       "      <th>gender</th>\n",
       "      <th>gen_bin</th>\n",
       "      <th>high_avg_rank</th>\n",
       "      <th>high_avg_help</th>\n",
       "      <th>high_avg_know</th>\n",
       "      <th>high_avg_punc</th>\n",
       "      <th>high_avg_staf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After almost 15 years of Dr's not being able ...</td>\n",
       "      <td>471809</td>\n",
       "      <td>115026</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.727273</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>2.090909</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>3.204545</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I read the other comments and was so surprise...</td>\n",
       "      <td>1182870</td>\n",
       "      <td>802317</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.090909</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.818182</td>\n",
       "      <td>3.545455</td>\n",
       "      <td>3.022727</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With all due respect to the patient who said ...</td>\n",
       "      <td>414404</td>\n",
       "      <td>27184</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr Steiner is a very knowledgable and caring ...</td>\n",
       "      <td>569651</td>\n",
       "      <td>103627</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I went to him because my husbands family all g...</td>\n",
       "      <td>765142</td>\n",
       "      <td>715854</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71724</th>\n",
       "      <td>Dr. Stone treats both me and my husband. We l...</td>\n",
       "      <td>307737</td>\n",
       "      <td>153207</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.750000</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71725</th>\n",
       "      <td>Such an amazing doctor...used to be a Profess...</td>\n",
       "      <td>999182</td>\n",
       "      <td>27922</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.766667</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71726</th>\n",
       "      <td>Dr. Fisher office staff is very unfriendly. It...</td>\n",
       "      <td>1294820</td>\n",
       "      <td>15780</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.062500</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71727</th>\n",
       "      <td>Spends the time with me and does answer my que...</td>\n",
       "      <td>821968</td>\n",
       "      <td>2586678</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71728</th>\n",
       "      <td>Excellent. knowledgeable, deliberate, responsi...</td>\n",
       "      <td>720684</td>\n",
       "      <td>37061</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71729 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review    hp_id  ratemds_id  \\\n",
       "0       After almost 15 years of Dr's not being able ...   471809      115026   \n",
       "1       I read the other comments and was so surprise...  1182870      802317   \n",
       "2       With all due respect to the patient who said ...   414404       27184   \n",
       "3       Dr Steiner is a very knowledgable and caring ...   569651      103627   \n",
       "4      I went to him because my husbands family all g...   765142      715854   \n",
       "...                                                  ...      ...         ...   \n",
       "71724   Dr. Stone treats both me and my husband. We l...   307737      153207   \n",
       "71725   Such an amazing doctor...used to be a Profess...   999182       27922   \n",
       "71726  Dr. Fisher office staff is very unfriendly. It...  1294820       15780   \n",
       "71727  Spends the time with me and does answer my que...   821968     2586678   \n",
       "71728  Excellent. knowledgeable, deliberate, responsi...   720684       37061   \n",
       "\n",
       "       hasorder  order_id  avg_help  avg_know  avg_punct  avg_staff  \\\n",
       "0             0       NaN  3.727273  3.363636   2.090909   3.636364   \n",
       "1             0       NaN  2.090909  2.636364   3.818182   3.545455   \n",
       "2             0       NaN  2.800000  2.800000   4.200000   0.600000   \n",
       "3             0       NaN  5.000000  5.000000   5.000000   3.500000   \n",
       "4             0       NaN  2.000000  1.500000   3.500000   3.500000   \n",
       "...         ...       ...       ...       ...        ...        ...   \n",
       "71724         0       NaN  4.666667  5.000000   5.000000   4.333333   \n",
       "71725         0       NaN  2.733333  3.000000   3.333333   2.000000   \n",
       "71726         0       NaN  3.000000  3.000000   2.000000   0.250000   \n",
       "71727         0       NaN  4.000000  4.000000   3.000000   3.000000   \n",
       "71728         0       NaN  5.000000  5.000000   5.000000        NaN   \n",
       "\n",
       "       avg_ranking gender  gen_bin  high_avg_rank  high_avg_help  \\\n",
       "0         3.204545      M        0              0              0   \n",
       "1         3.022727      M        0              0              0   \n",
       "2         2.600000      F        1              0              0   \n",
       "3         4.625000      F        1              1              1   \n",
       "4         2.625000      M        0              0              0   \n",
       "...            ...    ...      ...            ...            ...   \n",
       "71724     4.750000      M        0              1              1   \n",
       "71725     2.766667      M        0              0              0   \n",
       "71726     2.062500      M        0              0              0   \n",
       "71727     3.500000      M        0              0              1   \n",
       "71728     5.000000      M        0              1              1   \n",
       "\n",
       "       high_avg_know  high_avg_punc high_avg_staf  \n",
       "0                  0              0             0  \n",
       "1                  0              0             0  \n",
       "2                  0              1             0  \n",
       "3                  1              1             0  \n",
       "4                  0              0             0  \n",
       "...              ...            ...           ...  \n",
       "71724              1              1             1  \n",
       "71725              0              0             0  \n",
       "71726              0              0             0  \n",
       "71727              1              0             0  \n",
       "71728              1              1            NA  \n",
       "\n",
       "[71729 rows x 17 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split x data (literally just the review) from the other potential y variables (which also serve as metadata analysis features)\n",
    "\n",
    "x_train = df_train[\"Review\"]\n",
    "y_train_gen = df_train[\"gen_bin\"]\n",
    "y_train_rank = df_train[\"high_avg_rank\"]\n",
    "y_train_help = df_train[\"high_avg_help\"]\n",
    "y_train_know = df_train[\"high_avg_know\"]\n",
    "y_train_punc = df_train[\"high_avg_punc\"]\n",
    "\n",
    "x_test = df_test[\"Review\"]\n",
    "y_test_gen = df_test[\"gen_bin\"]\n",
    "y_test_rank = df_test[\"high_avg_rank\"]\n",
    "y_test_help = df_test[\"high_avg_help\"]\n",
    "y_test_know = df_test[\"high_avg_know\"]\n",
    "y_test_punc = df_test[\"high_avg_punc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         After almost 15 years of Dr's not being able ...\n",
       "1         I read the other comments and was so surprise...\n",
       "2         With all due respect to the patient who said ...\n",
       "3         Dr Steiner is a very knowledgable and caring ...\n",
       "4        I went to him because my husbands family all g...\n",
       "                               ...                        \n",
       "71724     Dr. Stone treats both me and my husband. We l...\n",
       "71725     Such an amazing doctor...used to be a Profess...\n",
       "71726    Dr. Fisher office staff is very unfriendly. It...\n",
       "71727    Spends the time with me and does answer my que...\n",
       "71728    Excellent. knowledgeable, deliberate, responsi...\n",
       "Name: Review, Length: 71729, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(s):\n",
    "    \n",
    "    # unwanted symbols/punctuations\n",
    "    s = sub(\"%\", \" percent\", s) ##percents\n",
    "    s = sub(\"&amp;\", \"and\", s) ##ampersands\n",
    "    s = sub(\"'s\", \"\", s) ##possessive or contraction\n",
    "    s = sub(\"'re\", \"\", s) ##contraction\n",
    "    s = sub(\"'ll\", \"\", s) ##contraction\n",
    "    s = sub(\"‚Äô\", \"\", s) ##contraction\n",
    "    s = sub(\"'t\", \" not\", s) ##contraction\n",
    "    \n",
    "    # typical text mining things\n",
    "    s = s.lower() ##case sensitivity\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.apply(lambda x: clean_word(x))\n",
    "x_test = x_test.apply(lambda x: clean_word(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The most basic model: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize text reviews to numbers\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "x_train_vec = vec.fit_transform(x_train).toarray()\n",
    "x_test_vec = vec.transform(x_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(x_train_vec, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81757727000237"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.score(x_train_vec, y_train_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7440330136069596"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.score(x_test_vec, y_test_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = model_nb.predict(x_test_vec)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(pred_val, actual_val):\n",
    "    if pred_val == 1:\n",
    "        if actual_val == 1:\n",
    "            return \"TP\"\n",
    "        elif actual_val == 0:\n",
    "            return \"FP\"\n",
    "    elif pred_val == 0:\n",
    "        if actual_val == 1:\n",
    "            return \"FN\"\n",
    "        elif actual_val == 0:\n",
    "            return \"TN\"\n",
    "    else:\n",
    "        return \"ERROR IN DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conf_df(model, x_df, y_df):\n",
    "    compared_df = pd.DataFrame()\n",
    "    compared_df[\"preds\"] = model.predict(x_df)\n",
    "    compared_df[\"actuals\"] = y_df\n",
    "    compared_df[\"error_type\"] = compared_df.apply(lambda x: conf_matrix(x.preds, x.actuals), axis=1)\n",
    "    compared_df[\"count\"] = 1\n",
    "    return compared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_cost_matrix(model, x_df, y_df):\n",
    "    conf_df = create_conf_df(model, x_df, y_df)\n",
    "    summary_df = conf_df[[\"error_type\", \"count\"]].groupby(by=[\"error_type\"]).sum()\n",
    "    fn = summary_df.iloc[0][0]\n",
    "    fp = summary_df.iloc[1][0]\n",
    "    tn = summary_df.iloc[2][0]\n",
    "    tp = summary_df.iloc[3][0]\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = (2*precision*recall)/(precision+recall)\n",
    "    F0_5score = ((1 + 0.5**2) * precision * recall) / (0.5**2 * precision + recall)\n",
    "    print(summary_df)\n",
    "    print(\"FN:\", fn)\n",
    "    print(\"FP:\", fp)\n",
    "    print(\"TN:\", tn)\n",
    "    print(\"TP:\", tp)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1_score)\n",
    "    print(\"F0.5:\", F0_5score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            count\n",
      "error_type       \n",
      "FN          10699\n",
      "FP           2386\n",
      "TN          48184\n",
      "TP          10460\n",
      "FN: 10699\n",
      "FP: 2386\n",
      "TN: 48184\n",
      "TP: 10460\n",
      "Precision: 0.8142612486377082\n",
      "Recall: 0.49435228507963513\n",
      "F1: 0.6152036465225702\n",
      "F0.5: 0.7209517113987566\n"
     ]
    }
   ],
   "source": [
    "summary_cost_matrix(model_nb, x_train_vec, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            count\n",
      "error_type       \n",
      "FN           3577\n",
      "FP           1013\n",
      "TN          11698\n",
      "TP           1644\n",
      "FN: 3577\n",
      "FP: 1013\n",
      "TN: 11698\n",
      "TP: 1644\n",
      "Precision: 0.6187429431689876\n",
      "Recall: 0.31488220647385556\n",
      "F1: 0.41736481340441733\n",
      "F0.5: 0.5186447094453909\n"
     ]
    }
   ],
   "source": [
    "summary_cost_matrix(model_nb, x_test_vec, y_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    " \n",
    "# n_estimators = # of trees\n",
    "rf_model_no_extra = RandomForestClassifier(n_estimators = 501,\n",
    "                                           criterion = 'entropy')\n",
    "                             \n",
    "rf_model_no_extra.fit(x_train_vec, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf_model_no_extra.score(x_train_vec, y_train_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_no_extra.score(x_test_vec, y_test_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred_rf_no_extra = rf_model_no_extra.predict(x_test_vec)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cost_matrix(rf_model_no_extra, x_train_vec, y_train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cost_matrix(rf_model_no_extra, x_test_vec, y_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding lemmatization, stemming etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['after',\n",
       " 'almost',\n",
       " '15',\n",
       " 'years',\n",
       " 'of',\n",
       " 'dr',\n",
       " 'not',\n",
       " 'being',\n",
       " 'able',\n",
       " 'to',\n",
       " 'find',\n",
       " 'out',\n",
       " 'what',\n",
       " 'was',\n",
       " 'wrong',\n",
       " 'with',\n",
       " 'my',\n",
       " 'stomach',\n",
       " 'issues',\n",
       " ',',\n",
       " 'it',\n",
       " 'took',\n",
       " 'just',\n",
       " '2',\n",
       " 'visits',\n",
       " 'with',\n",
       " 'dr.',\n",
       " 'follick',\n",
       " 'to',\n",
       " 'find',\n",
       " 'out',\n",
       " 'it',\n",
       " 'was',\n",
       " 'crohn',\n",
       " 'disease',\n",
       " 'and',\n",
       " 'malabsorption',\n",
       " '.',\n",
       " 'now',\n",
       " 'i',\n",
       " 'feel',\n",
       " '1000x',\n",
       " 'better',\n",
       " '.',\n",
       " 'he',\n",
       " 'has',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'humor',\n",
       " ',',\n",
       " 'is',\n",
       " 'extremely',\n",
       " 'knowledgeable',\n",
       " 'and',\n",
       " 'has',\n",
       " 'the',\n",
       " 'tenacity',\n",
       " 'of',\n",
       " 'a',\n",
       " 'pit',\n",
       " 'bull',\n",
       " 'when',\n",
       " 'it',\n",
       " 'comes',\n",
       " 'to',\n",
       " 'a',\n",
       " 'diagnosis',\n",
       " '.',\n",
       " 'sure',\n",
       " 'some',\n",
       " 'people',\n",
       " 'may',\n",
       " 'complain',\n",
       " 'that',\n",
       " 'the',\n",
       " 'waiting',\n",
       " 'room',\n",
       " 'is',\n",
       " 'crowded',\n",
       " 'but',\n",
       " 'would',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'better',\n",
       " 'going',\n",
       " 'to',\n",
       " 'a',\n",
       " 'dr.',\n",
       " 'who',\n",
       " 'has',\n",
       " 'no',\n",
       " 'one',\n",
       " 'waiting',\n",
       " 'to',\n",
       " 'see',\n",
       " 'him',\n",
       " '?']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "word_tokenize(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def lemm_sentence(tokenized_sentence):\n",
    "    new_tokenized_sentence = []\n",
    "    for word in tokenized_sentence:\n",
    "        new_tokenized_sentence.append(lemm.lemmatize(word))\n",
    "    return new_tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_lemmatized = x_train_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "# x_test_lemmatized = x_test_tokenized.apply(lambda x: lemm_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "ps = PorterStemmer()\n",
    " \n",
    "def stem_sentence(lemmatized_sentence):\n",
    "    new_lemmatized_sentence = []\n",
    "    for word in lemmatized_sentence:\n",
    "        new_lemmatized_sentence.append(ps.stem(word))\n",
    "    return new_lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_stemmed = x_train_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "# x_test_stemmed = x_test_lemmatized.apply(lambda x: stem_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_sentence_again(stem_sentence):\n",
    "    new_norm_sentence = \"\"\n",
    "    for word in stem_sentence:\n",
    "        new_norm_sentence = new_norm_sentence + \" \" + word\n",
    "    return new_norm_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_to_vec = x_train_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "# x_test_to_vec = x_test_stemmed.apply(lambda x: norm_sentence_again(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigram_strings(string_sentence):\n",
    "    bigram_list = []\n",
    "    for word in range(0, len(string_sentence.split())-1):\n",
    "        unigram_1 = string_sentence.split()[word]\n",
    "        unigram_2 = string_sentence.split()[word+1]\n",
    "        bigram_list.append(unigram_1 + \"_\" + unigram_2)\n",
    "    bigram_sentence = \"\"\n",
    "    for bigram in bigram_list:\n",
    "        bigram_sentence = bigram_sentence + \" \" + bigram\n",
    "    return bigram_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ratings_to_vectors(x_train, y_train, x_test, y_test):\n",
    "    \n",
    "    # clean them\n",
    "    inner_x_train = x_train.apply(lambda x: clean_word(x))\n",
    "    inner_x_test = x_test.apply(lambda x: clean_word(x))\n",
    "\n",
    "    # tokenize\n",
    "    inner_x_train_tokenized = inner_x_train.apply(lambda x: word_tokenize(x))\n",
    "    inner_x_test_tokenized = inner_x_test.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "    # lemmatize\n",
    "    lemm = WordNetLemmatizer()\n",
    "    inner_x_train_lemmatized = inner_x_train_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    inner_x_test_lemmatized = inner_x_test_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    \n",
    "    # stem\n",
    "    ps = PorterStemmer()\n",
    "    inner_x_train_stemmed = inner_x_train_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    inner_x_test_stemmed = inner_x_test_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    \n",
    "    # return to non-vectorized-list so we can manipulate\n",
    "    inner_x_train_to_vec = inner_x_train_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "    inner_x_test_to_vec = inner_x_test_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "    \n",
    "    # now count vectorize\n",
    "    cv = CountVectorizer()\n",
    "    inner_x_train_vec = cv.fit_transform(pd.Series(inner_x_train_to_vec)).toarray()\n",
    "    inner_x_test_vec = cv.transform(pd.Series(inner_x_test_to_vec)).toarray()\n",
    "\n",
    "    return inner_x_train_vec, y_train, inner_x_test_vec, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ratings_to_vectors_bigram(x_train, y_train, x_test, y_test):\n",
    "    # clean them\n",
    "    inner_x_train = x_train.apply(lambda x: clean_word(x))\n",
    "    inner_x_test = x_test.apply(lambda x: clean_word(x))\n",
    "\n",
    "    # tokenize\n",
    "    inner_x_train_tokenized = inner_x_train.apply(lambda x: word_tokenize(x))\n",
    "    inner_x_test_tokenized = inner_x_test.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "    # lemmatize\n",
    "    lemm = WordNetLemmatizer()\n",
    "    inner_x_train_lemmatized = inner_x_train_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    inner_x_test_lemmatized = inner_x_test_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    \n",
    "    # stem\n",
    "    ps = PorterStemmer()\n",
    "    inner_x_train_stemmed = inner_x_train_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    inner_x_test_stemmed = inner_x_test_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    \n",
    "    # return to non-vectorized-list so we can manipulate\n",
    "    inner_x_train_to_vec = inner_x_train_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "    inner_x_test_to_vec = inner_x_test_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "    \n",
    "    # convert this guy to a bigrammized sentence\n",
    "    bigrammed_sentence_x_train = inner_x_train_to_vec.apply(lambda x: create_bigram_strings(x))\n",
    "    bigrammed_sentence_x_test  = inner_x_test_to_vec.apply(lambda x: create_bigram_strings(x))\n",
    "    \n",
    "    # make them one big sentence:\n",
    "    comb_inner_x_train_to_vec = inner_x_train_to_vec + bigrammed_sentence_x_train\n",
    "    comb_inner_x_test_to_vec = inner_x_test_to_vec + bigrammed_sentence_x_test\n",
    "    \n",
    "    # now count vectorize\n",
    "    cv = CountVectorizer()\n",
    "    inner_x_train_vec = cv.fit_transform(pd.Series(comb_inner_x_train_to_vec)).toarray()\n",
    "    inner_x_test_vec = cv.transform(pd.Series(comb_inner_x_test_to_vec)).toarray()\n",
    "\n",
    "    return inner_x_train_vec, y_train, inner_x_test_vec, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bi_uni_df = train_ratings_to_vectors_bigram(x_train, y_train_gen, x_test, y_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df = train_ratings_to_vectors(x_train, y_train_gen, x_test, y_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_eval_metrics(trained_model, x_train, y_train, x_test, y_test):\n",
    "   \n",
    "    # vectorized usable versions only of x train\n",
    "    \n",
    "    train_fit = trained_model.score(x_train, y_train)\n",
    "    print(\"Train fit: \", train_fit)\n",
    "    test_fit = trained_model.score(x_test, y_test)\n",
    "    print(\"Test fit: \", test_fit)\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    y_predictions = trained_model.predict(x_test)\n",
    "\n",
    "    four_outputs = prec_reca_f1(y_test, y_predictions)\n",
    "    print(\"Precision: \", four_outputs[0])\n",
    "    print(\"Recall: \", four_outputs[1])\n",
    "    print(\"F1: \", four_outputs[2])\n",
    "    print(\"F0.5: \", four_outputs[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix in the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we have to redo the tokenization because we want it to be in a different format to compute cosine similarity, we will not use these after we have the dictionary cosine similarity vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def lemm_sentence(tokenized_sentence):\n",
    "    new_tokenized_sentence = []\n",
    "    for word in tokenized_sentence:\n",
    "        new_tokenized_sentence.append(lemm.lemmatize(word))\n",
    "    return new_tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "ps = PorterStemmer()\n",
    " \n",
    "def stem_sentence(lemmatized_sentence):\n",
    "    new_lemmatized_sentence = []\n",
    "    for word in lemmatized_sentence:\n",
    "        new_lemmatized_sentence.append(ps.stem(word))\n",
    "    return new_lemmatized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_sentence_again(stem_sentence):\n",
    "    new_norm_sentence = \"\"\n",
    "    for word in stem_sentence:\n",
    "        new_norm_sentence = new_norm_sentence + \" \" + word\n",
    "    return new_norm_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ratings_to_vectors_STEMMED(x_train, y_train, x_test, y_test):\n",
    "    # clean them\n",
    "    inner_x_train = x_train.apply(lambda x: clean_word(x))\n",
    "    inner_x_test = x_test.apply(lambda x: clean_word(x))\n",
    "\n",
    "    # tokenize\n",
    "    inner_x_train_tokenized = inner_x_train.apply(lambda x: word_tokenize(x))\n",
    "    inner_x_test_tokenized = inner_x_test.apply(lambda x: word_tokenize(x))\n",
    "\n",
    "    # lemmatize\n",
    "    lemm = WordNetLemmatizer()\n",
    "    inner_x_train_lemmatized = inner_x_train_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    inner_x_test_lemmatized = inner_x_test_tokenized.apply(lambda x: lemm_sentence(x))\n",
    "    \n",
    "    # stem\n",
    "    ps = PorterStemmer()\n",
    "    inner_x_train_stemmed = inner_x_train_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    inner_x_test_stemmed = inner_x_test_lemmatized.apply(lambda x: stem_sentence(x))\n",
    "    \n",
    "#     # return to non-vectorized-list so we can manipulate\n",
    "#     inner_x_train_to_vec = inner_x_train_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "#     inner_x_test_to_vec = inner_x_test_stemmed.apply(lambda x: norm_sentence_again(x))\n",
    "    \n",
    "#     # now count vectorize\n",
    "#     cv = CountVectorizer(max_features = 1500)\n",
    "#     inner_x_train_vec = cv.fit_transform(pd.Series(inner_x_train_to_vec)).toarray()\n",
    "#     inner_x_test_vec = cv.transform(pd.Series(inner_x_test_to_vec)).toarray()\n",
    "\n",
    "#    return inner_x_train_vec, inner_y_train, inner_x_test_vec, inner_y_test\n",
    "    return inner_x_train_stemmed, y_train, inner_x_test_stemmed, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df_for_str = train_ratings_to_vectors_STEMMED(x_train, y_train_gen, x_test, y_test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_to_string(tokenized_sent):\n",
    "    new_sent = \"\"\n",
    "    for word in tokenized_sent:\n",
    "        new_sent = new_sent + \" \" + word\n",
    "    return new_sent[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df_str = (standard_df_for_str[0].apply(tokenized_to_string),\n",
    "                   standard_df_for_str[1],\n",
    "                   standard_df_for_str[2].apply(tokenized_to_string),\n",
    "                   standard_df_for_str[3],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make covariate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_dict_pd = pd.read_excel('data/LIWC2007dictionary_cleaned.xls', sheet_name = \"Cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Quant', 'Numbers', 'Humans', 'Affect', 'Posemo', 'Negemo', 'Cause',\n",
       "       'Health', 'Money', 'Death', 'Ipron', 'aux_verb', 'adverbs', 'Negate',\n",
       "       'Family', 'Anx', 'Anger', 'Sad', 'CogMech', 'Insight', 'Discrep',\n",
       "       'Tentat', 'Certain', 'Inhib', 'Incl', 'Excl', 'Bio', 'Body', 'Sexual',\n",
       "       'Time', 'Achiev'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_dict_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in mass_dict_pd.columns:\n",
    "    mass_dict[col]  = []\n",
    "    for word in range(0, len(mass_dict_pd[col])):\n",
    "        if pd.notna(mass_dict_pd[col][word]):\n",
    "            clean_word = mass_dict_pd[col][word].replace(\"*\", \"\")\n",
    "            mass_dict[col].append(clean_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Quant': ['all',\n",
       "  'allot',\n",
       "  'alot',\n",
       "  'amount',\n",
       "  'another',\n",
       "  'any',\n",
       "  'anymore',\n",
       "  'besides',\n",
       "  'best ',\n",
       "  'bit ',\n",
       "  'bits',\n",
       "  'both',\n",
       "  'bunch',\n",
       "  'cetera',\n",
       "  'couple',\n",
       "  'difference',\n",
       "  'doubl',\n",
       "  'each',\n",
       "  'either',\n",
       "  'else',\n",
       "  'enough',\n",
       "  'entire',\n",
       "  'equal',\n",
       "  'etc',\n",
       "  'every',\n",
       "  'example',\n",
       "  'extent',\n",
       "  'extra',\n",
       "  'extremely',\n",
       "  'fairly',\n",
       "  'few',\n",
       "  'form',\n",
       "  'full',\n",
       "  'greater',\n",
       "  'greatest',\n",
       "  'highly',\n",
       "  'increas',\n",
       "  'item',\n",
       "  'lack',\n",
       "  'least',\n",
       "  'less',\n",
       "  'loads',\n",
       "  'lot',\n",
       "  'lotof',\n",
       "  'lots',\n",
       "  'lotsa',\n",
       "  'lotta',\n",
       "  'main',\n",
       "  'major',\n",
       "  'majority',\n",
       "  'many',\n",
       "  'more',\n",
       "  'most',\n",
       "  'much',\n",
       "  'mucho',\n",
       "  'neither',\n",
       "  'none',\n",
       "  'ones',\n",
       "  'own',\n",
       "  'page',\n",
       "  'part',\n",
       "  'partly',\n",
       "  'percent',\n",
       "  'piec',\n",
       "  'plenty',\n",
       "  'portion',\n",
       "  'remaining',\n",
       "  'rest',\n",
       "  'same',\n",
       "  'section',\n",
       "  'segment',\n",
       "  'selection',\n",
       "  'series',\n",
       "  'several',\n",
       "  'significant',\n",
       "  'simple',\n",
       "  'singl',\n",
       "  'some',\n",
       "  'somewhat',\n",
       "  'term',\n",
       "  'ton',\n",
       "  'tons',\n",
       "  'total',\n",
       "  'tripl',\n",
       "  'unique',\n",
       "  'various',\n",
       "  'version',\n",
       "  'whole',\n",
       "  'worst'],\n",
       " 'Numbers': ['billion',\n",
       "  'dozen',\n",
       "  'eight',\n",
       "  'eleven',\n",
       "  'fift',\n",
       "  'first',\n",
       "  'firstly',\n",
       "  'firsts',\n",
       "  'five',\n",
       "  'four',\n",
       "  'half',\n",
       "  'hundred',\n",
       "  'infinit',\n",
       "  'million',\n",
       "  'nine',\n",
       "  'once',\n",
       "  'one',\n",
       "  'quarter',\n",
       "  'second',\n",
       "  'seven',\n",
       "  'six',\n",
       "  'ten',\n",
       "  'tenth',\n",
       "  'third',\n",
       "  'thirt',\n",
       "  'thousand',\n",
       "  'three',\n",
       "  'trillion',\n",
       "  'twel',\n",
       "  'twent',\n",
       "  'twice',\n",
       "  'two',\n",
       "  'zero',\n",
       "  'zillion'],\n",
       " 'Humans': ['adult',\n",
       "  'adults',\n",
       "  'babe',\n",
       "  'babies',\n",
       "  'baby',\n",
       "  'bambino',\n",
       "  'boy',\n",
       "  \"boy's\",\n",
       "  'boys',\n",
       "  'chick',\n",
       "  \"chick'\",\n",
       "  'chicks',\n",
       "  'child',\n",
       "  'children',\n",
       "  \"child's\",\n",
       "  'citizen',\n",
       "  \"citizen'\",\n",
       "  'citizens',\n",
       "  'female',\n",
       "  'gentlem',\n",
       "  'girl',\n",
       "  \"girl's\",\n",
       "  'girls',\n",
       "  'grownup',\n",
       "  'guy',\n",
       "  'human',\n",
       "  'individual',\n",
       "  'infant',\n",
       "  \"infant's\",\n",
       "  'infants',\n",
       "  'kid',\n",
       "  \"kid'\",\n",
       "  'kids',\n",
       "  'ladies',\n",
       "  'lady',\n",
       "  \"lady's\",\n",
       "  \"ma'am\",\n",
       "  'male',\n",
       "  'males',\n",
       "  \"male's\",\n",
       "  'mam',\n",
       "  'man',\n",
       "  \"man's\",\n",
       "  'members',\n",
       "  'men',\n",
       "  \"men'\",\n",
       "  'mr ',\n",
       "  'mrs',\n",
       "  'newborn',\n",
       "  'participant',\n",
       "  'partner',\n",
       "  'people',\n",
       "  'person',\n",
       "  'persons',\n",
       "  \"person's\",\n",
       "  'self ',\n",
       "  'sir',\n",
       "  'societ',\n",
       "  'woman',\n",
       "  \"woman's\",\n",
       "  'women'],\n",
       " 'Affect': ['abandon',\n",
       "  'abuse',\n",
       "  'abusi',\n",
       "  'accept',\n",
       "  'accepta',\n",
       "  'accepted',\n",
       "  'accepting',\n",
       "  'accepts',\n",
       "  'ache',\n",
       "  'aching',\n",
       "  'active',\n",
       "  'admir',\n",
       "  'ador',\n",
       "  'advantag',\n",
       "  'adventur',\n",
       "  'advers',\n",
       "  'affection',\n",
       "  'afraid',\n",
       "  'aggravat',\n",
       "  'aggress',\n",
       "  'agitat',\n",
       "  'agoniz',\n",
       "  'agony',\n",
       "  'agree',\n",
       "  'agreeab',\n",
       "  'agreed',\n",
       "  'agreeing',\n",
       "  'agreement',\n",
       "  'agrees',\n",
       "  'alarm',\n",
       "  'alone',\n",
       "  'alright',\n",
       "  'amaz',\n",
       "  'amor',\n",
       "  'amus',\n",
       "  'anger',\n",
       "  'angr',\n",
       "  'anguish',\n",
       "  'annoy',\n",
       "  'antagoni',\n",
       "  'anxi',\n",
       "  'aok',\n",
       "  'apath',\n",
       "  'appall',\n",
       "  'appreciat',\n",
       "  'apprehens',\n",
       "  'argh',\n",
       "  'argu',\n",
       "  'arrogan',\n",
       "  'asham',\n",
       "  'assault',\n",
       "  'asshole',\n",
       "  'assur',\n",
       "  'attachment',\n",
       "  'attack',\n",
       "  'attract',\n",
       "  'aversi',\n",
       "  'avoid',\n",
       "  'award',\n",
       "  'awesome',\n",
       "  'awful',\n",
       "  'awkward',\n",
       "  'bad',\n",
       "  'bashful',\n",
       "  'bastard',\n",
       "  'battl',\n",
       "  'beaten',\n",
       "  'beaut',\n",
       "  'beloved',\n",
       "  'benefic',\n",
       "  'benefit',\n",
       "  'benefits',\n",
       "  'benefitt',\n",
       "  'benevolen',\n",
       "  'benign',\n",
       "  'best',\n",
       "  'better',\n",
       "  'bitch',\n",
       "  'bitter',\n",
       "  'blam',\n",
       "  'bless',\n",
       "  'bold',\n",
       "  'bonus',\n",
       "  'bore',\n",
       "  'boring',\n",
       "  'bother',\n",
       "  'brave',\n",
       "  'bright',\n",
       "  'brillian',\n",
       "  'broke',\n",
       "  'brutal',\n",
       "  'burden',\n",
       "  'calm',\n",
       "  'care',\n",
       "  'cared',\n",
       "  'carefree',\n",
       "  'careful',\n",
       "  'careless',\n",
       "  'cares',\n",
       "  'caring',\n",
       "  'casual',\n",
       "  'casually',\n",
       "  'certain',\n",
       "  'challeng',\n",
       "  'champ',\n",
       "  'charit',\n",
       "  'charm',\n",
       "  'cheat',\n",
       "  'cheer',\n",
       "  'cherish',\n",
       "  'chuckl',\n",
       "  'clever',\n",
       "  'comed',\n",
       "  'comfort',\n",
       "  'commitment',\n",
       "  'compassion',\n",
       "  'complain',\n",
       "  'compliment',\n",
       "  'concerned',\n",
       "  'confidence',\n",
       "  'confident',\n",
       "  'confidently',\n",
       "  'confront',\n",
       "  'confus',\n",
       "  'considerate',\n",
       "  'contempt',\n",
       "  'contented',\n",
       "  'contentment',\n",
       "  'contradic',\n",
       "  'convinc',\n",
       "  'cool',\n",
       "  'courag',\n",
       "  'crap',\n",
       "  'crappy',\n",
       "  'craz',\n",
       "  'create',\n",
       "  'creati',\n",
       "  'credit',\n",
       "  'cried',\n",
       "  'cries',\n",
       "  'critical',\n",
       "  'critici',\n",
       "  'crude',\n",
       "  'cruel',\n",
       "  'crushed',\n",
       "  'cry',\n",
       "  'crying',\n",
       "  'cunt',\n",
       "  'cut',\n",
       "  'cute',\n",
       "  'cutie',\n",
       "  'cynic',\n",
       "  'damag',\n",
       "  'damn',\n",
       "  'danger',\n",
       "  'daring',\n",
       "  'darlin',\n",
       "  'daze',\n",
       "  'dear',\n",
       "  'decay',\n",
       "  'defeat',\n",
       "  'defect',\n",
       "  'defenc',\n",
       "  'defens',\n",
       "  'definite',\n",
       "  'definitely',\n",
       "  'degrad',\n",
       "  'delectabl',\n",
       "  'delicate',\n",
       "  'delicious',\n",
       "  'deligh',\n",
       "  'depress',\n",
       "  'depriv',\n",
       "  'despair',\n",
       "  'desperat',\n",
       "  'despis',\n",
       "  'destroy',\n",
       "  'destruct',\n",
       "  'determina',\n",
       "  'determined',\n",
       "  'devastat',\n",
       "  'devil',\n",
       "  'devot',\n",
       "  'difficult',\n",
       "  'digni',\n",
       "  'disadvantage',\n",
       "  'disagree',\n",
       "  'disappoint',\n",
       "  'disaster',\n",
       "  'discomfort',\n",
       "  'discourag',\n",
       "  'disgust',\n",
       "  'dishearten',\n",
       "  'disillusion',\n",
       "  'dislike',\n",
       "  'disliked',\n",
       "  'dislikes',\n",
       "  'disliking',\n",
       "  'dismay',\n",
       "  'dissatisf',\n",
       "  'distract',\n",
       "  'distraught',\n",
       "  'distress',\n",
       "  'distrust',\n",
       "  'disturb',\n",
       "  'divin',\n",
       "  'domina',\n",
       "  'doom',\n",
       "  'dork',\n",
       "  'doubt',\n",
       "  'dread',\n",
       "  'dull',\n",
       "  'dumb',\n",
       "  'dump',\n",
       "  'dwell',\n",
       "  'dynam',\n",
       "  'eager',\n",
       "  'ease',\n",
       "  'easie',\n",
       "  'easily',\n",
       "  'easiness',\n",
       "  'easing',\n",
       "  'easy',\n",
       "  'ecsta',\n",
       "  'efficien',\n",
       "  'egotis',\n",
       "  'elegan',\n",
       "  'embarrass',\n",
       "  'emotion',\n",
       "  'emotion',\n",
       "  'emotional',\n",
       "  'empt',\n",
       "  'encourag',\n",
       "  'enemie',\n",
       "  'enemy',\n",
       "  'energ',\n",
       "  'engag',\n",
       "  'enjoy',\n",
       "  'enrag',\n",
       "  'entertain',\n",
       "  'enthus',\n",
       "  'envie',\n",
       "  'envious',\n",
       "  'envy',\n",
       "  'evil',\n",
       "  'excel',\n",
       "  'excit',\n",
       "  'excruciat',\n",
       "  'exhaust',\n",
       "  'fab',\n",
       "  'fabulous',\n",
       "  'fail',\n",
       "  'faith',\n",
       "  'fake',\n",
       "  'fantastic',\n",
       "  'fatal',\n",
       "  'fatigu',\n",
       "  'fault',\n",
       "  'favor',\n",
       "  'favour',\n",
       "  'fear',\n",
       "  'feared',\n",
       "  'fearful',\n",
       "  'fearing',\n",
       "  'fearless',\n",
       "  'fears',\n",
       "  'feroc',\n",
       "  'festiv',\n",
       "  'feud',\n",
       "  'fiery',\n",
       "  'fiesta',\n",
       "  'fight',\n",
       "  'fine',\n",
       "  'fired',\n",
       "  'flatter',\n",
       "  'flawless',\n",
       "  'flexib',\n",
       "  'flirt',\n",
       "  'flunk',\n",
       "  'foe',\n",
       "  'fond',\n",
       "  'fondly',\n",
       "  'fondness',\n",
       "  'fool',\n",
       "  'forbid',\n",
       "  'forgave',\n",
       "  'forgiv',\n",
       "  'fought',\n",
       "  'frantic',\n",
       "  'freak',\n",
       "  'free',\n",
       "  'freeb',\n",
       "  'freed',\n",
       "  'freeing',\n",
       "  'freely',\n",
       "  'freeness',\n",
       "  'freer',\n",
       "  'frees',\n",
       "  'friend',\n",
       "  'fright',\n",
       "  'frustrat',\n",
       "  'fuck',\n",
       "  'fucked',\n",
       "  'fucker',\n",
       "  'fuckin',\n",
       "  'fucks',\n",
       "  'fume',\n",
       "  'fuming',\n",
       "  'fun',\n",
       "  'funn',\n",
       "  'furious',\n",
       "  'fury',\n",
       "  'geek',\n",
       "  'genero',\n",
       "  'gentle',\n",
       "  'gentler',\n",
       "  'gentlest',\n",
       "  'gently',\n",
       "  'giggl',\n",
       "  'giver',\n",
       "  'giving',\n",
       "  'glad',\n",
       "  'gladly',\n",
       "  'glamor',\n",
       "  'glamour',\n",
       "  'gloom',\n",
       "  'glori',\n",
       "  'glory',\n",
       "  'goddam',\n",
       "  'good',\n",
       "  'goodness',\n",
       "  'gorgeous',\n",
       "  'gossip',\n",
       "  'grace',\n",
       "  'graced',\n",
       "  'graceful',\n",
       "  'graces',\n",
       "  'graci',\n",
       "  'grand',\n",
       "  'grande',\n",
       "  'gratef',\n",
       "  'grati',\n",
       "  'grave',\n",
       "  'great',\n",
       "  'greed',\n",
       "  'grief',\n",
       "  'griev',\n",
       "  'grim',\n",
       "  'grin',\n",
       "  'grinn',\n",
       "  'grins',\n",
       "  'gross',\n",
       "  'grouch',\n",
       "  'grr',\n",
       "  'guilt',\n",
       "  'ha',\n",
       "  'haha',\n",
       "  'handsom',\n",
       "  'happi',\n",
       "  'happy',\n",
       "  'harass',\n",
       "  'harm',\n",
       "  'harmed',\n",
       "  'harmful',\n",
       "  'harming',\n",
       "  'harmless',\n",
       "  'harmon',\n",
       "  'harms',\n",
       "  'hate',\n",
       "  'hated',\n",
       "  'hateful',\n",
       "  'hater',\n",
       "  'hates',\n",
       "  'hating',\n",
       "  'hatred',\n",
       "  'hazy',\n",
       "  'heartbreak',\n",
       "  'heartbroke',\n",
       "  'heartfelt',\n",
       "  'heartless',\n",
       "  'heartwarm',\n",
       "  'heaven',\n",
       "  'heh',\n",
       "  'hell',\n",
       "  'hellish',\n",
       "  'helper',\n",
       "  'helpful',\n",
       "  'helping',\n",
       "  'helpless',\n",
       "  'helps',\n",
       "  'hero',\n",
       "  'hesita',\n",
       "  'hilarious',\n",
       "  'hoho',\n",
       "  'homesick',\n",
       "  'honest',\n",
       "  'honor',\n",
       "  'honour',\n",
       "  'hope',\n",
       "  'hoped',\n",
       "  'hopeful',\n",
       "  'hopefully',\n",
       "  'hopefulness',\n",
       "  'hopeless',\n",
       "  'hopes',\n",
       "  'hoping',\n",
       "  'horr',\n",
       "  'hostil',\n",
       "  'hug',\n",
       "  'hugg',\n",
       "  'hugs',\n",
       "  'humiliat',\n",
       "  'humor',\n",
       "  'humour',\n",
       "  'hurra',\n",
       "  'hurt',\n",
       "  'ideal',\n",
       "  'idiot',\n",
       "  'ignor',\n",
       "  'immoral',\n",
       "  'impatien',\n",
       "  'impersonal',\n",
       "  'impolite',\n",
       "  'importan',\n",
       "  'impress',\n",
       "  'improve',\n",
       "  'improving',\n",
       "  'inadequa',\n",
       "  'incentive',\n",
       "  'indecis',\n",
       "  'ineffect',\n",
       "  'inferior ',\n",
       "  'inhib',\n",
       "  'innocen',\n",
       "  'insecur',\n",
       "  'insincer',\n",
       "  'inspir',\n",
       "  'insult',\n",
       "  'intell',\n",
       "  'interest',\n",
       "  'interrup',\n",
       "  'intimidat',\n",
       "  'invigor',\n",
       "  'irrational',\n",
       "  'irrita',\n",
       "  'isolat',\n",
       "  'jaded',\n",
       "  'jealous',\n",
       "  'jerk',\n",
       "  'jerked',\n",
       "  'jerks',\n",
       "  'joke',\n",
       "  'joking',\n",
       "  'joll',\n",
       "  'joy',\n",
       "  'keen',\n",
       "  'kidding',\n",
       "  'kill',\n",
       "  'kind',\n",
       "  'kindly',\n",
       "  'kindn',\n",
       "  'kiss',\n",
       "  'laidback',\n",
       "  'lame',\n",
       "  'laugh',\n",
       "  'lazie',\n",
       "  'lazy',\n",
       "  'liabilit',\n",
       "  'liar',\n",
       "  'libert',\n",
       "  'lied',\n",
       "  'lies',\n",
       "  'like',\n",
       "  'likeab',\n",
       "  'liked',\n",
       "  'likes',\n",
       "  'liking',\n",
       "  'livel',\n",
       "  'LMAO',\n",
       "  'LOL',\n",
       "  'lone',\n",
       "  'longing',\n",
       "  'lose',\n",
       "  'loser',\n",
       "  'loses',\n",
       "  'losing',\n",
       "  'loss',\n",
       "  'lost',\n",
       "  'lous',\n",
       "  'love',\n",
       "  'loved',\n",
       "  'lovely',\n",
       "  'lover',\n",
       "  'loves',\n",
       "  'loving',\n",
       "  'low',\n",
       "  'loyal',\n",
       "  'luck',\n",
       "  'lucked',\n",
       "  'lucki',\n",
       "  'luckless',\n",
       "  'lucks',\n",
       "  'lucky',\n",
       "  'ludicrous',\n",
       "  'lying',\n",
       "  'mad',\n",
       "  'maddening',\n",
       "  'madder',\n",
       "  'maddest',\n",
       "  'madly',\n",
       "  'magnific',\n",
       "  'maniac',\n",
       "  'masochis',\n",
       "  'melanchol',\n",
       "  'merit',\n",
       "  'merr',\n",
       "  'mess',\n",
       "  'messy',\n",
       "  'miser',\n",
       "  'miss',\n",
       "  'missed',\n",
       "  'misses',\n",
       "  'missing',\n",
       "  'mistak',\n",
       "  'mock',\n",
       "  'mocked',\n",
       "  'mocker',\n",
       "  'mocking',\n",
       "  'mocks',\n",
       "  'molest',\n",
       "  'mooch',\n",
       "  'mood',\n",
       "  'moodi',\n",
       "  'moods',\n",
       "  'moody',\n",
       "  'moron',\n",
       "  'mourn',\n",
       "  'murder',\n",
       "  'nag',\n",
       "  'nast',\n",
       "  'neat',\n",
       "  'needy',\n",
       "  'neglect',\n",
       "  'nerd',\n",
       "  'nervous',\n",
       "  'neurotic',\n",
       "  'nice',\n",
       "  'numb',\n",
       "  'nurtur',\n",
       "  'obnoxious',\n",
       "  'obsess',\n",
       "  'offence',\n",
       "  'offend',\n",
       "  'offens',\n",
       "  'ok',\n",
       "  'okay',\n",
       "  'okays',\n",
       "  'oks',\n",
       "  'openminded',\n",
       "  'openness',\n",
       "  'opportun',\n",
       "  'optimal',\n",
       "  'optimi',\n",
       "  'original',\n",
       "  'outgoing',\n",
       "  'outrag',\n",
       "  'overwhelm',\n",
       "  'pain',\n",
       "  'pained',\n",
       "  'painf',\n",
       "  'paining',\n",
       "  'painl',\n",
       "  'pains',\n",
       "  'palatabl',\n",
       "  'panic',\n",
       "  'paradise',\n",
       "  'paranoi',\n",
       "  'partie',\n",
       "  'party',\n",
       "  'passion',\n",
       "  'pathetic',\n",
       "  'peace',\n",
       "  'peculiar',\n",
       "  'perfect',\n",
       "  'personal',\n",
       "  'perver',\n",
       "  'pessimis',\n",
       "  'petrif',\n",
       "  'pettie',\n",
       "  'petty',\n",
       "  'phobi',\n",
       "  'piss',\n",
       "  'piti',\n",
       "  'pity ',\n",
       "  'play',\n",
       "  'played',\n",
       "  'playful',\n",
       "  'playing',\n",
       "  'plays',\n",
       "  'pleasant',\n",
       "  'please',\n",
       "  'pleasing',\n",
       "  'pleasur',\n",
       "  'poison',\n",
       "  'popular',\n",
       "  'positiv',\n",
       "  'prais',\n",
       "  'precious',\n",
       "  'prejudic',\n",
       "  'pressur',\n",
       "  'prettie',\n",
       "  'pretty',\n",
       "  'prick',\n",
       "  'pride',\n",
       "  'privileg',\n",
       "  'prize',\n",
       "  'problem',\n",
       "  'profit',\n",
       "  'promis',\n",
       "  'protest',\n",
       "  'protested',\n",
       "  'protesting',\n",
       "  'proud',\n",
       "  'puk',\n",
       "  'punish',\n",
       "  'radian',\n",
       "  'rage',\n",
       "  'raging',\n",
       "  'rancid',\n",
       "  'rape',\n",
       "  'raping',\n",
       "  'rapist',\n",
       "  'readiness',\n",
       "  'ready',\n",
       "  'reassur',\n",
       "  'rebel',\n",
       "  'reek',\n",
       "  'regret',\n",
       "  'reject',\n",
       "  'relax',\n",
       "  'relief',\n",
       "  'reliev',\n",
       "  'reluctan',\n",
       "  'remorse',\n",
       "  'repress',\n",
       "  'resent',\n",
       "  'resign',\n",
       "  'resolv',\n",
       "  'respect ',\n",
       "  'restless',\n",
       "  'revenge',\n",
       "  'revigor',\n",
       "  'reward',\n",
       "  'rich',\n",
       "  'ridicul',\n",
       "  'rigid',\n",
       "  'risk',\n",
       "  'ROFL',\n",
       "  'romanc',\n",
       "  'romantic',\n",
       "  'rotten',\n",
       "  'rude',\n",
       "  'ruin',\n",
       "  'sad',\n",
       "  'sadde',\n",
       "  'sadly',\n",
       "  'sadness',\n",
       "  'safe',\n",
       "  'sarcas',\n",
       "  'satisf',\n",
       "  'savage',\n",
       "  'save',\n",
       "  'scare',\n",
       "  'scaring',\n",
       "  'scary',\n",
       "  'sceptic',\n",
       "  'scream',\n",
       "  'screw',\n",
       "  'secur',\n",
       "  'selfish',\n",
       "  'sentimental',\n",
       "  'serious',\n",
       "  'seriously',\n",
       "  'seriousness',\n",
       "  'severe',\n",
       "  'shake',\n",
       "  'shaki',\n",
       "  'shaky',\n",
       "  'shame',\n",
       "  'share',\n",
       "  'shared',\n",
       "  'shares',\n",
       "  'sharing',\n",
       "  'shit',\n",
       "  'shock',\n",
       "  'shook',\n",
       "  'shy',\n",
       "  'sicken',\n",
       "  'sigh',\n",
       "  'sighed',\n",
       "  'sighing',\n",
       "  'sighs',\n",
       "  'silli',\n",
       "  'silly',\n",
       "  'sin',\n",
       "  'sincer',\n",
       "  'sinister',\n",
       "  'sins',\n",
       "  'skeptic',\n",
       "  'slut',\n",
       "  'smart',\n",
       "  'smil',\n",
       "  'smother',\n",
       "  'smug',\n",
       "  'snob',\n",
       "  'sob',\n",
       "  'sobbed',\n",
       "  'sobbing',\n",
       "  'sobs',\n",
       "  'sociab',\n",
       "  'solemn',\n",
       "  'sorrow',\n",
       "  'sorry',\n",
       "  'soulmate',\n",
       "  'special',\n",
       "  'spite',\n",
       "  'splend',\n",
       "  'stammer',\n",
       "  'stank',\n",
       "  'startl',\n",
       "  'steal',\n",
       "  'stench',\n",
       "  'stink',\n",
       "  'strain',\n",
       "  'strange',\n",
       "  'strength',\n",
       "  'stress',\n",
       "  'strong',\n",
       "  'struggl',\n",
       "  'stubborn',\n",
       "  'stunk',\n",
       "  'stunned',\n",
       "  'stuns',\n",
       "  'stupid',\n",
       "  'stutter',\n",
       "  'submissive',\n",
       "  'succeed',\n",
       "  'success',\n",
       "  'suck',\n",
       "  'sucked',\n",
       "  'sucker',\n",
       "  'sucks',\n",
       "  'sucky',\n",
       "  'suffer',\n",
       "  'suffered',\n",
       "  'sufferer',\n",
       "  'suffering',\n",
       "  'suffers',\n",
       "  'sunnier',\n",
       "  'sunniest',\n",
       "  'sunny',\n",
       "  'sunshin',\n",
       "  'super',\n",
       "  'superior',\n",
       "  'support',\n",
       "  'supported',\n",
       "  'supporter',\n",
       "  'supporting',\n",
       "  'supportive',\n",
       "  'supports',\n",
       "  'suprem',\n",
       "  'sure',\n",
       "  'surpris',\n",
       "  'suspicio',\n",
       "  'sweet',\n",
       "  'sweetheart',\n",
       "  'sweetie',\n",
       "  'sweetly',\n",
       "  'sweetness',\n",
       "  'sweets',\n",
       "  'talent',\n",
       "  'tantrum',\n",
       "  'tears',\n",
       "  'teas',\n",
       "  'tehe',\n",
       "  'temper',\n",
       "  'tempers',\n",
       "  'tender',\n",
       "  'tense',\n",
       "  'tensing',\n",
       "  'tension',\n",
       "  'terribl',\n",
       "  'terrific',\n",
       "  'terrified',\n",
       "  'terrifies',\n",
       "  'terrify ',\n",
       "  'terrifying',\n",
       "  'terror',\n",
       "  'thank',\n",
       "  'thanked',\n",
       "  'thankf',\n",
       "  'thanks',\n",
       "  'thief',\n",
       "  'thieve',\n",
       "  'thoughtful',\n",
       "  'threat',\n",
       "  'thrill',\n",
       "  'ticked',\n",
       "  'timid',\n",
       "  'toleran',\n",
       "  'tortur',\n",
       "  'tough',\n",
       "  'traged',\n",
       "  'tragic ',\n",
       "  'tranquil',\n",
       "  'trauma',\n",
       "  'treasur',\n",
       "  'treat',\n",
       "  'trembl',\n",
       "  'trick',\n",
       "  'trite',\n",
       "  'triumph',\n",
       "  'trivi',\n",
       "  'troubl',\n",
       "  'true ',\n",
       "  'trueness',\n",
       "  'truer',\n",
       "  'truest',\n",
       "  'truly',\n",
       "  'trust',\n",
       "  'truth',\n",
       "  'turmoil',\n",
       "  'ugh',\n",
       "  'ugl',\n",
       "  'unattractive',\n",
       "  'uncertain',\n",
       "  'uncomfortabl',\n",
       "  'uncontrol',\n",
       "  'uneas',\n",
       "  'unfortunate',\n",
       "  'unfriendly',\n",
       "  'ungrateful',\n",
       "  'unhapp',\n",
       "  'unimportant',\n",
       "  'unimpress',\n",
       "  'unkind',\n",
       "  'unlov',\n",
       "  'unpleasant',\n",
       "  'unprotected',\n",
       "  'unsavo',\n",
       "  'unsuccessful',\n",
       "  'unsure',\n",
       "  'unwelcom',\n",
       "  'upset',\n",
       "  'uptight',\n",
       "  'useful',\n",
       "  'useless ',\n",
       "  'vain',\n",
       "  'valuabl',\n",
       "  'value',\n",
       "  'valued',\n",
       "  'values',\n",
       "  'valuing',\n",
       "  'vanity',\n",
       "  'vicious',\n",
       "  'victim',\n",
       "  'vigor',\n",
       "  'vigour',\n",
       "  'vile',\n",
       "  'villain',\n",
       "  'violat',\n",
       "  'violent',\n",
       "  'virtue',\n",
       "  'virtuo',\n",
       "  'vital',\n",
       "  'vulnerab',\n",
       "  'vulture',\n",
       "  'war',\n",
       "  'warfare',\n",
       "  'warm',\n",
       "  'warred',\n",
       "  'warring',\n",
       "  'wars',\n",
       "  'weak',\n",
       "  'wealth',\n",
       "  'weapon',\n",
       "  'weep',\n",
       "  'weird',\n",
       "  'welcom',\n",
       "  'well',\n",
       "  'wept',\n",
       "  'whine',\n",
       "  'whining',\n",
       "  'whore',\n",
       "  'wicked',\n",
       "  'willing',\n",
       "  'wimp',\n",
       "  'win',\n",
       "  'winn',\n",
       "  'wins',\n",
       "  'wisdom',\n",
       "  'wise',\n",
       "  'witch',\n",
       "  'woe',\n",
       "  'won',\n",
       "  'wonderf',\n",
       "  'worr',\n",
       "  'worse',\n",
       "  'worship',\n",
       "  'worst',\n",
       "  'worthless ',\n",
       "  'worthwhile',\n",
       "  'wow',\n",
       "  'wrong',\n",
       "  'yay',\n",
       "  'yays',\n",
       "  'yearn'],\n",
       " 'Posemo': ['accept',\n",
       "  'accepta',\n",
       "  'accepted',\n",
       "  'accepting',\n",
       "  'accepts',\n",
       "  'active',\n",
       "  'admir',\n",
       "  'ador',\n",
       "  'advantag',\n",
       "  'adventur',\n",
       "  'affection',\n",
       "  'agree',\n",
       "  'agreeab',\n",
       "  'agreed',\n",
       "  'agreeing',\n",
       "  'agreement',\n",
       "  'agrees',\n",
       "  'alright',\n",
       "  'amaz',\n",
       "  'amor',\n",
       "  'amus',\n",
       "  'aok',\n",
       "  'appreciat',\n",
       "  'assur',\n",
       "  'attachment',\n",
       "  'attract',\n",
       "  'award',\n",
       "  'awesome',\n",
       "  'beaut',\n",
       "  'beloved',\n",
       "  'benefic',\n",
       "  'benefit',\n",
       "  'benefits',\n",
       "  'benefitt',\n",
       "  'benevolen',\n",
       "  'benign',\n",
       "  'best',\n",
       "  'better',\n",
       "  'bless',\n",
       "  'bold',\n",
       "  'bonus',\n",
       "  'brave',\n",
       "  'bright',\n",
       "  'brillian',\n",
       "  'calm',\n",
       "  'care',\n",
       "  'cared',\n",
       "  'carefree',\n",
       "  'careful',\n",
       "  'cares',\n",
       "  'caring',\n",
       "  'casual',\n",
       "  'casually',\n",
       "  'certain',\n",
       "  'challeng',\n",
       "  'champ',\n",
       "  'charit',\n",
       "  'charm',\n",
       "  'cheer',\n",
       "  'cherish',\n",
       "  'chuckl',\n",
       "  'clever',\n",
       "  'comed',\n",
       "  'comfort',\n",
       "  'commitment',\n",
       "  'compassion',\n",
       "  'compliment',\n",
       "  'confidence',\n",
       "  'confident',\n",
       "  'confidently',\n",
       "  'considerate',\n",
       "  'contented',\n",
       "  'contentment',\n",
       "  'convinc',\n",
       "  'cool',\n",
       "  'courag',\n",
       "  'create',\n",
       "  'creati',\n",
       "  'credit',\n",
       "  'cute',\n",
       "  'cutie',\n",
       "  'daring',\n",
       "  'darlin',\n",
       "  'dear',\n",
       "  'definite',\n",
       "  'definitely',\n",
       "  'delectabl',\n",
       "  'delicate',\n",
       "  'delicious',\n",
       "  'deligh',\n",
       "  'determina',\n",
       "  'determined',\n",
       "  'devot',\n",
       "  'digni',\n",
       "  'divin',\n",
       "  'dynam',\n",
       "  'eager',\n",
       "  'ease',\n",
       "  'easie',\n",
       "  'easily',\n",
       "  'easiness',\n",
       "  'easing',\n",
       "  'easy',\n",
       "  'ecsta',\n",
       "  'efficien',\n",
       "  'elegan',\n",
       "  'encourag',\n",
       "  'energ',\n",
       "  'engag',\n",
       "  'enjoy',\n",
       "  'entertain',\n",
       "  'enthus',\n",
       "  'excel',\n",
       "  'excit',\n",
       "  'fab',\n",
       "  'fabulous',\n",
       "  'faith',\n",
       "  'fantastic',\n",
       "  'favor',\n",
       "  'favour',\n",
       "  'fearless',\n",
       "  'festiv',\n",
       "  'fiesta',\n",
       "  'fine',\n",
       "  'flatter',\n",
       "  'flawless',\n",
       "  'flexib',\n",
       "  'flirt',\n",
       "  'fond',\n",
       "  'fondly',\n",
       "  'fondness',\n",
       "  'forgave',\n",
       "  'forgiv',\n",
       "  'free',\n",
       "  'free',\n",
       "  'freeb',\n",
       "  'freed',\n",
       "  'freeing',\n",
       "  'freely',\n",
       "  'freeness',\n",
       "  'freer',\n",
       "  'frees',\n",
       "  'friend',\n",
       "  'fun',\n",
       "  'funn',\n",
       "  'genero',\n",
       "  'gentle',\n",
       "  'gentler',\n",
       "  'gentlest',\n",
       "  'gently',\n",
       "  'giggl',\n",
       "  'giver',\n",
       "  'giving',\n",
       "  'glad',\n",
       "  'gladly',\n",
       "  'glamor',\n",
       "  'glamour',\n",
       "  'glori',\n",
       "  'glory',\n",
       "  'good',\n",
       "  'goodness',\n",
       "  'gorgeous',\n",
       "  'grace',\n",
       "  'graced',\n",
       "  'graceful',\n",
       "  'graces',\n",
       "  'graci',\n",
       "  'grand',\n",
       "  'grande',\n",
       "  'gratef',\n",
       "  'grati',\n",
       "  'great',\n",
       "  'grin',\n",
       "  'grinn',\n",
       "  'grins',\n",
       "  'ha',\n",
       "  'haha',\n",
       "  'handsom',\n",
       "  'happi',\n",
       "  'happy',\n",
       "  'harmless',\n",
       "  'harmon',\n",
       "  'heartfelt',\n",
       "  'heartwarm',\n",
       "  'heaven',\n",
       "  'heh',\n",
       "  'helper',\n",
       "  'helpful',\n",
       "  'helping',\n",
       "  'helps',\n",
       "  'hero',\n",
       "  'hilarious',\n",
       "  'hoho',\n",
       "  'honest',\n",
       "  'honor',\n",
       "  'honour',\n",
       "  'hope',\n",
       "  'hoped',\n",
       "  'hopeful',\n",
       "  'hopefully',\n",
       "  'hopefulness',\n",
       "  'hopes',\n",
       "  'hoping',\n",
       "  'hug ',\n",
       "  'hugg',\n",
       "  'hugs',\n",
       "  'humor',\n",
       "  'humour',\n",
       "  'hurra',\n",
       "  'ideal',\n",
       "  'importan',\n",
       "  'impress',\n",
       "  'improve',\n",
       "  'improving',\n",
       "  'incentive',\n",
       "  'innocen',\n",
       "  'inspir',\n",
       "  'intell',\n",
       "  'interest',\n",
       "  'invigor',\n",
       "  'joke',\n",
       "  'joking',\n",
       "  'joll',\n",
       "  'joy',\n",
       "  'keen',\n",
       "  'kidding',\n",
       "  'kind',\n",
       "  'kindly',\n",
       "  'kindn',\n",
       "  'kiss',\n",
       "  'laidback',\n",
       "  'laugh',\n",
       "  'libert',\n",
       "  'like',\n",
       "  'likeab',\n",
       "  'liked',\n",
       "  'likes',\n",
       "  'liking',\n",
       "  'livel',\n",
       "  'LMAO',\n",
       "  'LOL',\n",
       "  'love',\n",
       "  'loved',\n",
       "  'lovely',\n",
       "  'lover',\n",
       "  'loves',\n",
       "  'loving',\n",
       "  'loyal',\n",
       "  'luck',\n",
       "  'lucked',\n",
       "  'lucki',\n",
       "  'lucks',\n",
       "  'lucky',\n",
       "  'madly',\n",
       "  'magnific',\n",
       "  'merit',\n",
       "  'merr',\n",
       "  'neat',\n",
       "  'nice',\n",
       "  'nurtur',\n",
       "  'ok',\n",
       "  'okay',\n",
       "  'okays',\n",
       "  'oks',\n",
       "  'openminded',\n",
       "  'openness',\n",
       "  'opportun',\n",
       "  'optimal',\n",
       "  'optimi',\n",
       "  'original',\n",
       "  'outgoing',\n",
       "  'painl',\n",
       "  'palatabl',\n",
       "  'paradise',\n",
       "  'partie',\n",
       "  'party',\n",
       "  'passion',\n",
       "  'peace',\n",
       "  'perfect',\n",
       "  'play',\n",
       "  'played',\n",
       "  'playful',\n",
       "  'playing',\n",
       "  'plays',\n",
       "  'pleasant',\n",
       "  'please',\n",
       "  'pleasing',\n",
       "  'pleasur',\n",
       "  'popular',\n",
       "  'positiv',\n",
       "  'prais',\n",
       "  'precious',\n",
       "  'prettie',\n",
       "  'pretty',\n",
       "  'pride',\n",
       "  'privileg',\n",
       "  'prize',\n",
       "  'profit',\n",
       "  'promis',\n",
       "  'proud',\n",
       "  'radian',\n",
       "  'readiness',\n",
       "  'ready',\n",
       "  'reassur',\n",
       "  'relax',\n",
       "  'relief',\n",
       "  'reliev',\n",
       "  'resolv',\n",
       "  'respect ',\n",
       "  'revigor',\n",
       "  'reward',\n",
       "  'rich',\n",
       "  'ROFL',\n",
       "  'romanc',\n",
       "  'romantic',\n",
       "  'safe',\n",
       "  'satisf',\n",
       "  'save',\n",
       "  'scrumptious',\n",
       "  'secur',\n",
       "  'sentimental',\n",
       "  'share',\n",
       "  'shared',\n",
       "  'shares',\n",
       "  'sharing',\n",
       "  'silli',\n",
       "  'silly',\n",
       "  'sincer',\n",
       "  'smart',\n",
       "  'smil',\n",
       "  'sociab',\n",
       "  'soulmate',\n",
       "  'special',\n",
       "  'splend',\n",
       "  'strength',\n",
       "  'strong',\n",
       "  'succeed',\n",
       "  'success',\n",
       "  'sunnier',\n",
       "  'sunniest',\n",
       "  'sunny',\n",
       "  'sunshin',\n",
       "  'super',\n",
       "  'superior',\n",
       "  'support',\n",
       "  'supported',\n",
       "  'supporter',\n",
       "  'supporting',\n",
       "  'supportive',\n",
       "  'supports',\n",
       "  'suprem',\n",
       "  'sure',\n",
       "  'surpris',\n",
       "  'sweet',\n",
       "  'sweetheart',\n",
       "  'sweetie',\n",
       "  'sweetly',\n",
       "  'sweetness',\n",
       "  'sweets',\n",
       "  'talent',\n",
       "  'tehe',\n",
       "  'tender',\n",
       "  'terrific',\n",
       "  'thank',\n",
       "  'thanked',\n",
       "  'thankf',\n",
       "  'thanks',\n",
       "  'thoughtful',\n",
       "  'thrill',\n",
       "  'toleran',\n",
       "  'tranquil',\n",
       "  'treasur',\n",
       "  'treat',\n",
       "  'triumph',\n",
       "  'true ',\n",
       "  'trueness',\n",
       "  'truer',\n",
       "  'truest',\n",
       "  'truly',\n",
       "  'trust',\n",
       "  'truth',\n",
       "  'useful',\n",
       "  'valuabl',\n",
       "  'value',\n",
       "  'valued',\n",
       "  'values',\n",
       "  'valuing',\n",
       "  'vigor',\n",
       "  'vigour',\n",
       "  'virtue',\n",
       "  'virtuo',\n",
       "  'vital',\n",
       "  'warm',\n",
       "  'wealth',\n",
       "  'welcom',\n",
       "  'well',\n",
       "  'win',\n",
       "  'winn',\n",
       "  'wins',\n",
       "  'wisdom',\n",
       "  'wise',\n",
       "  'won',\n",
       "  'wonderf',\n",
       "  'worship',\n",
       "  'worthwhile',\n",
       "  'wow',\n",
       "  'yay',\n",
       "  'yays'],\n",
       " 'Negemo': ['abandon',\n",
       "  'abuse',\n",
       "  'abusi',\n",
       "  'ache',\n",
       "  'aching',\n",
       "  'advers',\n",
       "  'afraid',\n",
       "  'aggravat',\n",
       "  'aggress',\n",
       "  'agitat',\n",
       "  'agoniz',\n",
       "  'agony',\n",
       "  'alarm',\n",
       "  'alone',\n",
       "  'anger',\n",
       "  'angr',\n",
       "  'anguish',\n",
       "  'annoy',\n",
       "  'antagoni',\n",
       "  'anxi',\n",
       "  'apath',\n",
       "  'appall',\n",
       "  'apprehens',\n",
       "  'argh',\n",
       "  'argu',\n",
       "  'arrogan',\n",
       "  'asham',\n",
       "  'assault',\n",
       "  'asshole',\n",
       "  'attack',\n",
       "  'aversi',\n",
       "  'avoid',\n",
       "  'awful',\n",
       "  'awkward',\n",
       "  'bad',\n",
       "  'bashful',\n",
       "  'bastard',\n",
       "  'battl',\n",
       "  'beaten',\n",
       "  'bitch',\n",
       "  'bitter',\n",
       "  'blam',\n",
       "  'bore',\n",
       "  'boring',\n",
       "  'bother',\n",
       "  'broke',\n",
       "  'brutal',\n",
       "  'burden',\n",
       "  'careless',\n",
       "  'cheat',\n",
       "  'complain',\n",
       "  'confront',\n",
       "  'confus',\n",
       "  'contempt',\n",
       "  'contradic',\n",
       "  'crap',\n",
       "  'crappy',\n",
       "  'craz',\n",
       "  'cried',\n",
       "  'cries',\n",
       "  'critical',\n",
       "  'critici',\n",
       "  'crude',\n",
       "  'cruel',\n",
       "  'crushed',\n",
       "  'cry',\n",
       "  'crying',\n",
       "  'cunt',\n",
       "  'cut',\n",
       "  'cynic',\n",
       "  'damag',\n",
       "  'damn',\n",
       "  'danger',\n",
       "  'daze',\n",
       "  'decay',\n",
       "  'defeat',\n",
       "  'defect',\n",
       "  'defenc',\n",
       "  'defens',\n",
       "  'degrad',\n",
       "  'depress',\n",
       "  'depriv',\n",
       "  'despair',\n",
       "  'desperat',\n",
       "  'despis',\n",
       "  'destroy',\n",
       "  'destruct',\n",
       "  'devastat',\n",
       "  'devil',\n",
       "  'difficult',\n",
       "  'disadvantage',\n",
       "  'disagree',\n",
       "  'disappoint',\n",
       "  'disaster',\n",
       "  'discomfort',\n",
       "  'discourag',\n",
       "  'disgust',\n",
       "  'dishearten',\n",
       "  'disillusion',\n",
       "  'dislike',\n",
       "  'disliked',\n",
       "  'dislikes',\n",
       "  'disliking',\n",
       "  'dismay',\n",
       "  'dissatisf',\n",
       "  'distract',\n",
       "  'distraught',\n",
       "  'distress',\n",
       "  'distrust',\n",
       "  'disturb',\n",
       "  'domina',\n",
       "  'doom',\n",
       "  'dork',\n",
       "  'doubt',\n",
       "  'dread',\n",
       "  'dull',\n",
       "  'dumb',\n",
       "  'dump',\n",
       "  'dwell',\n",
       "  'egotis',\n",
       "  'embarrass',\n",
       "  'emotional',\n",
       "  'empt',\n",
       "  'enemie',\n",
       "  'enemy',\n",
       "  'snob',\n",
       "  'sob',\n",
       "  'sobbed',\n",
       "  'sobbing',\n",
       "  'sobs',\n",
       "  'solemn',\n",
       "  'sorrow',\n",
       "  'sorry',\n",
       "  'spite',\n",
       "  'stammer',\n",
       "  'stank',\n",
       "  'startl',\n",
       "  'steal',\n",
       "  'stench',\n",
       "  'stink',\n",
       "  'strain',\n",
       "  'strange',\n",
       "  'stress',\n",
       "  'struggl',\n",
       "  'stubborn',\n",
       "  'stunk',\n",
       "  'stunned',\n",
       "  'stuns',\n",
       "  'stupid',\n",
       "  'stutter',\n",
       "  'submissive',\n",
       "  'suck',\n",
       "  'sucked',\n",
       "  'sucker',\n",
       "  'sucks',\n",
       "  'sucky',\n",
       "  'suffer',\n",
       "  'suffered',\n",
       "  'sufferer',\n",
       "  'suffering',\n",
       "  'suffers',\n",
       "  'suspicio',\n",
       "  'tantrum',\n",
       "  'tears',\n",
       "  'teas',\n",
       "  'temper',\n",
       "  'tempers',\n",
       "  'tense',\n",
       "  'tensing',\n",
       "  'tension',\n",
       "  'terribl',\n",
       "  'terrified',\n",
       "  'terrifies',\n",
       "  'terrify',\n",
       "  'terrifying',\n",
       "  'terror',\n",
       "  'thief',\n",
       "  'thieve',\n",
       "  'threat',\n",
       "  'ticked',\n",
       "  'timid',\n",
       "  'tortur',\n",
       "  'tough',\n",
       "  'traged',\n",
       "  'tragic ',\n",
       "  'trauma',\n",
       "  'trembl',\n",
       "  'trick',\n",
       "  'trite',\n",
       "  'trivi',\n",
       "  'troubl',\n",
       "  'turmoil',\n",
       "  'ugh',\n",
       "  'ugl',\n",
       "  'unattractive',\n",
       "  'uncertain',\n",
       "  'uncomfortabl',\n",
       "  'uncontrol',\n",
       "  'uneas',\n",
       "  'unfortunate',\n",
       "  'unfriendly',\n",
       "  'ungrateful',\n",
       "  'unhapp',\n",
       "  'unimportant',\n",
       "  'unimpress',\n",
       "  'unkind',\n",
       "  'unlov',\n",
       "  'unpleasant',\n",
       "  'unprotected',\n",
       "  'unsavo',\n",
       "  'unsuccessful',\n",
       "  'unsure',\n",
       "  'unwelcom',\n",
       "  'upset',\n",
       "  'uptight',\n",
       "  'useless ',\n",
       "  'vain',\n",
       "  'vanity',\n",
       "  'vicious',\n",
       "  'victim',\n",
       "  'vile',\n",
       "  'villain',\n",
       "  'violat',\n",
       "  'violent',\n",
       "  'vulnerab',\n",
       "  'vulture',\n",
       "  'war',\n",
       "  'warfare',\n",
       "  'warred',\n",
       "  'warring',\n",
       "  'wars',\n",
       "  'weak',\n",
       "  'weapon',\n",
       "  'weep',\n",
       "  'weird',\n",
       "  'wept',\n",
       "  'whine',\n",
       "  'whining',\n",
       "  'whore',\n",
       "  'wicked',\n",
       "  'wimp',\n",
       "  'witch',\n",
       "  'woe',\n",
       "  'worr',\n",
       "  'worse',\n",
       "  'worst',\n",
       "  'worthless ',\n",
       "  'wrong',\n",
       "  'yearn',\n",
       "  'maddening',\n",
       "  'madder',\n",
       "  'maddest',\n",
       "  'maniac',\n",
       "  'masochis',\n",
       "  'melanchol',\n",
       "  'mess',\n",
       "  'messy',\n",
       "  'miser',\n",
       "  'miss',\n",
       "  'missed',\n",
       "  'misses',\n",
       "  'missing',\n",
       "  'mistak',\n",
       "  'mock',\n",
       "  'mocked',\n",
       "  'mocker',\n",
       "  'mocking',\n",
       "  'mocks',\n",
       "  'molest',\n",
       "  'mooch',\n",
       "  'moodi',\n",
       "  'moody',\n",
       "  'moron',\n",
       "  'mourn',\n",
       "  'murder',\n",
       "  'nag',\n",
       "  'nast',\n",
       "  'needy',\n",
       "  'neglect',\n",
       "  'nerd',\n",
       "  'nervous',\n",
       "  'neurotic',\n",
       "  'numb',\n",
       "  'obnoxious',\n",
       "  'obsess',\n",
       "  'offence',\n",
       "  'offend',\n",
       "  'offens',\n",
       "  'outrag',\n",
       "  'overwhelm',\n",
       "  'pain',\n",
       "  'pained',\n",
       "  'painf',\n",
       "  'paining',\n",
       "  'pains',\n",
       "  'panic',\n",
       "  'paranoi',\n",
       "  'pathetic',\n",
       "  'peculiar',\n",
       "  'perver',\n",
       "  'pessimis',\n",
       "  'petrif',\n",
       "  'pettie',\n",
       "  'petty',\n",
       "  'phobi',\n",
       "  'piss',\n",
       "  'piti',\n",
       "  'pity ',\n",
       "  'poison',\n",
       "  'prejudic',\n",
       "  'pressur',\n",
       "  'prick',\n",
       "  'problem',\n",
       "  'protest',\n",
       "  'protested',\n",
       "  'protesting',\n",
       "  'puk',\n",
       "  'punish',\n",
       "  'rage',\n",
       "  'raging',\n",
       "  'rancid',\n",
       "  'rape',\n",
       "  'raping',\n",
       "  'rapist',\n",
       "  'rebel',\n",
       "  'reek',\n",
       "  'regret',\n",
       "  'reject',\n",
       "  'reluctan',\n",
       "  'remorse',\n",
       "  'repress',\n",
       "  'resent',\n",
       "  'resign',\n",
       "  'restless',\n",
       "  'revenge',\n",
       "  'ridicul',\n",
       "  'rigid',\n",
       "  'risk',\n",
       "  'rotten',\n",
       "  'rude',\n",
       "  'ruin',\n",
       "  'sad',\n",
       "  'sadde',\n",
       "  'sadly',\n",
       "  'sadness',\n",
       "  'sarcas',\n",
       "  'savage',\n",
       "  'scare',\n",
       "  'scaring',\n",
       "  'scary',\n",
       "  'sceptic',\n",
       "  'scream',\n",
       "  'screw',\n",
       "  'selfish',\n",
       "  'serious',\n",
       "  'seriously',\n",
       "  'seriousness',\n",
       "  'severe',\n",
       "  'shake',\n",
       "  'shaki',\n",
       "  'shaky',\n",
       "  'shame',\n",
       "  'shit',\n",
       "  'shock',\n",
       "  'shook',\n",
       "  'shy',\n",
       "  'sicken',\n",
       "  'sin',\n",
       "  'sinister',\n",
       "  'sins',\n",
       "  'skeptic',\n",
       "  'slut',\n",
       "  'smother',\n",
       "  'smug',\n",
       "  'enrag',\n",
       "  'envie',\n",
       "  'envious',\n",
       "  'envy',\n",
       "  'evil',\n",
       "  'excruciat',\n",
       "  'exhaust',\n",
       "  'fail',\n",
       "  'fake',\n",
       "  'fatal',\n",
       "  'fatigu',\n",
       "  'fault',\n",
       "  'fear',\n",
       "  'feared',\n",
       "  'fearful',\n",
       "  'fearing',\n",
       "  'fears',\n",
       "  'feroc',\n",
       "  'feud',\n",
       "  'fiery',\n",
       "  'fight',\n",
       "  'fired',\n",
       "  'flunk',\n",
       "  'foe',\n",
       "  'fool',\n",
       "  'forbid',\n",
       "  'fought',\n",
       "  'frantic',\n",
       "  'freak',\n",
       "  'fright',\n",
       "  'frustrat',\n",
       "  'fuck',\n",
       "  'fucked',\n",
       "  'fucker',\n",
       "  'fuckin',\n",
       "  'fucks',\n",
       "  'fume',\n",
       "  'fuming',\n",
       "  'furious',\n",
       "  'fury',\n",
       "  'geek',\n",
       "  'gloom',\n",
       "  'goddam',\n",
       "  'gossip',\n",
       "  'grave',\n",
       "  'greed',\n",
       "  'grief',\n",
       "  'griev',\n",
       "  'grim',\n",
       "  'gross',\n",
       "  'grouch',\n",
       "  'grr',\n",
       "  'guilt',\n",
       "  'harass',\n",
       "  'harm',\n",
       "  'harmed',\n",
       "  'harmful',\n",
       "  'harming',\n",
       "  'harms',\n",
       "  'hate',\n",
       "  'hated',\n",
       "  'hateful',\n",
       "  'hater',\n",
       "  'hates',\n",
       "  'hating',\n",
       "  'hatred',\n",
       "  'heartbreak',\n",
       "  'heartbroke',\n",
       "  'heartless',\n",
       "  'hell',\n",
       "  'hellish',\n",
       "  'helpless',\n",
       "  'hesita',\n",
       "  'homesick',\n",
       "  'hopeless',\n",
       "  'horr',\n",
       "  'hostil',\n",
       "  'humiliat',\n",
       "  'hurt',\n",
       "  'idiot',\n",
       "  'ignor',\n",
       "  'immoral',\n",
       "  'impatien',\n",
       "  'impersonal',\n",
       "  'impolite',\n",
       "  'inadequa',\n",
       "  'indecis',\n",
       "  'ineffect',\n",
       "  'inferior ',\n",
       "  'inhib',\n",
       "  'insecur',\n",
       "  'insincer',\n",
       "  'insult',\n",
       "  'interrup',\n",
       "  'intimidat',\n",
       "  'irrational',\n",
       "  'irrita',\n",
       "  'isolat',\n",
       "  'jaded',\n",
       "  'jealous',\n",
       "  'jerk',\n",
       "  'jerked',\n",
       "  'jerks',\n",
       "  'kill',\n",
       "  'lame',\n",
       "  'lazie',\n",
       "  'lazy',\n",
       "  'liabilit',\n",
       "  'liar',\n",
       "  'lied',\n",
       "  'lies',\n",
       "  'lone',\n",
       "  'longing',\n",
       "  'lose',\n",
       "  'loser',\n",
       "  'loses',\n",
       "  'losing',\n",
       "  'loss',\n",
       "  'lost',\n",
       "  'lous',\n",
       "  'low',\n",
       "  'luckless',\n",
       "  'ludicrous',\n",
       "  'lying',\n",
       "  'mad'],\n",
       " 'Cause': ['activat',\n",
       "  'affect',\n",
       "  'affected',\n",
       "  'affecting',\n",
       "  'affects',\n",
       "  'aggravat',\n",
       "  'allow',\n",
       "  'attribut',\n",
       "  'based',\n",
       "  'bases',\n",
       "  'basis',\n",
       "  'because',\n",
       "  'boss',\n",
       "  'caus',\n",
       "  'change',\n",
       "  'changed',\n",
       "  'changes',\n",
       "  'changing',\n",
       "  'compel',\n",
       "  'compliance',\n",
       "  'complie',\n",
       "  'comply',\n",
       "  'conclud',\n",
       "  'consequen',\n",
       "  'control',\n",
       "  'cos',\n",
       "  'coz',\n",
       "  'create',\n",
       "  'creati',\n",
       "  'cuz',\n",
       "  'deduc',\n",
       "  'depend',\n",
       "  'depended',\n",
       "  'depending',\n",
       "  'depends',\n",
       "  'effect',\n",
       "  'elicit',\n",
       "  'experiment',\n",
       "  'force',\n",
       "  'foundation',\n",
       "  'founded',\n",
       "  'founder',\n",
       "  'generate',\n",
       "  'generating',\n",
       "  'generator',\n",
       "  'hence',\n",
       "  'how',\n",
       "  'hows',\n",
       "  \"how's\",\n",
       "  'ignit',\n",
       "  'implica',\n",
       "  'implie',\n",
       "  'imply',\n",
       "  'inact',\n",
       "  'independ',\n",
       "  'induc',\n",
       "  'infer',\n",
       "  'inferr',\n",
       "  'infers',\n",
       "  'influenc',\n",
       "  'intend',\n",
       "  'intent',\n",
       "  'justif',\n",
       "  'launch',\n",
       "  'lead',\n",
       "  'led',\n",
       "  'made',\n",
       "  'make',\n",
       "  'maker',\n",
       "  'makes',\n",
       "  'making',\n",
       "  'manipul',\n",
       "  'misle',\n",
       "  'motiv',\n",
       "  'obedien',\n",
       "  'obey',\n",
       "  'origin',\n",
       "  'originat',\n",
       "  'origins',\n",
       "  'outcome',\n",
       "  'permit',\n",
       "  'pick ',\n",
       "  'produc',\n",
       "  'provoc',\n",
       "  'provok',\n",
       "  'purpose',\n",
       "  'rational',\n",
       "  'react',\n",
       "  'reason',\n",
       "  'response',\n",
       "  'result',\n",
       "  'root',\n",
       "  'since',\n",
       "  'solution',\n",
       "  'solve',\n",
       "  'solved',\n",
       "  'solves',\n",
       "  'solving',\n",
       "  'source',\n",
       "  'stimul',\n",
       "  'therefor',\n",
       "  'thus',\n",
       "  'trigger',\n",
       "  'use',\n",
       "  'used',\n",
       "  'uses',\n",
       "  'using',\n",
       "  'why'],\n",
       " 'Health': ['abortion',\n",
       "  'ache',\n",
       "  'aching',\n",
       "  'acne',\n",
       "  'addict',\n",
       "  'advil',\n",
       "  'aids',\n",
       "  'alcohol',\n",
       "  'alive',\n",
       "  'allerg',\n",
       "  'amput',\n",
       "  'anorexi',\n",
       "  'antacid',\n",
       "  'antidepressant',\n",
       "  'appendic',\n",
       "  'arthr',\n",
       "  'aspirin',\n",
       "  'asthma',\n",
       "  'bandage',\n",
       "  'bandaid',\n",
       "  'binge',\n",
       "  'binging',\n",
       "  'bipolar',\n",
       "  'bleed',\n",
       "  'blind',\n",
       "  'bronchi',\n",
       "  'bulimi',\n",
       "  'burp',\n",
       "  'cancer',\n",
       "  'cardia',\n",
       "  'cardio',\n",
       "  'checkup',\n",
       "  'chills',\n",
       "  'chiropract',\n",
       "  'chlamydia',\n",
       "  'chok',\n",
       "  'cholester',\n",
       "  'chronic',\n",
       "  'clinic',\n",
       "  'codeine',\n",
       "  'colono',\n",
       "  'coma',\n",
       "  'congest',\n",
       "  'constipat',\n",
       "  'contag',\n",
       "  'coronar',\n",
       "  'cough',\n",
       "  'cramp',\n",
       "  'cyst',\n",
       "  'deaf',\n",
       "  'decongest',\n",
       "  'dentist',\n",
       "  'derma',\n",
       "  'detox',\n",
       "  'diabet',\n",
       "  'diagnos',\n",
       "  'diarr',\n",
       "  'digest',\n",
       "  'disease',\n",
       "  'dizz',\n",
       "  'doctor',\n",
       "  'dosage',\n",
       "  'dose',\n",
       "  'dosing',\n",
       "  'dr',\n",
       "  'drows',\n",
       "  'drs',\n",
       "  'drug',\n",
       "  'dx',\n",
       "  'emphysem',\n",
       "  'enema',\n",
       "  'estrogen',\n",
       "  'exercis',\n",
       "  'exhaust',\n",
       "  'faint',\n",
       "  'farsighted',\n",
       "  'fat',\n",
       "  'fatigu',\n",
       "  'fats',\n",
       "  'fatt',\n",
       "  'fever',\n",
       "  'flu',\n",
       "  'gland',\n",
       "  'glaucoma',\n",
       "  'gynecolog',\n",
       "  'gyno',\n",
       "  'hallucinat',\n",
       "  'hangover',\n",
       "  'headache',\n",
       "  'heal',\n",
       "  'healed',\n",
       "  'healer',\n",
       "  'healing',\n",
       "  'heals',\n",
       "  'health',\n",
       "  'heartburn',\n",
       "  'hemor',\n",
       "  'herpes',\n",
       "  'hiccup',\n",
       "  'hiv',\n",
       "  'hormone',\n",
       "  'hospital',\n",
       "  'hungover',\n",
       "  'hyperten',\n",
       "  'hypotherm',\n",
       "  'ibuprofen',\n",
       "  'ICU',\n",
       "  'ill',\n",
       "  'illness',\n",
       "  'immun',\n",
       "  'indigestion',\n",
       "  'infect',\n",
       "  'inflam',\n",
       "  'ingest',\n",
       "  'injur',\n",
       "  'insomnia',\n",
       "  'insulin',\n",
       "  'intox',\n",
       "  'itch',\n",
       "  'iv',\n",
       "  'leuke',\n",
       "  'life',\n",
       "  'living',\n",
       "  'lozenge',\n",
       "  'lump',\n",
       "  'lymph',\n",
       "  'mammogram',\n",
       "  'manicdep',\n",
       "  'medic',\n",
       "  'migrain',\n",
       "  'miscar',\n",
       "  'mono',\n",
       "  'mri ',\n",
       "  'myopi',\n",
       "  'nause',\n",
       "  'nearsighted',\n",
       "  'neurolog',\n",
       "  'numb',\n",
       "  'nurse',\n",
       "  'nutrition',\n",
       "  'obes',\n",
       "  'OCD',\n",
       "  'optometr',\n",
       "  'orthodon',\n",
       "  'orthoped',\n",
       "  'overweight',\n",
       "  'pain',\n",
       "  'pained',\n",
       "  'painf',\n",
       "  'paining',\n",
       "  'painl',\n",
       "  'pains',\n",
       "  'pap',\n",
       "  'paraly',\n",
       "  'patholog',\n",
       "  'pediatr',\n",
       "  'pharmac',\n",
       "  'phobi',\n",
       "  'physical',\n",
       "  'physician',\n",
       "  'pill',\n",
       "  'pills',\n",
       "  'pimple',\n",
       "  'pms',\n",
       "  'podiatr',\n",
       "  'poison',\n",
       "  'pregnan',\n",
       "  'prescri',\n",
       "  'prognos',\n",
       "  'prozac',\n",
       "  'puk',\n",
       "  'puss',\n",
       "  'queas',\n",
       "  'rehab',\n",
       "  'ritalin',\n",
       "  'rx',\n",
       "  'scab',\n",
       "  'schizophren',\n",
       "  'scrape',\n",
       "  'seizure',\n",
       "  'sick',\n",
       "  'sickday',\n",
       "  'sicker',\n",
       "  'sickest',\n",
       "  'sickleave',\n",
       "  'sickly',\n",
       "  'sickness',\n",
       "  'sinus',\n",
       "  'sore',\n",
       "  'std',\n",
       "  'stiff',\n",
       "  'strept',\n",
       "  'stroke',\n",
       "  'sunburn',\n",
       "  'surgeon',\n",
       "  'surger',\n",
       "  'swelling',\n",
       "  'swollen',\n",
       "  'symptom',\n",
       "  'syndrome',\n",
       "  'syphili',\n",
       "  'tender',\n",
       "  'tendoni',\n",
       "  'testosterone',\n",
       "  'therap',\n",
       "  'thermometer',\n",
       "  'throb',\n",
       "  'thyroid',\n",
       "  'tingl',\n",
       "  'tire',\n",
       "  'tiring',\n",
       "  'tox',\n",
       "  'tumo',\n",
       "  'twitch',\n",
       "  'tylenol',\n",
       "  'ulcer',\n",
       "  'unhealth',\n",
       "  'vertigo',\n",
       "  'viagra',\n",
       "  'vicodin',\n",
       "  'vitamin',\n",
       "  'vomit',\n",
       "  'wart',\n",
       "  'warts',\n",
       "  'wash',\n",
       "  'weak',\n",
       "  'weary',\n",
       "  'wheez',\n",
       "  'withdrawal',\n",
       "  'wound',\n",
       "  'xanax',\n",
       "  'xray',\n",
       "  'yawn',\n",
       "  'zit',\n",
       "  'zits',\n",
       "  'zoloft'],\n",
       " 'Money': ['account',\n",
       "  'atm',\n",
       "  'atms',\n",
       "  'auction',\n",
       "  'audit',\n",
       "  'audited',\n",
       "  'auditing',\n",
       "  'auditor',\n",
       "  'auditors',\n",
       "  'audits',\n",
       "  'bank',\n",
       "  'bargain',\n",
       "  'beggar',\n",
       "  'begging',\n",
       "  'bet',\n",
       "  'bets',\n",
       "  'betting',\n",
       "  'bill',\n",
       "  'billed',\n",
       "  'billing',\n",
       "  'bills',\n",
       "  'bonus',\n",
       "  'borrow',\n",
       "  'bought',\n",
       "  'broker',\n",
       "  'buck',\n",
       "  'bucks',\n",
       "  'budget',\n",
       "  'business',\n",
       "  'buy',\n",
       "  'cash',\n",
       "  'casino',\n",
       "  'cent',\n",
       "  'cents',\n",
       "  'charit',\n",
       "  'cheap',\n",
       "  'check',\n",
       "  'checking',\n",
       "  'checks',\n",
       "  'chequ',\n",
       "  'coin',\n",
       "  'coins',\n",
       "  'compensat',\n",
       "  'consumer',\n",
       "  'corporat',\n",
       "  'cost',\n",
       "  'coupon',\n",
       "  'credit',\n",
       "  'currenc',\n",
       "  'customer',\n",
       "  'debit',\n",
       "  'debt',\n",
       "  'deposit',\n",
       "  'dime',\n",
       "  'dinar',\n",
       "  'dinero',\n",
       "  'discount',\n",
       "  'dividend',\n",
       "  'dollar',\n",
       "  'donat',\n",
       "  'econ',\n",
       "  'embezzl',\n",
       "  'euro',\n",
       "  'euros',\n",
       "  'exchang',\n",
       "  'expens',\n",
       "  'fee',\n",
       "  'fees',\n",
       "  'financ',\n",
       "  'fortune',\n",
       "  'franc',\n",
       "  'franchis',\n",
       "  'francs',\n",
       "  'free',\n",
       "  'freeb',\n",
       "  'fund',\n",
       "  'funded',\n",
       "  'funding',\n",
       "  'funds',\n",
       "  'gambl',\n",
       "  'greed',\n",
       "  'income',\n",
       "  'inexpens',\n",
       "  'inherit',\n",
       "  'insurance',\n",
       "  'invest',\n",
       "  'IRS',\n",
       "  'jackpot',\n",
       "  'kopek',\n",
       "  'kron',\n",
       "  'lease',\n",
       "  'leasing',\n",
       "  'lira',\n",
       "  'loan',\n",
       "  'lotter',\n",
       "  'mastercard',\n",
       "  'merchant',\n",
       "  'money',\n",
       "  'monopol',\n",
       "  'mortg',\n",
       "  'nickel',\n",
       "  'overpaid',\n",
       "  'overtime',\n",
       "  'owe ',\n",
       "  'owed',\n",
       "  'owes',\n",
       "  'owing',\n",
       "  'paid',\n",
       "  'pay',\n",
       "  'pence',\n",
       "  'pennies',\n",
       "  'penny',\n",
       "  'peso',\n",
       "  'pesos',\n",
       "  'poor',\n",
       "  'portfolio',\n",
       "  'poverty',\n",
       "  'price',\n",
       "  'prici',\n",
       "  'profit',\n",
       "  'purchas',\n",
       "  'rebate',\n",
       "  'recession',\n",
       "  'refund',\n",
       "  'reimburs',\n",
       "  'rent',\n",
       "  'retail',\n",
       "  'revenue',\n",
       "  'rich',\n",
       "  'ruble',\n",
       "  'rupee',\n",
       "  'salar',\n",
       "  'sale',\n",
       "  'sales',\n",
       "  'saving',\n",
       "  'scholarship',\n",
       "  'sell',\n",
       "  'seller',\n",
       "  'selling',\n",
       "  'sells',\n",
       "  'shilling',\n",
       "  'shop',\n",
       "  'shopaholic',\n",
       "  'shopp',\n",
       "  'shops',\n",
       "  'sold',\n",
       "  'spend',\n",
       "  'spender',\n",
       "  'spending',\n",
       "  'spends',\n",
       "  'spent',\n",
       "  'stipend',\n",
       "  'stocks',\n",
       "  'store',\n",
       "  'tax',\n",
       "  'taxa',\n",
       "  'taxed',\n",
       "  'taxes',\n",
       "  'taxing',\n",
       "  'thrift',\n",
       "  'trade',\n",
       "  'trading',\n",
       "  'tuition',\n",
       "  'underpaid',\n",
       "  'value',\n",
       "  'visa',\n",
       "  'wage',\n",
       "  'wager',\n",
       "  'wages',\n",
       "  'wealth',\n",
       "  'worth',\n",
       "  'yen',\n",
       "  'yuan'],\n",
       " 'Death': ['autops',\n",
       "  'alive',\n",
       "  'bereave',\n",
       "  'burial',\n",
       "  'buried',\n",
       "  'bury',\n",
       "  'casket',\n",
       "  'casualt',\n",
       "  'cemet',\n",
       "  'coffin',\n",
       "  'coroner',\n",
       "  'corpse',\n",
       "  'cremat',\n",
       "  'crypt',\n",
       "  'dead',\n",
       "  'death',\n",
       "  'decease',\n",
       "  'demise',\n",
       "  'die',\n",
       "  'died',\n",
       "  'dies',\n",
       "  'DOA',\n",
       "  'drown',\n",
       "  'dying',\n",
       "  'embalm',\n",
       "  'epidemic',\n",
       "  'execution',\n",
       "  'exterminat',\n",
       "  'fatal',\n",
       "  'funer',\n",
       "  'genocid',\n",
       "  'ghost',\n",
       "  'grave',\n",
       "  'grief',\n",
       "  'griev',\n",
       "  'hearse',\n",
       "  'holocaust',\n",
       "  'homocid',\n",
       "  'immortal',\n",
       "  'kill',\n",
       "  'lethal',\n",
       "  'lynch',\n",
       "  'manslaughter',\n",
       "  'massacre',\n",
       "  'mausoleum',\n",
       "  'morgue',\n",
       "  'mortal',\n",
       "  'mortician',\n",
       "  'mourn',\n",
       "  'murder',\n",
       "  'obit',\n",
       "  'od',\n",
       "  'oded',\n",
       "  'overdosed',\n",
       "  'pallbearer',\n",
       "  'plague',\n",
       "  'reaper',\n",
       "  'slaughter',\n",
       "  'suicid',\n",
       "  'tomb',\n",
       "  'urn',\n",
       "  'war'],\n",
       " 'Ipron': ['anybod',\n",
       "  'anyone',\n",
       "  'anything',\n",
       "  'everybod',\n",
       "  'everyone',\n",
       "  'everything',\n",
       "  'it',\n",
       "  'itd',\n",
       "  \"it'd\",\n",
       "  'itll',\n",
       "  \"it'll\",\n",
       "  'its',\n",
       "  \"it's\",\n",
       "  'itself',\n",
       "  'nobod',\n",
       "  'other',\n",
       "  'others',\n",
       "  'somebod',\n",
       "  'someone',\n",
       "  'something',\n",
       "  'somewhere',\n",
       "  'stuff',\n",
       "  'that',\n",
       "  'thatd',\n",
       "  \"that'd\",\n",
       "  'thatll',\n",
       "  \"that'll\",\n",
       "  'thats',\n",
       "  \"that's\",\n",
       "  'these',\n",
       "  'thing',\n",
       "  'this',\n",
       "  'those',\n",
       "  'what',\n",
       "  'whatever',\n",
       "  'whats',\n",
       "  \"what's\",\n",
       "  'which',\n",
       "  'whichever',\n",
       "  'who',\n",
       "  'whod',\n",
       "  \"who'd\",\n",
       "  'wholl',\n",
       "  \"who'll\",\n",
       "  'whom',\n",
       "  'whose'],\n",
       " 'aux_verb': ['aint',\n",
       "  \"ain't\",\n",
       "  'am',\n",
       "  'are',\n",
       "  'arent',\n",
       "  \"aren't\",\n",
       "  'be',\n",
       "  'became',\n",
       "  'become',\n",
       "  'becomes',\n",
       "  'becoming',\n",
       "  'been',\n",
       "  'being',\n",
       "  'can',\n",
       "  'cannot',\n",
       "  'cant',\n",
       "  \"can't\",\n",
       "  'could',\n",
       "  'couldnt',\n",
       "  \"couldn't\",\n",
       "  'couldve',\n",
       "  \"could've\",\n",
       "  'did',\n",
       "  'didnt',\n",
       "  \"didn't\",\n",
       "  'do',\n",
       "  'does',\n",
       "  'doesnt',\n",
       "  \"doesn't\",\n",
       "  'doing',\n",
       "  'done',\n",
       "  'dont',\n",
       "  \"don't\",\n",
       "  'had',\n",
       "  'hadnt',\n",
       "  \"hadn't\",\n",
       "  'has',\n",
       "  'hasnt',\n",
       "  \"hasn't\",\n",
       "  'have',\n",
       "  'havent',\n",
       "  \"haven't\",\n",
       "  'having',\n",
       "  'hed',\n",
       "  \"he'd\",\n",
       "  'heres',\n",
       "  \"here's\",\n",
       "  'hes',\n",
       "  \"he's\",\n",
       "  'Id',\n",
       "  \"I'd\",\n",
       "  \"I'll\",\n",
       "  'Im',\n",
       "  \"I'm\",\n",
       "  'is',\n",
       "  'isnt',\n",
       "  \"isn't\",\n",
       "  'itd',\n",
       "  \"it'd\",\n",
       "  'itll',\n",
       "  \"it'll\",\n",
       "  \"it's\",\n",
       "  'ive',\n",
       "  \"I've\",\n",
       "  'let ',\n",
       "  'may',\n",
       "  'might',\n",
       "  'mightve',\n",
       "  \"might've\",\n",
       "  'must',\n",
       "  'mustnt',\n",
       "  \"must'nt\",\n",
       "  \"mustn't\",\n",
       "  'mustve',\n",
       "  \"must've\",\n",
       "  'ought',\n",
       "  'oughta',\n",
       "  'oughtnt',\n",
       "  \"ought'nt\",\n",
       "  \"oughtn't\",\n",
       "  'oughtve',\n",
       "  \"ought've\",\n",
       "  'shall',\n",
       "  'shant',\n",
       "  \"shan't\",\n",
       "  \"she'd\",\n",
       "  \"she'll\",\n",
       "  'shes',\n",
       "  \"she's\",\n",
       "  'should',\n",
       "  'shouldnt',\n",
       "  \"should'nt\",\n",
       "  \"shouldn't\",\n",
       "  'shouldve',\n",
       "  \"should've\",\n",
       "  'thatd',\n",
       "  \"that'd\",\n",
       "  'thatll',\n",
       "  \"that'll\",\n",
       "  'thats',\n",
       "  \"that's\",\n",
       "  'theres',\n",
       "  \"there's\",\n",
       "  'theyd',\n",
       "  \"they'd\",\n",
       "  'theyll',\n",
       "  \"they'll\",\n",
       "  'theyre',\n",
       "  \"they're\",\n",
       "  'theyve',\n",
       "  \"they've\",\n",
       "  'was',\n",
       "  'wasnt',\n",
       "  \"wasn't\",\n",
       "  \"we'd\",\n",
       "  \"we'll\",\n",
       "  'were',\n",
       "  \"weren't\",\n",
       "  'weve',\n",
       "  \"we've\",\n",
       "  'whats',\n",
       "  \"what's\",\n",
       "  'wheres',\n",
       "  \"where's\",\n",
       "  'whod',\n",
       "  \"who'd\",\n",
       "  'wholl',\n",
       "  \"who'll\",\n",
       "  'will',\n",
       "  'wont',\n",
       "  \"won't\",\n",
       "  'would',\n",
       "  'wouldnt',\n",
       "  \"wouldn't\",\n",
       "  'wouldve',\n",
       "  \"would've\",\n",
       "  'youd',\n",
       "  \"you'd\",\n",
       "  'youll',\n",
       "  \"you'll\",\n",
       "  'youre',\n",
       "  \"you're\",\n",
       "  'youve',\n",
       "  \"you've\"],\n",
       " 'adverbs': ['about',\n",
       "  'absolutely',\n",
       "  'actually',\n",
       "  'again',\n",
       "  'also',\n",
       "  'anyway',\n",
       "  'anywhere',\n",
       "  'apparently',\n",
       "  'around',\n",
       "  'back',\n",
       "  'basically',\n",
       "  'beyond',\n",
       "  'clearly',\n",
       "  'completely',\n",
       "  'constantly',\n",
       "  'definitely',\n",
       "  'especially',\n",
       "  'even',\n",
       "  'eventually',\n",
       "  'ever',\n",
       "  'frequently',\n",
       "  'generally',\n",
       "  'here',\n",
       "  'heres',\n",
       "  \"here's\",\n",
       "  'hopefully',\n",
       "  'how',\n",
       "  'however',\n",
       "  'immediately',\n",
       "  'instead',\n",
       "  'just',\n",
       "  'lately',\n",
       "  'maybe',\n",
       "  'mostly',\n",
       "  'nearly',\n",
       "  'now',\n",
       "  'often',\n",
       "  'only',\n",
       "  'perhaps',\n",
       "  'primarily',\n",
       "  'probably',\n",
       "  'push',\n",
       "  'quick',\n",
       "  'rarely',\n",
       "  'rather',\n",
       "  'really',\n",
       "  'seriously',\n",
       "  'simply',\n",
       "  'so',\n",
       "  'somehow',\n",
       "  'soon',\n",
       "  'sooo',\n",
       "  'still',\n",
       "  'such',\n",
       "  'there',\n",
       "  'theres',\n",
       "  \"there's\",\n",
       "  'tho',\n",
       "  'though',\n",
       "  'too',\n",
       "  'totally',\n",
       "  'truly',\n",
       "  'usually',\n",
       "  'very',\n",
       "  'well',\n",
       "  'when ',\n",
       "  'whenever',\n",
       "  'where',\n",
       "  'yet'],\n",
       " 'Negate': ['hasnt',\n",
       "  \"hasn't\",\n",
       "  'havent',\n",
       "  \"haven't\",\n",
       "  'isnt',\n",
       "  \"isn't\",\n",
       "  'mustnt',\n",
       "  \"must'nt\",\n",
       "  \"mustn't\",\n",
       "  'neednt',\n",
       "  \"need'nt\",\n",
       "  \"needn't\",\n",
       "  'negat',\n",
       "  'neither',\n",
       "  'never',\n",
       "  'no',\n",
       "  'nobod',\n",
       "  'none',\n",
       "  'nope',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'nothing',\n",
       "  'nowhere',\n",
       "  'oughtnt',\n",
       "  \"ought'nt\",\n",
       "  \"oughtn't\",\n",
       "  'shant',\n",
       "  \"shan't\",\n",
       "  'shouldnt',\n",
       "  \"should'nt\",\n",
       "  \"shouldn't\",\n",
       "  'uhuh',\n",
       "  'wasnt',\n",
       "  \"wasn't\",\n",
       "  \"weren't\",\n",
       "  'without',\n",
       "  'wont',\n",
       "  \"won't\",\n",
       "  'wouldnt',\n",
       "  \"wouldn't\"],\n",
       " 'Family': ['grandchil',\n",
       "  'granddad',\n",
       "  'granddau',\n",
       "  'grandf',\n",
       "  'grandkid',\n",
       "  'grandm',\n",
       "  'grandpa',\n",
       "  'grandson',\n",
       "  'granny',\n",
       "  'hubby',\n",
       "  'husband',\n",
       "  'kin',\n",
       "  'ma',\n",
       "  'marrie',\n",
       "  \"ma's\",\n",
       "  'mom',\n",
       "  'momma',\n",
       "  'mommy',\n",
       "  'moms',\n",
       "  \"mom's\",\n",
       "  'mother',\n",
       "  'mothers',\n",
       "  'mum',\n",
       "  'mummy',\n",
       "  'mums',\n",
       "  \"mum's\",\n",
       "  'nephew',\n",
       "  'niece',\n",
       "  'pa',\n",
       "  'pappy',\n",
       "  'parent',\n",
       "  \"pa's\",\n",
       "  'relatives',\n",
       "  'sis',\n",
       "  'sister',\n",
       "  'son',\n",
       "  'sons',\n",
       "  \"son's\",\n",
       "  'spous',\n",
       "  'stepchild',\n",
       "  'stepfat',\n",
       "  'stepkid',\n",
       "  'stepmot',\n",
       "  'uncle',\n",
       "  'uncles',\n",
       "  \"uncle's\",\n",
       "  'wife',\n",
       "  'wive'],\n",
       " 'Anx': ['afraid',\n",
       "  'alarm',\n",
       "  'anguish',\n",
       "  'anxi',\n",
       "  'apprehens',\n",
       "  'asham',\n",
       "  'aversi',\n",
       "  'avoid',\n",
       "  'awkward',\n",
       "  'confus',\n",
       "  'craz',\n",
       "  'desperat',\n",
       "  'discomfort',\n",
       "  'distract',\n",
       "  'distraught',\n",
       "  'distress',\n",
       "  'disturb',\n",
       "  'doubt',\n",
       "  'dread',\n",
       "  'dwell',\n",
       "  'embarrass',\n",
       "  'emotional',\n",
       "  'fear',\n",
       "  'feared',\n",
       "  'fearful',\n",
       "  'fearing',\n",
       "  'fears',\n",
       "  'frantic',\n",
       "  'fright',\n",
       "  'guilt',\n",
       "  'hesita',\n",
       "  'horr',\n",
       "  'humiliat',\n",
       "  'impatien',\n",
       "  'inadequa',\n",
       "  'indecis',\n",
       "  'inhib',\n",
       "  'insecur',\n",
       "  'irrational',\n",
       "  'irrita',\n",
       "  'miser',\n",
       "  'nervous',\n",
       "  'neurotic',\n",
       "  'obsess',\n",
       "  'overwhelm',\n",
       "  'panic',\n",
       "  'petrif',\n",
       "  'phobi',\n",
       "  'pressur',\n",
       "  'reluctan',\n",
       "  'repress',\n",
       "  'restless',\n",
       "  'rigid',\n",
       "  'risk',\n",
       "  'scare',\n",
       "  'scaring',\n",
       "  'scary',\n",
       "  'shake',\n",
       "  'shaki',\n",
       "  'shaky',\n",
       "  'shame',\n",
       "  'shook',\n",
       "  'shy',\n",
       "  'sicken',\n",
       "  'startl',\n",
       "  'strain',\n",
       "  'stress',\n",
       "  'struggl',\n",
       "  'stunned',\n",
       "  'stuns',\n",
       "  'suspicio',\n",
       "  'tense',\n",
       "  'tensing',\n",
       "  'tension',\n",
       "  'terrified',\n",
       "  'terrifies',\n",
       "  'terrify ',\n",
       "  'terrifying',\n",
       "  'terror',\n",
       "  'timid',\n",
       "  'trembl',\n",
       "  'turmoil',\n",
       "  'uncertain',\n",
       "  'uncomfortabl',\n",
       "  'uncontrol',\n",
       "  'uneas',\n",
       "  'unsure',\n",
       "  'upset',\n",
       "  'uptight',\n",
       "  'vulnerab',\n",
       "  'worr'],\n",
       " 'Anger': ['abuse',\n",
       "  'abusi',\n",
       "  'aggravat',\n",
       "  'aggress',\n",
       "  'agitat',\n",
       "  'anger',\n",
       "  'angr',\n",
       "  'annoy',\n",
       "  'antagoni',\n",
       "  'argh',\n",
       "  'argu',\n",
       "  'arrogan',\n",
       "  'assault',\n",
       "  'asshole',\n",
       "  'attack',\n",
       "  'bastard',\n",
       "  'battl',\n",
       "  'beaten',\n",
       "  'bitch',\n",
       "  'bitter',\n",
       "  'blam',\n",
       "  'bother',\n",
       "  'brutal',\n",
       "  'cheat',\n",
       "  'confront',\n",
       "  'contempt',\n",
       "  'contradic',\n",
       "  'crap',\n",
       "  'crappy',\n",
       "  'critical',\n",
       "  'critici',\n",
       "  'crude',\n",
       "  'cruel',\n",
       "  'cunt',\n",
       "  'cut',\n",
       "  'cynic',\n",
       "  'damn',\n",
       "  'danger',\n",
       "  'defenc',\n",
       "  'defens',\n",
       "  'despis',\n",
       "  'destroy',\n",
       "  'destruct',\n",
       "  'disgust',\n",
       "  'distrust',\n",
       "  'domina',\n",
       "  'dumb',\n",
       "  'dump',\n",
       "  'enemie',\n",
       "  'enemy',\n",
       "  'enrag',\n",
       "  'envie',\n",
       "  'envious',\n",
       "  'envy',\n",
       "  'evil',\n",
       "  'feroc',\n",
       "  'feud',\n",
       "  'fiery',\n",
       "  'fight',\n",
       "  'foe',\n",
       "  'fought',\n",
       "  'frustrat',\n",
       "  'fuck',\n",
       "  'fucked',\n",
       "  'fucker',\n",
       "  'fuckin',\n",
       "  'fucks',\n",
       "  'fume',\n",
       "  'fuming',\n",
       "  'furious',\n",
       "  'fury',\n",
       "  'goddam',\n",
       "  'greed',\n",
       "  'grouch',\n",
       "  'grr',\n",
       "  'harass',\n",
       "  'hate',\n",
       "  'hated',\n",
       "  'hateful',\n",
       "  'hater',\n",
       "  'hates',\n",
       "  'hating',\n",
       "  'hatred',\n",
       "  'heartless',\n",
       "  'hell',\n",
       "  'hellish',\n",
       "  'hostil',\n",
       "  'humiliat',\n",
       "  'idiot',\n",
       "  'insult',\n",
       "  'interrup',\n",
       "  'intimidat',\n",
       "  'jealous',\n",
       "  'jerk',\n",
       "  'jerked',\n",
       "  'jerks',\n",
       "  'kill',\n",
       "  'liar',\n",
       "  'lied',\n",
       "  'lies',\n",
       "  'lous',\n",
       "  'ludicrous',\n",
       "  'lying',\n",
       "  'mad',\n",
       "  'maddening',\n",
       "  'madder',\n",
       "  'maddest',\n",
       "  'maniac',\n",
       "  'mock',\n",
       "  'mocked',\n",
       "  'mocker',\n",
       "  'mocking',\n",
       "  'mocks',\n",
       "  'molest',\n",
       "  'moron',\n",
       "  'murder',\n",
       "  'nag',\n",
       "  'nast',\n",
       "  'obnoxious',\n",
       "  'offence',\n",
       "  'offend',\n",
       "  'offens',\n",
       "  'outrag',\n",
       "  'paranoi',\n",
       "  'pettie',\n",
       "  'petty',\n",
       "  'piss',\n",
       "  'poison',\n",
       "  'prejudic',\n",
       "  'prick',\n",
       "  'protest',\n",
       "  'protested',\n",
       "  'protesting',\n",
       "  'punish',\n",
       "  'rage',\n",
       "  'raging',\n",
       "  'rape',\n",
       "  'raping',\n",
       "  'rapist',\n",
       "  'rebel',\n",
       "  'resent',\n",
       "  'revenge',\n",
       "  'ridicul',\n",
       "  'rude',\n",
       "  'sarcas',\n",
       "  'savage',\n",
       "  'sceptic',\n",
       "  'screw',\n",
       "  'shit',\n",
       "  'sinister',\n",
       "  'skeptic',\n",
       "  'smother',\n",
       "  'snob',\n",
       "  'spite',\n",
       "  'stubborn',\n",
       "  'stupid',\n",
       "  'suck',\n",
       "  'sucked',\n",
       "  'sucker',\n",
       "  'sucks',\n",
       "  'sucky',\n",
       "  'tantrum',\n",
       "  'teas',\n",
       "  'temper',\n",
       "  'tempers',\n",
       "  'terrify',\n",
       "  'threat',\n",
       "  'ticked',\n",
       "  'tortur',\n",
       "  'trick',\n",
       "  'ugl',\n",
       "  'vicious',\n",
       "  'victim',\n",
       "  'vile',\n",
       "  'villain',\n",
       "  'violat',\n",
       "  'violent',\n",
       "  'war',\n",
       "  'warfare',\n",
       "  'warred',\n",
       "  'warring',\n",
       "  'wars',\n",
       "  'weapon',\n",
       "  'wicked'],\n",
       " 'Sad': ['abandon',\n",
       "  'ache',\n",
       "  'aching',\n",
       "  'agoniz',\n",
       "  'agony',\n",
       "  'alone',\n",
       "  'broke',\n",
       "  'cried',\n",
       "  'cries',\n",
       "  'crushed',\n",
       "  'cry',\n",
       "  'crying',\n",
       "  'damag',\n",
       "  'defeat',\n",
       "  'depress',\n",
       "  'depriv',\n",
       "  'despair',\n",
       "  'devastat',\n",
       "  'disadvantage',\n",
       "  'disappoint',\n",
       "  'discourag',\n",
       "  'dishearten',\n",
       "  'disillusion',\n",
       "  'dissatisf',\n",
       "  'doom',\n",
       "  'dull',\n",
       "  'empt',\n",
       "  'fail',\n",
       "  'fatigu',\n",
       "  'flunk',\n",
       "  'gloom',\n",
       "  'grave',\n",
       "  'grief',\n",
       "  'griev',\n",
       "  'grim',\n",
       "  'heartbreak',\n",
       "  'heartbroke',\n",
       "  'helpless',\n",
       "  'homesick',\n",
       "  'hopeless',\n",
       "  'hurt',\n",
       "  'inadequa',\n",
       "  'inferior ',\n",
       "  'isolat',\n",
       "  'lame',\n",
       "  'lone',\n",
       "  'longing',\n",
       "  'lose',\n",
       "  'loser',\n",
       "  'loses',\n",
       "  'losing',\n",
       "  'loss',\n",
       "  'lost',\n",
       "  'low',\n",
       "  'melanchol',\n",
       "  'miser',\n",
       "  'miss',\n",
       "  'missed',\n",
       "  'misses',\n",
       "  'missing',\n",
       "  'mourn',\n",
       "  'neglect',\n",
       "  'overwhelm',\n",
       "  'pathetic',\n",
       "  'pessimis',\n",
       "  'piti',\n",
       "  'pity ',\n",
       "  'regret',\n",
       "  'reject',\n",
       "  'remorse',\n",
       "  'resign',\n",
       "  'ruin',\n",
       "  'sad',\n",
       "  'sadde',\n",
       "  'sadly',\n",
       "  'sadness',\n",
       "  'sob',\n",
       "  'sobbed',\n",
       "  'sobbing',\n",
       "  'sobs',\n",
       "  'solemn',\n",
       "  'sorrow',\n",
       "  'suffer',\n",
       "  'suffered',\n",
       "  'sufferer',\n",
       "  'suffering',\n",
       "  'suffers',\n",
       "  'tears',\n",
       "  'traged',\n",
       "  'tragic ',\n",
       "  'unhapp',\n",
       "  'unimportant',\n",
       "  'unsuccessful',\n",
       "  'useless ',\n",
       "  'weep',\n",
       "  'wept',\n",
       "  'whine',\n",
       "  'whining',\n",
       "  'woe',\n",
       "  'worthless ',\n",
       "  'yearn'],\n",
       " 'CogMech': ['abandon',\n",
       "  'absolute',\n",
       "  'absolutely',\n",
       "  'abstain',\n",
       "  'accept',\n",
       "  'accepta',\n",
       "  'accepted',\n",
       "  'accepting',\n",
       "  'accepts',\n",
       "  'accura',\n",
       "  'acknowledg',\n",
       "  'activat',\n",
       "  'add',\n",
       "  'addit',\n",
       "  'adjust',\n",
       "  'admit',\n",
       "  'admits',\n",
       "  'admitted',\n",
       "  'admitting',\n",
       "  'affect',\n",
       "  'affected',\n",
       "  'affecting',\n",
       "  'affects',\n",
       "  'afterthought',\n",
       "  'aggravat',\n",
       "  'all',\n",
       "  'allot',\n",
       "  'allow',\n",
       "  'almost',\n",
       "  'along',\n",
       "  'alot',\n",
       "  'altogether',\n",
       "  'always',\n",
       "  'ambigu',\n",
       "  'anal',\n",
       "  'analy',\n",
       "  'and',\n",
       "  'answer',\n",
       "  'any',\n",
       "  'anybod',\n",
       "  'anyhow',\n",
       "  'anyone',\n",
       "  'anything',\n",
       "  'anytime',\n",
       "  'anywhere',\n",
       "  'apparent',\n",
       "  'apparently',\n",
       "  'appear',\n",
       "  'appeared',\n",
       "  'appearing',\n",
       "  'appears',\n",
       "  'appreciat',\n",
       "  'approximat',\n",
       "  'arbitrar',\n",
       "  'around',\n",
       "  'assum',\n",
       "  'assur',\n",
       "  'attent',\n",
       "  'attribut',\n",
       "  'avert',\n",
       "  'avoid',\n",
       "  'aware',\n",
       "  'ban',\n",
       "  'banned',\n",
       "  'banning',\n",
       "  'bans',\n",
       "  'barely',\n",
       "  'barrier',\n",
       "  'based',\n",
       "  'bases',\n",
       "  'basis',\n",
       "  'became',\n",
       "  'because',\n",
       "  'become',\n",
       "  'becomes',\n",
       "  'becoming',\n",
       "  'belief',\n",
       "  'believe',\n",
       "  'believed',\n",
       "  'believes',\n",
       "  'believing',\n",
       "  'besides',\n",
       "  'bet',\n",
       "  'bets',\n",
       "  'betting',\n",
       "  'binding',\n",
       "  'blatant',\n",
       "  'block',\n",
       "  'blocked',\n",
       "  'blocker',\n",
       "  'blocking',\n",
       "  'blocks',\n",
       "  'blur',\n",
       "  'borderline',\n",
       "  'boss',\n",
       "  'both',\n",
       "  'bound',\n",
       "  'brake',\n",
       "  'bridle',\n",
       "  'but',\n",
       "  'came',\n",
       "  'careful',\n",
       "  'categor',\n",
       "  'caus',\n",
       "  'caut',\n",
       "  'cease',\n",
       "  'ceasing',\n",
       "  'certain',\n",
       "  'chance',\n",
       "  'change',\n",
       "  'changed',\n",
       "  'changes',\n",
       "  'changing',\n",
       "  'choice',\n",
       "  'choos',\n",
       "  'clarif',\n",
       "  'clear',\n",
       "  'clearly',\n",
       "  'close',\n",
       "  'closure',\n",
       "  'cohere',\n",
       "  'come',\n",
       "  'commit',\n",
       "  'commitment',\n",
       "  'commits',\n",
       "  'committ',\n",
       "  'compel',\n",
       "  'complete',\n",
       "  'completed',\n",
       "  'completely',\n",
       "  'completes',\n",
       "  'complex',\n",
       "  'compliance',\n",
       "  'complica',\n",
       "  'complie',\n",
       "  'comply',\n",
       "  'compreh',\n",
       "  'compulsiv',\n",
       "  'concentrat',\n",
       "  'conclud',\n",
       "  'conclus',\n",
       "  'confess',\n",
       "  'confidence',\n",
       "  'confident',\n",
       "  'confidently',\n",
       "  'confin',\n",
       "  'conflict',\n",
       "  'confus',\n",
       "  'conscious',\n",
       "  'consequen',\n",
       "  'conserv',\n",
       "  'consider',\n",
       "  'considered',\n",
       "  'considering',\n",
       "  'considers',\n",
       "  'constrain',\n",
       "  'constrict',\n",
       "  'contain',\n",
       "  'contemplat',\n",
       "  'contingen',\n",
       "  'contradic',\n",
       "  'control',\n",
       "  'correct',\n",
       "  'correlat',\n",
       "  'cos',\n",
       "  'could',\n",
       "  'couldnt',\n",
       "  \"couldn't\",\n",
       "  'couldve',\n",
       "  \"could've\",\n",
       "  'coz',\n",
       "  'create',\n",
       "  'creati',\n",
       "  'curb',\n",
       "  'curio',\n",
       "  'curtail',\n",
       "  'cuz',\n",
       "  'decid',\n",
       "  'decis',\n",
       "  'deduc',\n",
       "  'defenc',\n",
       "  'defens',\n",
       "  'define',\n",
       "  'defined',\n",
       "  'defines',\n",
       "  'defining',\n",
       "  'definite',\n",
       "  'definitely',\n",
       "  'definitive',\n",
       "  'delay',\n",
       "  'denia',\n",
       "  'denie',\n",
       "  'deny',\n",
       "  'depend',\n",
       "  'depended',\n",
       "  'depending',\n",
       "  'depends',\n",
       "  'desir',\n",
       "  'determina',\n",
       "  'determine',\n",
       "  'determined',\n",
       "  'determines',\n",
       "  'determining',\n",
       "  'difference',\n",
       "  'differentiat',\n",
       "  'directly',\n",
       "  'discern',\n",
       "  'disciplin',\n",
       "  'disclo',\n",
       "  'discourag',\n",
       "  'discover',\n",
       "  'disorient',\n",
       "  'disregard',\n",
       "  'distinct',\n",
       "  'distinguish',\n",
       "  'doubt',\n",
       "  'dubious',\n",
       "  'dunno',\n",
       "  'duti',\n",
       "  'duty',\n",
       "  'each',\n",
       "  'effect',\n",
       "  'either',\n",
       "  'elicit',\n",
       "  'enclos',\n",
       "  'enlighten',\n",
       "  'entire',\n",
       "  'essential',\n",
       "  'evaluat',\n",
       "  'ever',\n",
       "  'every',\n",
       "  'everybod',\n",
       "  'everything',\n",
       "  'evident',\n",
       "  'exact',\n",
       "  'examin',\n",
       "  'except',\n",
       "  'exclu',\n",
       "  'expect',\n",
       "  'experiment',\n",
       "  'explain',\n",
       "  'explained',\n",
       "  'explaining',\n",
       "  'explains',\n",
       "  'explanat',\n",
       "  'explicit',\n",
       "  'explor',\n",
       "  'extremely',\n",
       "  'fact',\n",
       "  'facts',\n",
       "  'factual',\n",
       "  'fairly',\n",
       "  'feel',\n",
       "  'feeling',\n",
       "  'feels',\n",
       "  'felt',\n",
       "  'fenc',\n",
       "  'figur',\n",
       "  'find',\n",
       "  'forbid',\n",
       "  'force',\n",
       "  'forever',\n",
       "  'forgave',\n",
       "  'forget',\n",
       "  'forgiv',\n",
       "  'forgot',\n",
       "  'found',\n",
       "  'foundation',\n",
       "  'founded',\n",
       "  'founder',\n",
       "  'frankly',\n",
       "  'fundamental',\n",
       "  'fundamentalis',\n",
       "  'fundamentally',\n",
       "  'fundamentals',\n",
       "  'fuzz',\n",
       "  'general',\n",
       "  'generally',\n",
       "  'generate',\n",
       "  'generating',\n",
       "  'generator',\n",
       "  'grasp',\n",
       "  'guarant',\n",
       "  'guard',\n",
       "  'guess',\n",
       "  'guessed',\n",
       "  'guesses',\n",
       "  'guessing',\n",
       "  'halfass',\n",
       "  'halt',\n",
       "  'hangup',\n",
       "  'hardly',\n",
       "  'harness',\n",
       "  'hazie',\n",
       "  'hazy',\n",
       "  'held',\n",
       "  'hence',\n",
       "  'hesita',\n",
       "  'hold',\n",
       "  'hope',\n",
       "  'hoped',\n",
       "  'hopeful',\n",
       "  'hopefully',\n",
       "  'hopefulness',\n",
       "  'hopes',\n",
       "  'hoping',\n",
       "  'how',\n",
       "  'hows',\n",
       "  \"how's\",\n",
       "  'hypothes',\n",
       "  'hypothetic',\n",
       "  'idea',\n",
       "  'ideal',\n",
       "  'ideas',\n",
       "  'identif',\n",
       "  'if',\n",
       "  'ignit',\n",
       "  'ignor',\n",
       "  'imagin',\n",
       "  'implica',\n",
       "  'implicit',\n",
       "  'implie',\n",
       "  'imply',\n",
       "  'impossib',\n",
       "  'inact',\n",
       "  'inadequa',\n",
       "  'inclu',\n",
       "  'incomplet',\n",
       "  'indecis',\n",
       "  'indeed',\n",
       "  'indefinit',\n",
       "  'independ',\n",
       "  'indetermin',\n",
       "  'indirect',\n",
       "  'induc',\n",
       "  'inevitab',\n",
       "  'infallib',\n",
       "  'infer',\n",
       "  'inferr',\n",
       "  'infers',\n",
       "  'influenc',\n",
       "  'info',\n",
       "  'inform',\n",
       "  'information',\n",
       "  'informative',\n",
       "  'informed',\n",
       "  'informing',\n",
       "  'informs',\n",
       "  'inhib',\n",
       "  'inquir',\n",
       "  'inside',\n",
       "  'insight',\n",
       "  'inspir',\n",
       "  'intend',\n",
       "  'intent',\n",
       "  'interfer',\n",
       "  'interpret',\n",
       "  'into',\n",
       "  'invariab',\n",
       "  'irrational',\n",
       "  'irrefu',\n",
       "  'issue',\n",
       "  'just',\n",
       "  'justif',\n",
       "  'keep',\n",
       "  'keeping',\n",
       "  'keeps',\n",
       "  'kept',\n",
       "  'kind (of)',\n",
       "  'kinda',\n",
       "  'kindof',\n",
       "  'knew',\n",
       "  'know',\n",
       "  'knowab',\n",
       "  'knower',\n",
       "  'knowing',\n",
       "  'knowledg',\n",
       "  'known',\n",
       "  'knows',\n",
       "  'lack',\n",
       "  'launch',\n",
       "  'law',\n",
       "  'lead',\n",
       "  'learn',\n",
       "  'led',\n",
       "  'lesson',\n",
       "  'liabilit',\n",
       "  'likel',\n",
       "  'limit',\n",
       "  'link',\n",
       "  'logic',\n",
       "  'lot',\n",
       "  'lotof',\n",
       "  'lots ',\n",
       "  'lotsa',\n",
       "  'lotta',\n",
       "  'luck',\n",
       "  'lucked',\n",
       "  'lucki',\n",
       "  'luckless',\n",
       "  'lucks',\n",
       "  'lucky',\n",
       "  'made',\n",
       "  'mainly',\n",
       "  'make ',\n",
       "  'maker',\n",
       "  'makes',\n",
       "  'making',\n",
       "  'manipul',\n",
       "  'marginal',\n",
       "  'matter',\n",
       "  'may',\n",
       "  'maybe',\n",
       "  'mean',\n",
       "  'meaning',\n",
       "  'means',\n",
       "  'meant',\n",
       "  'memor',\n",
       "  'might',\n",
       "  'mightve',\n",
       "  \"might've\",\n",
       "  'mind',\n",
       "  'misle',\n",
       "  'mistak',\n",
       "  'misunder',\n",
       "  'most',\n",
       "  'mostly',\n",
       "  'motiv',\n",
       "  'must',\n",
       "  'mustnt',\n",
       "  \"must'nt\",\n",
       "  \"mustn't\",\n",
       "  'mustve',\n",
       "  \"must've\",\n",
       "  'myster',\n",
       "  'name',\n",
       "  'nearly',\n",
       "  'necessar',\n",
       "  'needed',\n",
       "  'needing',\n",
       "  'neednt',\n",
       "  \"need'nt\",\n",
       "  \"needn't\",\n",
       "  'needs',\n",
       "  'neglect',\n",
       "  'never',\n",
       "  'news',\n",
       "  'normal',\n",
       "  'not',\n",
       "  'notice',\n",
       "  'noticing',\n",
       "  'obedien',\n",
       "  'obey',\n",
       "  'obscur',\n",
       "  'obstac',\n",
       "  'obvious',\n",
       "  'occasional',\n",
       "  'often',\n",
       "  'open',\n",
       "  'opinion',\n",
       "  'oppos',\n",
       "  'option',\n",
       "  'or',\n",
       "  'organiz',\n",
       "  'origin',\n",
       "  'originat',\n",
       "  'origins',\n",
       "  'ought',\n",
       "  'oughta',\n",
       "  'oughtnt',\n",
       "  \"ought'nt\",\n",
       "  \"oughtn't\",\n",
       "  'oughtve',\n",
       "  \"ought've\",\n",
       "  'out',\n",
       "  'outcome',\n",
       "  'outstanding',\n",
       "  'overall',\n",
       "  'partly',\n",
       "  'perceiv',\n",
       "  'percept',\n",
       "  'perfect',\n",
       "  'perhaps',\n",
       "  'permit',\n",
       "  'pick ',\n",
       "  'plus',\n",
       "  'ponder',\n",
       "  'positiv',\n",
       "  'possib',\n",
       "  'practically',\n",
       "  'precis',\n",
       "  'prefer',\n",
       "  'presum',\n",
       "  'pretty',\n",
       "  'prevent',\n",
       "  'probable',\n",
       "  'probablistic',\n",
       "  'probably',\n",
       "  'problem',\n",
       "  'produc',\n",
       "  'prohib',\n",
       "  'proof',\n",
       "  'prove',\n",
       "  'proving',\n",
       "  'provoc',\n",
       "  'provok',\n",
       "  'prude',\n",
       "  'prudes',\n",
       "  'prudish',\n",
       "  'pure',\n",
       "  'purpose',\n",
       "  'puzzl',\n",
       "  'quer',\n",
       "  'question',\n",
       "  'quite',\n",
       "  'random',\n",
       "  'rather',\n",
       "  'rational',\n",
       "  'react',\n",
       "  'real ',\n",
       "  'reality',\n",
       "  'realiz',\n",
       "  'really',\n",
       "  'rearrang',\n",
       "  'reason',\n",
       "  'recall',\n",
       "  'reckon',\n",
       "  'recogni',\n",
       "  'recollect',\n",
       "  'reconcil',\n",
       "  'reconsider',\n",
       "  'reconstruct',\n",
       "  'reevaluat',\n",
       "  'refer',\n",
       "  'reflect',\n",
       "  'refrain',\n",
       "  'refus',\n",
       "  'regardless',\n",
       "  'regret',\n",
       "  'rein',\n",
       "  'relate',\n",
       "  'relating',\n",
       "  'relation',\n",
       "  'reluctan',\n",
       "  'rememb',\n",
       "  'reorgani',\n",
       "  'repress',\n",
       "  'requir',\n",
       "  'reserved',\n",
       "  'resolu',\n",
       "  'resolv',\n",
       "  'response',\n",
       "  'responsib',\n",
       "  'restrain',\n",
       "  'restrict',\n",
       "  'restructur',\n",
       "  'result',\n",
       "  'retain',\n",
       "  'rethink',\n",
       "  'reveal',\n",
       "  'revelat',\n",
       "  'rigid',\n",
       "  'root',\n",
       "  'safe',\n",
       "  'same',\n",
       "  'save',\n",
       "  'secret',\n",
       "  'secrets',\n",
       "  'seem',\n",
       "  'seemed',\n",
       "  'seeming',\n",
       "  'seems',\n",
       "  'sense',\n",
       "  'sensed',\n",
       "  'senses',\n",
       "  'sensing',\n",
       "  'shaki',\n",
       "  'shaky',\n",
       "  'should',\n",
       "  'shouldnt',\n",
       "  \"should'nt\",\n",
       "  \"shouldn't\",\n",
       "  'shoulds',\n",
       "  'shouldve',\n",
       "  \"should've\",\n",
       "  'sign',\n",
       "  'since',\n",
       "  'solution',\n",
       "  'solve',\n",
       "  'solved',\n",
       "  'solves',\n",
       "  'solving',\n",
       "  'some',\n",
       "  'somebod',\n",
       "  'somehow',\n",
       "  'someone',\n",
       "  'something',\n",
       "  'sometime',\n",
       "  'sometimes',\n",
       "  'somewhat',\n",
       "  'somewhere',\n",
       "  'sort',\n",
       "  'sorta',\n",
       "  'sortof',\n",
       "  'sorts',\n",
       "  'sortsa',\n",
       "  'source',\n",
       "  'spose',\n",
       "  'standard',\n",
       "  'statement',\n",
       "  'stiff',\n",
       "  'stimul',\n",
       "  'stop',\n",
       "  'stopped',\n",
       "  'stopper',\n",
       "  'stopping',\n",
       "  'stops',\n",
       "  'stories',\n",
       "  'story',\n",
       "  'stubborn',\n",
       "  'subdue',\n",
       "  'suppose',\n",
       "  'supposed',\n",
       "  'supposes',\n",
       "  'supposing',\n",
       "  'supposition',\n",
       "  'suppress',\n",
       "  'sure',\n",
       "  'suspect',\n",
       "  'suspicio',\n",
       "  'taboo',\n",
       "  'tempora',\n",
       "  'tentativ',\n",
       "  'theor',\n",
       "  'therefor',\n",
       "  'think',\n",
       "  'thinker',\n",
       "  'thinking',\n",
       "  'thinks',\n",
       "  'thought',\n",
       "  'thoughts',\n",
       "  'thus',\n",
       "  'tidi',\n",
       "  'tidy',\n",
       "  'tight',\n",
       "  'total',\n",
       "  'totally',\n",
       "  'trigger',\n",
       "  'true',\n",
       "  'truest',\n",
       "  'truly',\n",
       "  'truth',\n",
       "  'typically',\n",
       "  'unaccept',\n",
       "  'unambigu',\n",
       "  'unaware',\n",
       "  'uncertain',\n",
       "  'unclear',\n",
       "  'undecided',\n",
       "  'undeniab',\n",
       "  'understand',\n",
       "  'understandab',\n",
       "  'understanding',\n",
       "  'understands',\n",
       "  'understood',\n",
       "  'undesire',\n",
       "  'undetermin',\n",
       "  'undo ',\n",
       "  'undoubt',\n",
       "  'unknow',\n",
       "  'unless',\n",
       "  'unlikel',\n",
       "  'unluck',\n",
       "  'unneccess',\n",
       "  'unneed',\n",
       "  'unquestion',\n",
       "  'unrelat',\n",
       "  'unresolv',\n",
       "  'unsettl',\n",
       "  'unsure',\n",
       "  'unwant',\n",
       "  'uptight',\n",
       "  'use',\n",
       "  'used',\n",
       "  'uses',\n",
       "  'using',\n",
       "  'usually',\n",
       "  'vague',\n",
       "  'variab',\n",
       "  'varies',\n",
       "  'vary',\n",
       "  'versus',\n",
       "  'veto',\n",
       "  'vs',\n",
       "  'wait',\n",
       "  'waited',\n",
       "  'waiting',\n",
       "  'waits',\n",
       "  'wanna',\n",
       "  'want',\n",
       "  'wanted',\n",
       "  'wanting',\n",
       "  'wants',\n",
       "  'we',\n",
       "  'whether',\n",
       "  'wholly',\n",
       "  'why',\n",
       "  'wish',\n",
       "  'wished',\n",
       "  'wishes',\n",
       "  'wishing',\n",
       "  'with',\n",
       "  'withheld',\n",
       "  'withhold',\n",
       "  'without',\n",
       "  'wonder',\n",
       "  'wondered',\n",
       "  'wondering',\n",
       "  'wonders',\n",
       "  'word',\n",
       "  'would',\n",
       "  'wouldnt',\n",
       "  \"wouldn't\",\n",
       "  'wouldve',\n",
       "  \"would've\",\n",
       "  'write',\n",
       "  'writing',\n",
       "  'wrote',\n",
       "  'yearn',\n",
       "  'yield'],\n",
       " 'Insight': ['accept',\n",
       "  'accepta',\n",
       "  'accepted',\n",
       "  'accepting',\n",
       "  'accepts',\n",
       "  'acknowledg',\n",
       "  'adjust',\n",
       "  'admit',\n",
       "  'admits',\n",
       "  'admitted',\n",
       "  'admitting',\n",
       "  'afterthought',\n",
       "  'analy',\n",
       "  'answer',\n",
       "  'appreciat',\n",
       "  'assum',\n",
       "  'attent',\n",
       "  'aware',\n",
       "  'became',\n",
       "  'become',\n",
       "  'becomes',\n",
       "  'becoming',\n",
       "  'belief',\n",
       "  'believe',\n",
       "  'believed',\n",
       "  'believes',\n",
       "  'believing',\n",
       "  'categor',\n",
       "  'choice',\n",
       "  'choos',\n",
       "  'clarif',\n",
       "  'closure',\n",
       "  'cohere',\n",
       "  'complex',\n",
       "  'complica',\n",
       "  'compreh',\n",
       "  'concentrat',\n",
       "  'conclud',\n",
       "  'conclus',\n",
       "  'confess',\n",
       "  'conscious',\n",
       "  'consider',\n",
       "  'considered',\n",
       "  'considering',\n",
       "  'considers',\n",
       "  'contemplat',\n",
       "  'correlat',\n",
       "  'curio',\n",
       "  'decid',\n",
       "  'decis',\n",
       "  'deduc',\n",
       "  'define',\n",
       "  'defines',\n",
       "  'defining',\n",
       "  'determina',\n",
       "  'determine',\n",
       "  'determined',\n",
       "  'determines',\n",
       "  'determining',\n",
       "  'differentiat',\n",
       "  'discern',\n",
       "  'disclo',\n",
       "  'discover',\n",
       "  'distinguish',\n",
       "  'effect',\n",
       "  'enlighten',\n",
       "  'evaluat',\n",
       "  'examin',\n",
       "  'explain',\n",
       "  'explained',\n",
       "  'explaining',\n",
       "  'explains',\n",
       "  'explanat',\n",
       "  'explor',\n",
       "  'feel',\n",
       "  'feeling',\n",
       "  'feels',\n",
       "  'felt',\n",
       "  'figur',\n",
       "  'find',\n",
       "  'forgave',\n",
       "  'forgiv',\n",
       "  'found',\n",
       "  'grasp',\n",
       "  'idea',\n",
       "  'ideas',\n",
       "  'identif',\n",
       "  'imagin',\n",
       "  'induc',\n",
       "  'infer',\n",
       "  'inferr',\n",
       "  'infers',\n",
       "  'info',\n",
       "  'inform',\n",
       "  'information',\n",
       "  'informative',\n",
       "  'informed',\n",
       "  'informing',\n",
       "  'informs',\n",
       "  'inquir',\n",
       "  'insight',\n",
       "  'inspir',\n",
       "  'interpret',\n",
       "  'justif',\n",
       "  'knew',\n",
       "  'know',\n",
       "  'knowab',\n",
       "  'knower',\n",
       "  'knowing',\n",
       "  'knowledg',\n",
       "  'known',\n",
       "  'knows',\n",
       "  'learn',\n",
       "  'lesson',\n",
       "  'link',\n",
       "  'logic',\n",
       "  'mean',\n",
       "  'meaning',\n",
       "  'means',\n",
       "  'meant',\n",
       "  'memor',\n",
       "  'misunder',\n",
       "  'motiv',\n",
       "  'notice',\n",
       "  'noticing',\n",
       "  'perceiv',\n",
       "  'percept',\n",
       "  'ponder',\n",
       "  'prefer',\n",
       "  'presum',\n",
       "  'prove',\n",
       "  'proving',\n",
       "  'quer',\n",
       "  'question',\n",
       "  'rational',\n",
       "  'realiz',\n",
       "  'rearrang',\n",
       "  'reason',\n",
       "  'recall',\n",
       "  'reckon',\n",
       "  'recogni',\n",
       "  'recollect',\n",
       "  'reconcil',\n",
       "  'reconsider',\n",
       "  'reconstruct',\n",
       "  'reevaluat',\n",
       "  'refer',\n",
       "  'reflect',\n",
       "  'relate',\n",
       "  'relating',\n",
       "  'relation',\n",
       "  'rememb',\n",
       "  'reorgani',\n",
       "  'resolu',\n",
       "  'resolv',\n",
       "  'restructur',\n",
       "  'rethink',\n",
       "  'reveal',\n",
       "  'revelat',\n",
       "  'secret',\n",
       "  'secrets',\n",
       "  'seem',\n",
       "  'seemed',\n",
       "  'seeming',\n",
       "  'seems',\n",
       "  'sense',\n",
       "  'sensed',\n",
       "  'senses',\n",
       "  'sensing',\n",
       "  'solution',\n",
       "  'solve',\n",
       "  'solved ',\n",
       "  'solves',\n",
       "  'solving',\n",
       "  'statement',\n",
       "  'suspect',\n",
       "  'suspicio',\n",
       "  'think',\n",
       "  'thinker',\n",
       "  'thinking',\n",
       "  'thinks',\n",
       "  'thought',\n",
       "  'thoughts',\n",
       "  'unaccept',\n",
       "  'unaware',\n",
       "  'understand',\n",
       "  'understandab',\n",
       "  'understanding',\n",
       "  'understands',\n",
       "  'understood',\n",
       "  'unrelat',\n",
       "  'wonder',\n",
       "  'wondered',\n",
       "  'wondering',\n",
       "  'wonders'],\n",
       " 'Discrep': ['besides',\n",
       "  'could',\n",
       "  'couldnt',\n",
       "  \"couldn't\",\n",
       "  'couldve',\n",
       "  \"could've\",\n",
       "  'desir',\n",
       "  'expect',\n",
       "  'hope',\n",
       "  'hoped',\n",
       "  'hopeful',\n",
       "  'hopefully',\n",
       "  'hopefulness',\n",
       "  'hopes',\n",
       "  'hoping',\n",
       "  'ideal',\n",
       "  'if',\n",
       "  'impossib',\n",
       "  'inadequa',\n",
       "  'lack',\n",
       "  'liabilit',\n",
       "  'mistak',\n",
       "  'must',\n",
       "  'mustnt',\n",
       "  \"must'nt\",\n",
       "  \"mustn't\",\n",
       "  'mustve',\n",
       "  \"must've\",\n",
       "  'need',\n",
       "  'needed',\n",
       "  'needing',\n",
       "  'neednt',\n",
       "  \"need'nt\",\n",
       "  \"needn't\",\n",
       "  'needs',\n",
       "  'normal',\n",
       "  'ought',\n",
       "  'oughta',\n",
       "  'oughtnt',\n",
       "  \"ought'nt\",\n",
       "  \"oughtn't\",\n",
       "  'oughtve',\n",
       "  \"ought've\",\n",
       "  'outstanding',\n",
       "  'prefer',\n",
       "  'problem',\n",
       "  'rather',\n",
       "  'regardless',\n",
       "  'regret',\n",
       "  'should',\n",
       "  'shouldnt',\n",
       "  \"should'nt\",\n",
       "  \"shouldn't\",\n",
       "  'shoulds',\n",
       "  'shouldve',\n",
       "  \"should've\",\n",
       "  'undesire',\n",
       "  'undo',\n",
       "  'unneccess',\n",
       "  'unneed',\n",
       "  'unwant',\n",
       "  'wanna',\n",
       "  'want',\n",
       "  'wanted',\n",
       "  'wanting',\n",
       "  'wants',\n",
       "  'wish',\n",
       "  'wished',\n",
       "  'wishes',\n",
       "  'wishing',\n",
       "  'would',\n",
       "  'wouldnt',\n",
       "  \"wouldn't\",\n",
       "  'wouldve',\n",
       "  \"would've\",\n",
       "  'yearn'],\n",
       " 'Tentat': ['allot',\n",
       "  'almost',\n",
       "  'alot',\n",
       "  'ambigu',\n",
       "  'any',\n",
       "  'anybod',\n",
       "  'anyhow',\n",
       "  'anyone',\n",
       "  'anything',\n",
       "  'anytime',\n",
       "  'anywhere',\n",
       "  'apparently',\n",
       "  'appear',\n",
       "  'appeared',\n",
       "  'appearing',\n",
       "  'appears',\n",
       "  'approximat',\n",
       "  'arbitrar',\n",
       "  'assum',\n",
       "  'barely',\n",
       "  'bet',\n",
       "  'bets',\n",
       "  'betting',\n",
       "  'blur',\n",
       "  'borderline',\n",
       "  'chance',\n",
       "  'confus',\n",
       "  'contingen',\n",
       "  'depend',\n",
       "  'depended',\n",
       "  'depending',\n",
       "  'depends',\n",
       "  'disorient',\n",
       "  'doubt',\n",
       "  'dubious',\n",
       "  'dunno',\n",
       "  'fairly',\n",
       "  'fuzz',\n",
       "  'generally',\n",
       "  'guess',\n",
       "  'guessed',\n",
       "  'guesses',\n",
       "  'guessing',\n",
       "  'halfass',\n",
       "  'hardly',\n",
       "  'hazie',\n",
       "  'hazy',\n",
       "  'hesita',\n",
       "  'hope',\n",
       "  'hoped',\n",
       "  'hopeful',\n",
       "  'hopefully',\n",
       "  'hopefulness',\n",
       "  'hopes',\n",
       "  'hoping',\n",
       "  'hypothes',\n",
       "  'hypothetic',\n",
       "  'if',\n",
       "  'incomplet',\n",
       "  'indecis',\n",
       "  'indefinit',\n",
       "  'indetermin',\n",
       "  'indirect',\n",
       "  'kind (of)',\n",
       "  'kinda',\n",
       "  'kindof',\n",
       "  'likel',\n",
       "  'lot',\n",
       "  'lotof',\n",
       "  'lots ',\n",
       "  'lotsa',\n",
       "  'lotta',\n",
       "  'luck',\n",
       "  'lucked',\n",
       "  'lucki',\n",
       "  'luckless',\n",
       "  'lucks',\n",
       "  'lucky',\n",
       "  'mainly',\n",
       "  'marginal',\n",
       "  'may',\n",
       "  'maybe',\n",
       "  'might',\n",
       "  'mightve',\n",
       "  \"might've\",\n",
       "  'most',\n",
       "  'mostly',\n",
       "  'myster',\n",
       "  'nearly',\n",
       "  'obscur',\n",
       "  'occasional',\n",
       "  'often',\n",
       "  'opinion',\n",
       "  'option',\n",
       "  'or',\n",
       "  'overall',\n",
       "  'partly',\n",
       "  'perhaps',\n",
       "  'possib',\n",
       "  'practically',\n",
       "  'pretty',\n",
       "  'probable',\n",
       "  'probablistic',\n",
       "  'probably',\n",
       "  'puzzl',\n",
       "  'question',\n",
       "  'quite',\n",
       "  'random',\n",
       "  'seem',\n",
       "  'seemed',\n",
       "  'seeming',\n",
       "  'seems',\n",
       "  'shaki',\n",
       "  'shaky',\n",
       "  'some',\n",
       "  'somebod',\n",
       "  'somehow',\n",
       "  'someone',\n",
       "  'something',\n",
       "  'sometime',\n",
       "  'sometimes',\n",
       "  'somewhat',\n",
       "  'sort',\n",
       "  'sorta',\n",
       "  'sortof',\n",
       "  'sorts',\n",
       "  'sortsa',\n",
       "  'spose',\n",
       "  'suppose',\n",
       "  'supposed',\n",
       "  'supposes',\n",
       "  'supposing',\n",
       "  'supposition',\n",
       "  'tempora',\n",
       "  'tentativ',\n",
       "  'theor',\n",
       "  'typically',\n",
       "  'uncertain',\n",
       "  'unclear',\n",
       "  'undecided',\n",
       "  'undetermin',\n",
       "  'unknow',\n",
       "  'unlikel',\n",
       "  'unluck',\n",
       "  'unresolv',\n",
       "  'unsettl',\n",
       "  'unsure',\n",
       "  'usually',\n",
       "  'vague',\n",
       "  'variab',\n",
       "  'varies',\n",
       "  'vary',\n",
       "  'wonder',\n",
       "  'wondered',\n",
       "  'wondering',\n",
       "  'wonders'],\n",
       " 'Certain': ['absolute',\n",
       "  'absolutely',\n",
       "  'accura',\n",
       "  'all',\n",
       "  'altogether',\n",
       "  'always',\n",
       "  'apparent',\n",
       "  'assur',\n",
       "  'blatant',\n",
       "  'certain',\n",
       "  'clear',\n",
       "  'clearly',\n",
       "  'commit',\n",
       "  'commitment',\n",
       "  'commits',\n",
       "  'committ',\n",
       "  'complete',\n",
       "  'completed',\n",
       "  'completely',\n",
       "  'completes',\n",
       "  'confidence',\n",
       "  'confident',\n",
       "  'confidently',\n",
       "  'correct',\n",
       "  'defined',\n",
       "  'definite',\n",
       "  'definitely',\n",
       "  'definitive',\n",
       "  'directly',\n",
       "  'distinct',\n",
       "  'entire',\n",
       "  'essential',\n",
       "  'ever',\n",
       "  'every',\n",
       "  'everybod',\n",
       "  'everything',\n",
       "  'evident',\n",
       "  'exact',\n",
       "  'explicit',\n",
       "  'extremely',\n",
       "  'fact',\n",
       "  'facts',\n",
       "  'factual',\n",
       "  'forever',\n",
       "  'frankly',\n",
       "  'fundamental',\n",
       "  'fundamentalis',\n",
       "  'fundamentally',\n",
       "  'fundamentals',\n",
       "  'guarant',\n",
       "  'implicit',\n",
       "  'indeed',\n",
       "  'inevitab',\n",
       "  'infallib',\n",
       "  'invariab',\n",
       "  'irrefu',\n",
       "  'must',\n",
       "  'mustnt',\n",
       "  \"must'nt\",\n",
       "  \"mustn't\",\n",
       "  'mustve',\n",
       "  \"must've\",\n",
       "  'necessar',\n",
       "  'never',\n",
       "  'obvious',\n",
       "  'perfect',\n",
       "  'positiv',\n",
       "  'precis',\n",
       "  'proof',\n",
       "  'prove',\n",
       "  'pure',\n",
       "  'sure',\n",
       "  'total',\n",
       "  'totally',\n",
       "  'true',\n",
       "  'truest',\n",
       "  'truly',\n",
       "  'truth',\n",
       "  'unambigu',\n",
       "  'undeniab',\n",
       "  'undoubt',\n",
       "  'unquestion',\n",
       "  'wholly'],\n",
       " 'Inhib': ['abandon',\n",
       "  'abstain',\n",
       "  'anal',\n",
       "  'avert',\n",
       "  'avoid',\n",
       "  'ban',\n",
       "  'banned',\n",
       "  'banning',\n",
       "  'bans',\n",
       "  'barrier',\n",
       "  'binding',\n",
       "  'block',\n",
       "  'blocked',\n",
       "  'blocker',\n",
       "  'blocking',\n",
       "  'blocks',\n",
       "  'bound',\n",
       "  'brake',\n",
       "  'bridle',\n",
       "  'careful',\n",
       "  'caut',\n",
       "  'cease',\n",
       "  'ceasing',\n",
       "  'compulsiv',\n",
       "  'confin',\n",
       "  'conflict',\n",
       "  'conserv',\n",
       "  'constrain',\n",
       "  'constrict',\n",
       "  'contain',\n",
       "  'contradic',\n",
       "  'control',\n",
       "  'curb',\n",
       "  'curtail',\n",
       "  'defenc',\n",
       "  'defens',\n",
       "  'delay',\n",
       "  'denia',\n",
       "  'denie',\n",
       "  'deny',\n",
       "  'disciplin',\n",
       "  'discourag',\n",
       "  'disregard',\n",
       "  'duti',\n",
       "  'duty',\n",
       "  'enclos',\n",
       "  'fenc',\n",
       "  'forbid',\n",
       "  'forgot',\n",
       "  'guard',\n",
       "  'halt',\n",
       "  'hangup',\n",
       "  'harness',\n",
       "  'held',\n",
       "  'hesita',\n",
       "  'hold',\n",
       "  'ignor',\n",
       "  'inhib',\n",
       "  'interfer',\n",
       "  'keep',\n",
       "  'keeping',\n",
       "  'keeps',\n",
       "  'kept',\n",
       "  'limit',\n",
       "  'neglect',\n",
       "  'obstac',\n",
       "  'oppos',\n",
       "  'prevent',\n",
       "  'prohib',\n",
       "  'protect',\n",
       "  'prude',\n",
       "  'prudes',\n",
       "  'prudish',\n",
       "  'refrain',\n",
       "  'refus',\n",
       "  'rein',\n",
       "  'reluctan',\n",
       "  'repress',\n",
       "  'requir',\n",
       "  'reserved',\n",
       "  'responsib',\n",
       "  'restrain',\n",
       "  'restrict'],\n",
       " 'Incl': ['add',\n",
       "  'addit',\n",
       "  'along',\n",
       "  'and',\n",
       "  'around',\n",
       "  'both',\n",
       "  'came',\n",
       "  'close',\n",
       "  'come',\n",
       "  'each',\n",
       "  'inclu',\n",
       "  'inside',\n",
       "  'into',\n",
       "  'open',\n",
       "  'out',\n",
       "  'plus',\n",
       "  'we',\n",
       "  'with'],\n",
       " 'Excl': ['but',\n",
       "  'either',\n",
       "  'except',\n",
       "  'exclu',\n",
       "  'if',\n",
       "  'just',\n",
       "  'not',\n",
       "  'or',\n",
       "  'rather',\n",
       "  'really',\n",
       "  'something',\n",
       "  'sometime',\n",
       "  'unless',\n",
       "  'versus',\n",
       "  'vs',\n",
       "  'whether',\n",
       "  'without'],\n",
       " 'Bio': ['abdomen',\n",
       "  'abortion',\n",
       "  'abs',\n",
       "  'ache',\n",
       "  'aching',\n",
       "  'acne',\n",
       "  'addict',\n",
       "  'advil',\n",
       "  'aids',\n",
       "  'alcohol',\n",
       "  'alive',\n",
       "  'allerg',\n",
       "  'amput',\n",
       "  'anal',\n",
       "  'ankle',\n",
       "  'anorexi',\n",
       "  'antacid',\n",
       "  'antidepressant',\n",
       "  'anus',\n",
       "  'appendic',\n",
       "  'appendix',\n",
       "  'appeti',\n",
       "  'arch',\n",
       "  'arm',\n",
       "  'armpit',\n",
       "  'arms',\n",
       "  'arous',\n",
       "  'arse',\n",
       "  'arses',\n",
       "  'arter',\n",
       "  'arthr',\n",
       "  'asleep',\n",
       "  'aspirin',\n",
       "  'ass',\n",
       "  'asses',\n",
       "  'asthma',\n",
       "  'ate',\n",
       "  'bake',\n",
       "  'baking',\n",
       "  'bald',\n",
       "  'bandage',\n",
       "  'bandaid',\n",
       "  'bar',\n",
       "  'bars',\n",
       "  'beer',\n",
       "  'bellies',\n",
       "  'belly',\n",
       "  'bi',\n",
       "  'bicep',\n",
       "  'binge',\n",
       "  'binging',\n",
       "  'bipolar',\n",
       "  'bladder',\n",
       "  'bleed',\n",
       "  'blind',\n",
       "  'blood',\n",
       "  'bloody',\n",
       "  'bodi',\n",
       "  'body',\n",
       "  'boil',\n",
       "  'bone',\n",
       "  'boner',\n",
       "  'bones',\n",
       "  'bony',\n",
       "  'boob',\n",
       "  'booz',\n",
       "  'bowel',\n",
       "  'brain',\n",
       "  'bread',\n",
       "  'breakfast',\n",
       "  'breast',\n",
       "  'breath',\n",
       "  'bronchi',\n",
       "  'brunch',\n",
       "  'bulimi',\n",
       "  'burp',\n",
       "  'butt',\n",
       "  'butts',\n",
       "  \"butt's\",\n",
       "  'cafeteria',\n",
       "  'cancer',\n",
       "  'candie',\n",
       "  'candy',\n",
       "  'cardia',\n",
       "  'cardio',\n",
       "  'checkup',\n",
       "  'cheek',\n",
       "  'chest',\n",
       "  'chew',\n",
       "  'chills',\n",
       "  'chiropract',\n",
       "  'chlamydia',\n",
       "  'chok',\n",
       "  'cholester',\n",
       "  'chow',\n",
       "  'chronic',\n",
       "  'cigar',\n",
       "  'clinic',\n",
       "  'clothes',\n",
       "  'cock',\n",
       "  'cocks',\n",
       "  'codeine',\n",
       "  'coffee',\n",
       "  'coke',\n",
       "  'colon',\n",
       "  'colono',\n",
       "  'colons',\n",
       "  'coma',\n",
       "  'condom',\n",
       "  'condoms',\n",
       "  'congest',\n",
       "  'constipat',\n",
       "  'contag',\n",
       "  'cook',\n",
       "  'cornea',\n",
       "  'coronar',\n",
       "  'cough',\n",
       "  'cramp',\n",
       "  'crap',\n",
       "  'crotch',\n",
       "  'cuddl',\n",
       "  'cyst',\n",
       "  'deaf',\n",
       "  'decongest',\n",
       "  'dentist',\n",
       "  'derma',\n",
       "  'dessert',\n",
       "  'detox',\n",
       "  'diabet',\n",
       "  'diagnos',\n",
       "  'diarr',\n",
       "  'dick',\n",
       "  'dicks',\n",
       "  'diet',\n",
       "  'digest',\n",
       "  'dine ',\n",
       "  'dined',\n",
       "  'diner',\n",
       "  'diners',\n",
       "  'dines',\n",
       "  'dining',\n",
       "  'dinner',\n",
       "  'disease',\n",
       "  'dish',\n",
       "  'dishes',\n",
       "  'dizz',\n",
       "  'doctor',\n",
       "  'dosage',\n",
       "  'dose',\n",
       "  'dosing',\n",
       "  'dr',\n",
       "  'drank',\n",
       "  'drink',\n",
       "  'drool',\n",
       "  'drows',\n",
       "  'drs',\n",
       "  'drug',\n",
       "  'drunk',\n",
       "  'dx',\n",
       "  'dyke',\n",
       "  'ear',\n",
       "  'ears',\n",
       "  'eat',\n",
       "  'eaten',\n",
       "  'eating',\n",
       "  'eats',\n",
       "  'egg',\n",
       "  'elbow',\n",
       "  'emphysem',\n",
       "  'enema',\n",
       "  'erectile',\n",
       "  'erection',\n",
       "  'erotic',\n",
       "  'espresso',\n",
       "  'estrogen',\n",
       "  'exercis',\n",
       "  'exhaust',\n",
       "  'eye',\n",
       "  'face',\n",
       "  'faces',\n",
       "  'facial',\n",
       "  'faint',\n",
       "  'farsighted',\n",
       "  'fat',\n",
       "  'fatigu',\n",
       "  'fats',\n",
       "  'fatt',\n",
       "  'fed',\n",
       "  'feed',\n",
       "  'feeder',\n",
       "  'feeding',\n",
       "  'feeds',\n",
       "  'feet',\n",
       "  'fever',\n",
       "  'finger',\n",
       "  'flesh',\n",
       "  'flu',\n",
       "  'food',\n",
       "  'foot',\n",
       "  'forearm',\n",
       "  'forehead',\n",
       "  'foreplay',\n",
       "  'fries',\n",
       "  'fruit',\n",
       "  'fry',\n",
       "  'fuck',\n",
       "  'fucked',\n",
       "  'fucker',\n",
       "  'fuckin',\n",
       "  'fucks',\n",
       "  'gay',\n",
       "  'gays',\n",
       "  'genital',\n",
       "  'gland',\n",
       "  'glaucoma',\n",
       "  'glutton',\n",
       "  'gobble',\n",
       "  'gobbling',\n",
       "  'gonorrhea',\n",
       "  'goosebump',\n",
       "  'grocer',\n",
       "  'gulp',\n",
       "  'gums',\n",
       "  'gut',\n",
       "  'guts',\n",
       "  'gynecolog',\n",
       "  'gyno',\n",
       "  'hair',\n",
       "  'hallucinat',\n",
       "  'hamstring',\n",
       "  'hand',\n",
       "  'hands',\n",
       "  'hangover',\n",
       "  'head',\n",
       "  'headache',\n",
       "  'heads',\n",
       "  'heal',\n",
       "  'healed',\n",
       "  'healer',\n",
       "  'healing',\n",
       "  'heals',\n",
       "  'health',\n",
       "  'heart',\n",
       "  'heartburn',\n",
       "  'hearts',\n",
       "  'heel',\n",
       "  'helpings',\n",
       "  'hemor',\n",
       "  'herpes',\n",
       "  'hiccup',\n",
       "  'hip',\n",
       "  'hips',\n",
       "  'hiv',\n",
       "  'ho',\n",
       "  'homo',\n",
       "  'homos',\n",
       "  'homosexual',\n",
       "  'hormone',\n",
       "  'hornie',\n",
       "  'horny',\n",
       "  'hospital',\n",
       "  'hug',\n",
       "  'hugg',\n",
       "  'hugs',\n",
       "  'hump',\n",
       "  'hunger',\n",
       "  'hungover',\n",
       "  'hungr',\n",
       "  'hyperten',\n",
       "  'hypotherm',\n",
       "  'ibuprofen',\n",
       "  'ICU',\n",
       "  'ill',\n",
       "  'illness',\n",
       "  'immun',\n",
       "  'incest',\n",
       "  'indigestion',\n",
       "  'infect',\n",
       "  'inflam',\n",
       "  'ingest',\n",
       "  'injur',\n",
       "  'insomnia',\n",
       "  'insulin',\n",
       "  'intestin',\n",
       "  'intox',\n",
       "  'itch',\n",
       "  'iv',\n",
       "  'jaw',\n",
       "  'jissom',\n",
       "  'jizz',\n",
       "  'joints',\n",
       "  'kidney',\n",
       "  'kiss',\n",
       "  'kitchen',\n",
       "  'knee',\n",
       "  'knuckle',\n",
       "  'leg',\n",
       "  'legs',\n",
       "  'lesbian',\n",
       "  'leuke',\n",
       "  'libid',\n",
       "  'life',\n",
       "  'lip',\n",
       "  'lips',\n",
       "  'liquor',\n",
       "  'liver',\n",
       "  'living',\n",
       "  'love',\n",
       "  'loved',\n",
       "  'lover',\n",
       "  'loves',\n",
       "  'lozenge',\n",
       "  'lump',\n",
       "  'lunch',\n",
       "  'lung',\n",
       "  'lust',\n",
       "  'lymph',\n",
       "  'makeout',\n",
       "  'mammogram',\n",
       "  'manicdep',\n",
       "  'meal',\n",
       "  'medic',\n",
       "  'migrain',\n",
       "  'milk',\n",
       "  'miscar',\n",
       "  'mono',\n",
       "  'mouth',\n",
       "  'mri ',\n",
       "  'mucous',\n",
       "  'muscle',\n",
       "  'muscular',\n",
       "  'myopi',\n",
       "  'naked',\n",
       "  'nasal',\n",
       "  'nause',\n",
       "  'nearsighted',\n",
       "  'neck',\n",
       "  'nerve',\n",
       "  'neural',\n",
       "  'neurolog',\n",
       "  'neuron',\n",
       "  'nipple',\n",
       "  'nose',\n",
       "  'nostril',\n",
       "  'nude',\n",
       "  'nudi',\n",
       "  'numb',\n",
       "  'nurse',\n",
       "  'nutrition',\n",
       "  'obes',\n",
       "  'OCD',\n",
       "  'optometr',\n",
       "  'orgasm',\n",
       "  'orgies',\n",
       "  'orgy',\n",
       "  'orthodon',\n",
       "  'orthoped',\n",
       "  'ovar',\n",
       "  'overate',\n",
       "  'overeat',\n",
       "  'overweight',\n",
       "  'pain',\n",
       "  'pained',\n",
       "  'painf',\n",
       "  'paining',\n",
       "  'painl',\n",
       "  'pains',\n",
       "  'palm',\n",
       "  'palms',\n",
       "  'pap',\n",
       "  'paraly',\n",
       "  'passion',\n",
       "  'pasta',\n",
       "  'patholog',\n",
       "  'pediatr',\n",
       "  'pee',\n",
       "  'pelvi',\n",
       "  'penis',\n",
       "  'perspir',\n",
       "  'perver',\n",
       "  'pharmac',\n",
       "  'phobi',\n",
       "  'physical',\n",
       "  'physician',\n",
       "  'pill',\n",
       "  'pills',\n",
       "  'pimple',\n",
       "  'piss',\n",
       "  'pizza',\n",
       "  'pms',\n",
       "  'podiatr',\n",
       "  'poison',\n",
       "  'poop',\n",
       "  'porn',\n",
       "  'pregnan',\n",
       "  'prescri',\n",
       "  'prick',\n",
       "  'prognos',\n",
       "  'prostat',\n",
       "  'prostitu',\n",
       "  'prozac',\n",
       "  'prude',\n",
       "  'prudes',\n",
       "  'prudish',\n",
       "  'pubic',\n",
       "  'puk',\n",
       "  'pulse',\n",
       "  'puss',\n",
       "  'pussies',\n",
       "  'pussy',\n",
       "  'queas',\n",
       "  'queer',\n",
       "  'rape',\n",
       "  'raping',\n",
       "  'rapist',\n",
       "  'rash',\n",
       "  'rehab',\n",
       "  'restau',\n",
       "  'retina',\n",
       "  'rib',\n",
       "  'ribs',\n",
       "  'ritalin',\n",
       "  'rx',\n",
       "  'salad',\n",
       "  'saliv',\n",
       "  'sandwich',\n",
       "  'scab',\n",
       "  'scalp',\n",
       "  'schizophren',\n",
       "  'scrape',\n",
       "  'screw',\n",
       "  'seduc',\n",
       "  'seizure',\n",
       "  'sensation',\n",
       "  'sensations',\n",
       "  'servings',\n",
       "  'sex',\n",
       "  'shirt',\n",
       "  'shit',\n",
       "  'shoe',\n",
       "  'shoulder',\n",
       "  'sick',\n",
       "  'sickday',\n",
       "  'sicker',\n",
       "  'sickest',\n",
       "  'sickleave',\n",
       "  'sickly',\n",
       "  'sickness',\n",
       "  'sinus',\n",
       "  'skelet',\n",
       "  'skin',\n",
       "  'skinni',\n",
       "  'skinny',\n",
       "  'skull',\n",
       "  'sleep',\n",
       "  'slender',\n",
       "  'slept',\n",
       "  'slut',\n",
       "  'smok',\n",
       "  'snack',\n",
       "  'soda',\n",
       "  'sore',\n",
       "  'spat',\n",
       "  'spinal',\n",
       "  'spine',\n",
       "  'spit',\n",
       "  'spits',\n",
       "  'spitting',\n",
       "  'starve',\n",
       "  'starving',\n",
       "  'std',\n",
       "  'stiff',\n",
       "  'stomach',\n",
       "  'strept',\n",
       "  'stroke',\n",
       "  'stud',\n",
       "  'stuffed',\n",
       "  'sugar',\n",
       "  'sunburn',\n",
       "  'supper',\n",
       "  'surgeon',\n",
       "  'surger',\n",
       "  'swallow',\n",
       "  'sweat',\n",
       "  'swelling',\n",
       "  'swollen',\n",
       "  'symptom',\n",
       "  'syndrome',\n",
       "  'syphili',\n",
       "  'tast',\n",
       "  'tea',\n",
       "  'teeth',\n",
       "  'tender',\n",
       "  'tendon ',\n",
       "  'tendoni',\n",
       "  'tendons',\n",
       "  'testosterone',\n",
       "  'therap',\n",
       "  'thermometer',\n",
       "  'thigh',\n",
       "  'thirst',\n",
       "  'throat',\n",
       "  'throb',\n",
       "  'thyroid',\n",
       "  'tingl',\n",
       "  'tire',\n",
       "  'tiring',\n",
       "  'tit',\n",
       "  'tits',\n",
       "  'titties',\n",
       "  'titty',\n",
       "  'toe',\n",
       "  'toenail',\n",
       "  'toes',\n",
       "  'tongue',\n",
       "  'tonsils',\n",
       "  'tooth',\n",
       "  'tox',\n",
       "  'tricep',\n",
       "  'tumo',\n",
       "  'twitch',\n",
       "  'tylenol',\n",
       "  'ulcer',\n",
       "  'unhealth',\n",
       "  'urin',\n",
       "  'uter',\n",
       "  'vagina',\n",
       "  'vd',\n",
       "  'veget',\n",
       "  'veggie',\n",
       "  'vein',\n",
       "  'vertigo',\n",
       "  'viagra',\n",
       "  'vicodin',\n",
       "  'virgin',\n",
       "  'vitamin',\n",
       "  'vomit',\n",
       "  'waist',\n",
       "  'wake',\n",
       "  'wart',\n",
       "  'warts',\n",
       "  'wash',\n",
       "  'water',\n",
       "  'weak',\n",
       "  'wear',\n",
       "  'weary',\n",
       "  'weigh ',\n",
       "  'weighed',\n",
       "  'weighing',\n",
       "  'weighs',\n",
       "  'weight',\n",
       "  'wheez',\n",
       "  'whiskey',\n",
       "  'whisky',\n",
       "  'whore',\n",
       "  'wine',\n",
       "  'wines',\n",
       "  'withdrawal',\n",
       "  'womb',\n",
       "  'wound',\n",
       "  'wrist',\n",
       "  'xanax',\n",
       "  'xray',\n",
       "  'yawn',\n",
       "  'zit',\n",
       "  'zits',\n",
       "  'zoloft'],\n",
       " 'Body': ['abdomen',\n",
       "  'abs',\n",
       "  'anal',\n",
       "  'ankle',\n",
       "  'anus',\n",
       "  'appendix',\n",
       "  'arch',\n",
       "  'arm',\n",
       "  'armpit',\n",
       "  'arms',\n",
       "  'arous',\n",
       "  'arse',\n",
       "  'arses',\n",
       "  'arter',\n",
       "  'asleep',\n",
       "  'ass',\n",
       "  'asses',\n",
       "  'bald',\n",
       "  'bellies',\n",
       "  'belly',\n",
       "  'bicep',\n",
       "  'bladder',\n",
       "  'blood',\n",
       "  'bloody',\n",
       "  'bodi',\n",
       "  'body',\n",
       "  'bone',\n",
       "  'bones',\n",
       "  'bony',\n",
       "  'boob',\n",
       "  'bowel',\n",
       "  'brain',\n",
       "  'breast',\n",
       "  'breath',\n",
       "  'butt',\n",
       "  'butts',\n",
       "  'cheek',\n",
       "  'chest',\n",
       "  'clothes',\n",
       "  'cock',\n",
       "  'cocks',\n",
       "  'colon',\n",
       "  'colons',\n",
       "  'cornea',\n",
       "  'crap',\n",
       "  'crotch',\n",
       "  'dick',\n",
       "  'dicks',\n",
       "  'drool',\n",
       "  'ear',\n",
       "  'ears',\n",
       "  'elbow',\n",
       "  'erectile',\n",
       "  'erection',\n",
       "  'eye',\n",
       "  'face',\n",
       "  'faces',\n",
       "  'facial',\n",
       "  'fat',\n",
       "  'fatt',\n",
       "  'feet',\n",
       "  'finger',\n",
       "  'flesh',\n",
       "  'foot',\n",
       "  'forearm',\n",
       "  'forehead',\n",
       "  'genital',\n",
       "  'goosebump',\n",
       "  'gums',\n",
       "  'gut',\n",
       "  'guts',\n",
       "  'hair',\n",
       "  'hamstring',\n",
       "  'hand',\n",
       "  'hands',\n",
       "  'head',\n",
       "  'heads',\n",
       "  'heart',\n",
       "  'hearts',\n",
       "  'heel',\n",
       "  'hip',\n",
       "  'hips',\n",
       "  'hornie',\n",
       "  'horny',\n",
       "  'intestin',\n",
       "  'itch',\n",
       "  'jaw',\n",
       "  'joints',\n",
       "  'kidney',\n",
       "  'knee',\n",
       "  'knuckle',\n",
       "  'leg',\n",
       "  'legs',\n",
       "  'lip',\n",
       "  'lips',\n",
       "  'liver',\n",
       "  'lung',\n",
       "  'mouth',\n",
       "  'mucous',\n",
       "  'muscle',\n",
       "  'muscular',\n",
       "  'naked',\n",
       "  'nasal',\n",
       "  'neck',\n",
       "  'nerve',\n",
       "  'neural',\n",
       "  'neuron',\n",
       "  'nipple',\n",
       "  'nose',\n",
       "  'nostril',\n",
       "  'nude',\n",
       "  'nudi',\n",
       "  'orgasm',\n",
       "  'ovar',\n",
       "  'palm',\n",
       "  'palms',\n",
       "  'pee',\n",
       "  'pelvi',\n",
       "  'penis',\n",
       "  'perspir',\n",
       "  'piss',\n",
       "  'poop',\n",
       "  'prick',\n",
       "  'prostat',\n",
       "  'pulse',\n",
       "  'pussies',\n",
       "  'pussy',\n",
       "  'rash',\n",
       "  'retina',\n",
       "  'rib',\n",
       "  'ribs',\n",
       "  'saliv',\n",
       "  'scalp',\n",
       "  'sensation',\n",
       "  'sensations',\n",
       "  'shirt',\n",
       "  'shit',\n",
       "  'shoe',\n",
       "  'shoulder',\n",
       "  'skelet',\n",
       "  'skin',\n",
       "  'skinni',\n",
       "  'skull',\n",
       "  'sleep',\n",
       "  'slender',\n",
       "  'slept',\n",
       "  'spat',\n",
       "  'spinal',\n",
       "  'spine',\n",
       "  'spit',\n",
       "  'spits',\n",
       "  'spitting',\n",
       "  'stomach',\n",
       "  'sweat',\n",
       "  'teeth',\n",
       "  'tendon ',\n",
       "  'tendons',\n",
       "  'thigh',\n",
       "  'thirst',\n",
       "  'throat',\n",
       "  'tit',\n",
       "  'tits',\n",
       "  'titties',\n",
       "  'titty',\n",
       "  'toe',\n",
       "  'toenail',\n",
       "  'toes',\n",
       "  'tongue',\n",
       "  'tonsils',\n",
       "  'tooth',\n",
       "  'tricep',\n",
       "  'urin',\n",
       "  'uter',\n",
       "  'vagina',\n",
       "  'vein',\n",
       "  'waist',\n",
       "  'wake',\n",
       "  'wear',\n",
       "  'womb',\n",
       "  'wrist'],\n",
       " 'Sexual': ['abortion',\n",
       "  'aids',\n",
       "  'arous',\n",
       "  'ass',\n",
       "  'asses',\n",
       "  'bi',\n",
       "  'boner',\n",
       "  'boob',\n",
       "  'breast',\n",
       "  'butt',\n",
       "  'butts',\n",
       "  \"butt's\",\n",
       "  'chlamydia',\n",
       "  'cock',\n",
       "  'cocks',\n",
       "  'condom',\n",
       "  'condoms',\n",
       "  'cuddl',\n",
       "  'dick',\n",
       "  'dicks',\n",
       "  'dyke',\n",
       "  'erectile',\n",
       "  'erection',\n",
       "  'erotic',\n",
       "  'foreplay',\n",
       "  'fuck',\n",
       "  'fucked',\n",
       "  'fucker',\n",
       "  'fuckin',\n",
       "  'fucks',\n",
       "  'gay',\n",
       "  'gays',\n",
       "  'genital',\n",
       "  'gonorrhea',\n",
       "  'hiv',\n",
       "  'ho',\n",
       "  'homo',\n",
       "  'homos',\n",
       "  'homosexual',\n",
       "  'hornie',\n",
       "  'horny',\n",
       "  'hug',\n",
       "  'hugg',\n",
       "  'hugs',\n",
       "  'hump',\n",
       "  'incest',\n",
       "  'jissom',\n",
       "  'jizz',\n",
       "  'kiss',\n",
       "  'lesbian',\n",
       "  'libid',\n",
       "  'love',\n",
       "  'loved',\n",
       "  'lover',\n",
       "  'loves',\n",
       "  'lust',\n",
       "  'makeout',\n",
       "  'naked',\n",
       "  'nipple',\n",
       "  'nude',\n",
       "  'orgasm',\n",
       "  'orgies',\n",
       "  'orgy',\n",
       "  'ovar',\n",
       "  'passion',\n",
       "  'penis',\n",
       "  'perver',\n",
       "  'porn',\n",
       "  'pregnan',\n",
       "  'prostat',\n",
       "  'prostitu',\n",
       "  'prude',\n",
       "  'prudes',\n",
       "  'prudish',\n",
       "  'pubic',\n",
       "  'pussy',\n",
       "  'queer',\n",
       "  'rape',\n",
       "  'raping',\n",
       "  'rapist',\n",
       "  'screw',\n",
       "  'seduc',\n",
       "  'sex',\n",
       "  'slut',\n",
       "  'std',\n",
       "  'stud',\n",
       "  'syphili',\n",
       "  'tit',\n",
       "  'tits',\n",
       "  'titties',\n",
       "  'titty',\n",
       "  'vagina',\n",
       "  'vd',\n",
       "  'virgin',\n",
       "  'whore',\n",
       "  'womb'],\n",
       " 'Time': ['abrupt',\n",
       "  'after',\n",
       "  'afterlife',\n",
       "  'aftermath',\n",
       "  'afternoon',\n",
       "  'afterthought',\n",
       "  'afterward',\n",
       "  'again',\n",
       "  'age',\n",
       "  'aged',\n",
       "  'ages',\n",
       "  'aging',\n",
       "  'ago',\n",
       "  'ahead',\n",
       "  'already',\n",
       "  'always',\n",
       "  'ancient',\n",
       "  'annual',\n",
       "  'anymore',\n",
       "  'anytime',\n",
       "  'april',\n",
       "  'august',\n",
       "  'autumn',\n",
       "  'awhile',\n",
       "  'back',\n",
       "  'before',\n",
       "  'began',\n",
       "  'begin',\n",
       "  'beginn',\n",
       "  'begins',\n",
       "  'begun',\n",
       "  'biannu',\n",
       "  'bimonth',\n",
       "  'birth',\n",
       "  'biweek',\n",
       "  'born',\n",
       "  'busy',\n",
       "  'bye',\n",
       "  'cease',\n",
       "  'ceasing',\n",
       "  'centur',\n",
       "  'childhood',\n",
       "  'christmas',\n",
       "  'clock',\n",
       "  'common',\n",
       "  'constant ',\n",
       "  'constantly',\n",
       "  'continu',\n",
       "  'current',\n",
       "  'cycle',\n",
       "  'dail',\n",
       "  'date',\n",
       "  'day',\n",
       "  'decade',\n",
       "  'decay',\n",
       "  'december',\n",
       "  'delay',\n",
       "  'due',\n",
       "  'during',\n",
       "  'earli',\n",
       "  'early',\n",
       "  'end',\n",
       "  'ended',\n",
       "  'ending',\n",
       "  'ends',\n",
       "  'era',\n",
       "  'etern',\n",
       "  'eve',\n",
       "  'evening',\n",
       "  'event',\n",
       "  'eventually',\n",
       "  'ever',\n",
       "  'everyday',\n",
       "  'fade',\n",
       "  'fading',\n",
       "  'fast',\n",
       "  'faster',\n",
       "  'fastest',\n",
       "  'february',\n",
       "  'final ',\n",
       "  'finally',\n",
       "  'finish',\n",
       "  'first',\n",
       "  'firstly',\n",
       "  'firsts',\n",
       "  'followup',\n",
       "  'forever',\n",
       "  'former',\n",
       "  'forward',\n",
       "  'frequent',\n",
       "  'frequented',\n",
       "  'frequenting',\n",
       "  'frequently',\n",
       "  'frequents',\n",
       "  'Friday',\n",
       "  'futur',\n",
       "  'generation',\n",
       "  'happening',\n",
       "  'histor',\n",
       "  'hour',\n",
       "  'hurrie',\n",
       "  'hurry',\n",
       "  'immediate',\n",
       "  'immediately',\n",
       "  'immediateness',\n",
       "  'immortal',\n",
       "  'inciden',\n",
       "  'infinit',\n",
       "  'initial',\n",
       "  'initiat',\n",
       "  'instan',\n",
       "  'interval',\n",
       "  'january',\n",
       "  'july',\n",
       "  'june',\n",
       "  'last',\n",
       "  'late',\n",
       "  'lately',\n",
       "  'later',\n",
       "  'latest',\n",
       "  'like',\n",
       "  'long',\n",
       "  'longe',\n",
       "  'march',\n",
       "  'meantime',\n",
       "  'meanwhile',\n",
       "  'min',\n",
       "  'minute',\n",
       "  'modern',\n",
       "  'moment',\n",
       "  'monday',\n",
       "  'month',\n",
       "  'morning',\n",
       "  'never',\n",
       "  'new',\n",
       "  'newer',\n",
       "  'newest',\n",
       "  'newly',\n",
       "  'next',\n",
       "  'night',\n",
       "  'nightly',\n",
       "  'nights',\n",
       "  'noon',\n",
       "  'november',\n",
       "  'now',\n",
       "  'occasional',\n",
       "  'oclock',\n",
       "  \"o'clock\",\n",
       "  'october',\n",
       "  'old',\n",
       "  'olden',\n",
       "  'older',\n",
       "  'oldest',\n",
       "  'once',\n",
       "  'origin',\n",
       "  'past',\n",
       "  'period',\n",
       "  'perpetual',\n",
       "  'preced',\n",
       "  'present',\n",
       "  'presently',\n",
       "  'prior',\n",
       "  'proceed',\n",
       "  'quick',\n",
       "  'recency',\n",
       "  'recent',\n",
       "  'recur',\n",
       "  'repeat',\n",
       "  'repetit',\n",
       "  'respectively',\n",
       "  'return',\n",
       "  'rhythm',\n",
       "  'saturday',\n",
       "  'schedul',\n",
       "  'season',\n",
       "  'seconds',\n",
       "  'senior',\n",
       "  'september',\n",
       "  'sequen',\n",
       "  'simultaneous',\n",
       "  'slow',\n",
       "  'sometime ',\n",
       "  'sometimes',\n",
       "  'soon',\n",
       "  'soone',\n",
       "  'sped',\n",
       "  'speed',\n",
       "  'spring',\n",
       "  'start',\n",
       "  'started',\n",
       "  'starter',\n",
       "  'starting',\n",
       "  'starts',\n",
       "  'startup',\n",
       "  'still',\n",
       "  'stop',\n",
       "  'stopped',\n",
       "  'stopper',\n",
       "  'stopping',\n",
       "  'stops',\n",
       "  'subsequen',\n",
       "  'sudden',\n",
       "  'summer',\n",
       "  'sunday',\n",
       "  'synch',\n",
       "  'tempora',\n",
       "  'term ',\n",
       "  'terminat',\n",
       "  'then',\n",
       "  'thursday',\n",
       "  'til',\n",
       "  'till',\n",
       "  'time',\n",
       "  'timing',\n",
       "  'today',\n",
       "  'tomorrow',\n",
       "  'tonight',\n",
       "  'tuesday',\n",
       "  'until',\n",
       "  'updat',\n",
       "  'usual',\n",
       "  'usually',\n",
       "  'wednesday',\n",
       "  'week',\n",
       "  \"week'\",\n",
       "  'weekend',\n",
       "  'weekl',\n",
       "  'weeks',\n",
       "  'when',\n",
       "  'whenever',\n",
       "  'while',\n",
       "  'whilst',\n",
       "  'winter',\n",
       "  'year ',\n",
       "  'yearly',\n",
       "  'years',\n",
       "  'yesterday',\n",
       "  'yet',\n",
       "  'young',\n",
       "  'youth'],\n",
       " 'Achiev': ['abilit',\n",
       "  'able',\n",
       "  'accomplish',\n",
       "  'ace',\n",
       "  'achiev',\n",
       "  'acquir',\n",
       "  'acquisition',\n",
       "  'adequa',\n",
       "  'advanc',\n",
       "  'advantag',\n",
       "  'ahead',\n",
       "  'ambiti',\n",
       "  'approv',\n",
       "  'attain',\n",
       "  'attempt',\n",
       "  'authorit',\n",
       "  'award',\n",
       "  'beat',\n",
       "  'beaten',\n",
       "  'best',\n",
       "  'better',\n",
       "  'bonus',\n",
       "  'burnout',\n",
       "  'capab',\n",
       "  'celebrat',\n",
       "  'challeng',\n",
       "  'champ',\n",
       "  'climb',\n",
       "  'closure',\n",
       "  'compet',\n",
       "  'conclud',\n",
       "  'conclus',\n",
       "  'confidence',\n",
       "  'confident',\n",
       "  'confidently',\n",
       "  'conquer',\n",
       "  'conscientious',\n",
       "  'control',\n",
       "  'create',\n",
       "  'creati',\n",
       "  'crown',\n",
       "  'defeat',\n",
       "  'determina',\n",
       "  'determined',\n",
       "  'diligen',\n",
       "  'domina',\n",
       "  'domote',\n",
       "  'driven',\n",
       "  'dropout',\n",
       "  'earn',\n",
       "  'effect',\n",
       "  'efficien',\n",
       "  'effort',\n",
       "  'elit',\n",
       "  'enabl',\n",
       "  'endeav',\n",
       "  'excel',\n",
       "  'fail',\n",
       "  'finaliz',\n",
       "  'first',\n",
       "  'firsts',\n",
       "  'founded',\n",
       "  'founder',\n",
       "  'founding',\n",
       "  'fulfill',\n",
       "  'gain',\n",
       "  'goal',\n",
       "  'hero',\n",
       "  'honor',\n",
       "  'honour',\n",
       "  'ideal',\n",
       "  'importan',\n",
       "  'improve',\n",
       "  'improving',\n",
       "  'inadequa',\n",
       "  'incapab',\n",
       "  'incentive',\n",
       "  'incompeten',\n",
       "  'ineffect',\n",
       "  'initiat',\n",
       "  'irresponsible',\n",
       "  'king',\n",
       "  'lazie',\n",
       "  'lazy',\n",
       "  'lead',\n",
       "  'lesson',\n",
       "  'limit',\n",
       "  'lose',\n",
       "  'loser',\n",
       "  'loses',\n",
       "  'losing',\n",
       "  'loss',\n",
       "  'lost',\n",
       "  'master',\n",
       "  'mastered',\n",
       "  'masterful',\n",
       "  'mastering',\n",
       "  'mastermind',\n",
       "  'masters',\n",
       "  'mastery',\n",
       "  'medal',\n",
       "  'mediocr',\n",
       "  'motiv',\n",
       "  'obtain',\n",
       "  'opportun',\n",
       "  'organiz',\n",
       "  'originat',\n",
       "  'outcome',\n",
       "  'overcome',\n",
       "  'overconfiden',\n",
       "  'overtak',\n",
       "  'perfect',\n",
       "  'perform',\n",
       "  'persever',\n",
       "  'persist',\n",
       "  'plan',\n",
       "  'planned',\n",
       "  'planner',\n",
       "  'planning',\n",
       "  'plans',\n",
       "  'potential',\n",
       "  'power',\n",
       "  'practice',\n",
       "  'prais',\n",
       "  'presiden',\n",
       "  'pride',\n",
       "  'prize',\n",
       "  'produc',\n",
       "  'proficien',\n",
       "  'progress',\n",
       "  'promot',\n",
       "  'proud',\n",
       "  'purpose',\n",
       "  'queen',\n",
       "  'queenly',\n",
       "  'quit',\n",
       "  'quitt',\n",
       "  'rank',\n",
       "  'ranked',\n",
       "  'ranking',\n",
       "  'ranks',\n",
       "  'recover',\n",
       "  'requir',\n",
       "  'resolv',\n",
       "  'resourceful',\n",
       "  'responsib',\n",
       "  'reward',\n",
       "  'skill',\n",
       "  'skilled',\n",
       "  'skills',\n",
       "  'solution',\n",
       "  'solve',\n",
       "  'solved',\n",
       "  'solves',\n",
       "  'solving',\n",
       "  'strateg',\n",
       "  'strength',\n",
       "  'striv',\n",
       "  'strong',\n",
       "  'succeed',\n",
       "  'success',\n",
       "  'super',\n",
       "  'superb',\n",
       "  'surviv',\n",
       "  'team',\n",
       "  'top',\n",
       "  'tried',\n",
       "  'tries',\n",
       "  'triumph',\n",
       "  'try',\n",
       "  'trying',\n",
       "  'unable',\n",
       "  'unbeat',\n",
       "  'unproduc',\n",
       "  'unsuccessful',\n",
       "  'victor',\n",
       "  'win',\n",
       "  'winn',\n",
       "  'wins',\n",
       "  'won',\n",
       "  'work ',\n",
       "  'workabl',\n",
       "  'worked',\n",
       "  'worker',\n",
       "  'working',\n",
       "  'works']}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mass_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Semantic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath\n",
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_as_str = defaultdict(str) \n",
    "dicts_as_nlp = defaultdict(lambda: 'initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_key in mass_dict:\n",
    "    for item in mass_dict[col_key]:\n",
    "        dicts_as_str[col_key] = dicts_as_str[col_key] + item + \" \" \n",
    "    dicts_as_nlp[col_key] = nlp(dicts_as_str[col_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts_as_str = the dictionary as a concatenated string\n",
    "# dicts_as_nlp = the nlp(objects) of these concatenated string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        after almost 15 year of dr not be abl to find ...\n",
       "1        i read the other comment and wa so surpris . d...\n",
       "2        with all due respect to the patient who said t...\n",
       "3        dr steiner is a veri knowledg and care doctor ...\n",
       "4        i went to him becaus my husband famili all go ...\n",
       "                               ...                        \n",
       "71724    dr. stone treat both me and my husband . we li...\n",
       "71725    such an amaz doctor ... use to be a professor ...\n",
       "71726    dr. fisher offic staff is veri unfriendli . it...\n",
       "71727    spend the time with me and doe answer my quest...\n",
       "71728    excel . knowledg , deliber , respons , level-h...\n",
       "Name: Review, Length: 71729, dtype: object"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_df_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Quant', 'Numbers', 'Humans', 'Affect', 'Posemo', 'Negemo', 'Cause', 'Health', 'Money', 'Death', 'Ipron', 'aux_verb', 'adverbs', 'Negate', 'Family', 'Anx', 'Anger', 'Sad', 'CogMech', 'Insight', 'Discrep', 'Tentat', 'Certain', 'Inhib', 'Incl', 'Excl', 'Bio', 'Body', 'Sexual', 'Time', 'Achiev'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts_as_nlp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Cosine Similarity Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_cosine_similarity(string_sentence, dict_col, dict_to_compare = dicts_as_nlp):\n",
    "    ind_dict = dict_to_compare[dict_col]\n",
    "    main_doc = nlp(string_sentence.lower())\n",
    "    \n",
    "    all_scores = set()\n",
    "    \n",
    "    for nlp_word in dicts_as_nlp[dict_col]:\n",
    "        all_scores.add(main_doc.similarity(nlp_word))\n",
    "    \n",
    "    return st.median(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_cols_sem_we(string_sentence, dict_to_compare = dicts_as_nlp):\n",
    "    # must be a string sentence! not a tokenized one\n",
    "    # works best with a concatenated string of tokenized words\n",
    "    new_meta_cols = ['Quant', \n",
    "                     'Numbers', \n",
    "                     'Humans', \n",
    "                     'Affect', \n",
    "                     'Posemo', \n",
    "                     'Negemo', \n",
    "                     'Cause', \n",
    "                     'Health', \n",
    "                     'Money', \n",
    "                     'Death', \n",
    "                     'Ipron', \n",
    "                     'aux_verb', \n",
    "                     'adverbs', \n",
    "                     'Negate', \n",
    "                     'Family', \n",
    "                     'Anx', \n",
    "                     'Anger', \n",
    "                     'Sad', \n",
    "                     'CogMech', \n",
    "                     'Insight', \n",
    "                     'Discrep', \n",
    "                     'Tentat', \n",
    "                     'Certain', \n",
    "                     'Inhib', \n",
    "                     'Incl', \n",
    "                     'Excl', \n",
    "                     'Bio', \n",
    "                     'Body', \n",
    "                     'Sexual', \n",
    "                     'Time', \n",
    "                     'Achiev']\n",
    "    blank_row_to_fill = pd.DataFrame(columns=new_meta_cols)\n",
    "    blank_row_to_fill.loc[0] = 0\n",
    "    \n",
    "    blank_row_to_fill['Length'] = len(string_sentence.split())\n",
    "    \n",
    "    string_doc1 = string_sentence.lower()\n",
    "    \n",
    "    for dict_key in dict_to_compare:\n",
    "        score = median_cosine_similarity(string_doc1, dict_key)        \n",
    "        blank_row_to_fill[dict_key] = score\n",
    "\n",
    "    return blank_row_to_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_meta_dfs_sem_we(train_df_tokenized_strings, test_df_tokenized_strings):\n",
    "    # must be tokenized string rows\n",
    "    new_meta_cols = ['Quant', \n",
    "                     'Numbers', \n",
    "                     'Humans', \n",
    "                     'Affect', \n",
    "                     'Posemo', \n",
    "                     'Negemo', \n",
    "                     'Cause', \n",
    "                     'Health', \n",
    "                     'Money', \n",
    "                     'Death', \n",
    "                     'Ipron', \n",
    "                     'aux_verb', \n",
    "                     'adverbs', \n",
    "                     'Negate', \n",
    "                     'Family', \n",
    "                     'Anx', \n",
    "                     'Anger', \n",
    "                     'Sad', \n",
    "                     'CogMech', \n",
    "                     'Insight', \n",
    "                     'Discrep', \n",
    "                     'Tentat', \n",
    "                     'Certain', \n",
    "                     'Inhib', \n",
    "                     'Incl', \n",
    "                     'Excl', \n",
    "                     'Bio', \n",
    "                     'Body', \n",
    "                     'Sexual', \n",
    "                     'Time', \n",
    "                     'Achiev']\n",
    "    \n",
    "    meta_train = pd.DataFrame(columns=new_meta_cols)\n",
    "    meta_test = pd.DataFrame(columns=new_meta_cols)\n",
    "\n",
    "    for row in train_df_tokenized_strings:\n",
    "        meta_train = meta_train.append(create_meta_cols_sem_we(row)).reset_index(drop = True)\n",
    "\n",
    "    for row in test_df_tokenized_strings:\n",
    "        meta_test = meta_test.append(create_meta_cols_sem_we(row)).reset_index(drop = True)\n",
    "        \n",
    "    return meta_train, meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliabarnett/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x_train_meta_data, x_test_meta_data = create_meta_dfs_sem_we(standard_df_str[0], standard_df_str[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab proper ngram data\n",
    "x_train_bi_uni = bi_uni_df[0]\n",
    "y_train_bi_uni = bi_uni_df[1]\n",
    "x_test_bi_uni = bi_uni_df[2]\n",
    "y_test_bi_uni = bi_uni_df[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab proper ngram data\n",
    "x_train_uni = standard_df[0]\n",
    "y_train_uni = standard_df[1]\n",
    "x_test_uni = standard_df[2]\n",
    "y_test_uni = standard_df[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_unibi_train = pd.concat([x_train_meta_data, pd.DataFrame(x_train_bi_uni)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Quant</th>\n",
       "      <th>Numbers</th>\n",
       "      <th>Humans</th>\n",
       "      <th>Affect</th>\n",
       "      <th>Posemo</th>\n",
       "      <th>Negemo</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Health</th>\n",
       "      <th>Money</th>\n",
       "      <th>...</th>\n",
       "      <th>16296</th>\n",
       "      <th>16297</th>\n",
       "      <th>16298</th>\n",
       "      <th>16299</th>\n",
       "      <th>16300</th>\n",
       "      <th>16301</th>\n",
       "      <th>16302</th>\n",
       "      <th>16303</th>\n",
       "      <th>16304</th>\n",
       "      <th>16305</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0.353801</td>\n",
       "      <td>0.337185</td>\n",
       "      <td>0.335885</td>\n",
       "      <td>0.306786</td>\n",
       "      <td>0.323733</td>\n",
       "      <td>0.293186</td>\n",
       "      <td>0.340358</td>\n",
       "      <td>0.216565</td>\n",
       "      <td>0.245024</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.445641</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.403198</td>\n",
       "      <td>0.318483</td>\n",
       "      <td>0.366868</td>\n",
       "      <td>0.291880</td>\n",
       "      <td>0.389588</td>\n",
       "      <td>0.195020</td>\n",
       "      <td>0.303050</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.400896</td>\n",
       "      <td>0.405256</td>\n",
       "      <td>0.334174</td>\n",
       "      <td>0.286815</td>\n",
       "      <td>0.304097</td>\n",
       "      <td>0.271695</td>\n",
       "      <td>0.408599</td>\n",
       "      <td>0.224943</td>\n",
       "      <td>0.270521</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.434727</td>\n",
       "      <td>0.427285</td>\n",
       "      <td>0.365813</td>\n",
       "      <td>0.312239</td>\n",
       "      <td>0.347695</td>\n",
       "      <td>0.294671</td>\n",
       "      <td>0.428450</td>\n",
       "      <td>0.233821</td>\n",
       "      <td>0.300104</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0.380094</td>\n",
       "      <td>0.381828</td>\n",
       "      <td>0.398283</td>\n",
       "      <td>0.322352</td>\n",
       "      <td>0.358448</td>\n",
       "      <td>0.301454</td>\n",
       "      <td>0.365488</td>\n",
       "      <td>0.215303</td>\n",
       "      <td>0.284628</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.066280</td>\n",
       "      <td>-0.107174</td>\n",
       "      <td>-0.056800</td>\n",
       "      <td>0.009591</td>\n",
       "      <td>-0.005325</td>\n",
       "      <td>0.019552</td>\n",
       "      <td>-0.031881</td>\n",
       "      <td>0.036827</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>14</td>\n",
       "      <td>0.439474</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>0.409544</td>\n",
       "      <td>0.326273</td>\n",
       "      <td>0.360026</td>\n",
       "      <td>0.302010</td>\n",
       "      <td>0.413421</td>\n",
       "      <td>0.238387</td>\n",
       "      <td>0.328467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>15</td>\n",
       "      <td>0.395877</td>\n",
       "      <td>0.398751</td>\n",
       "      <td>0.386112</td>\n",
       "      <td>0.309341</td>\n",
       "      <td>0.343397</td>\n",
       "      <td>0.291269</td>\n",
       "      <td>0.350512</td>\n",
       "      <td>0.221610</td>\n",
       "      <td>0.301734</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>2</td>\n",
       "      <td>0.167023</td>\n",
       "      <td>0.118744</td>\n",
       "      <td>0.165098</td>\n",
       "      <td>0.197201</td>\n",
       "      <td>0.225408</td>\n",
       "      <td>0.174447</td>\n",
       "      <td>0.142732</td>\n",
       "      <td>0.132263</td>\n",
       "      <td>0.115279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>24</td>\n",
       "      <td>0.438327</td>\n",
       "      <td>0.431567</td>\n",
       "      <td>0.407259</td>\n",
       "      <td>0.322881</td>\n",
       "      <td>0.377415</td>\n",
       "      <td>0.293954</td>\n",
       "      <td>0.390035</td>\n",
       "      <td>0.240979</td>\n",
       "      <td>0.328833</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 16318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Length     Quant   Numbers    Humans    Affect    Posemo    Negemo  \\\n",
       "0        12  0.353801  0.337185  0.335885  0.306786  0.323733  0.293186   \n",
       "1        13  0.445641  0.472066  0.403198  0.318483  0.366868  0.291880   \n",
       "2         7  0.400896  0.405256  0.334174  0.286815  0.304097  0.271695   \n",
       "3        12  0.434727  0.427285  0.365813  0.312239  0.347695  0.294671   \n",
       "4        18  0.380094  0.381828  0.398283  0.322352  0.358448  0.301454   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "1995      2 -0.066280 -0.107174 -0.056800  0.009591 -0.005325  0.019552   \n",
       "1996     14  0.439474  0.441441  0.409544  0.326273  0.360026  0.302010   \n",
       "1997     15  0.395877  0.398751  0.386112  0.309341  0.343397  0.291269   \n",
       "1998      2  0.167023  0.118744  0.165098  0.197201  0.225408  0.174447   \n",
       "1999     24  0.438327  0.431567  0.407259  0.322881  0.377415  0.293954   \n",
       "\n",
       "         Cause    Health     Money  ...  16296  16297  16298  16299  16300  \\\n",
       "0     0.340358  0.216565  0.245024  ...      0      0      0      0      0   \n",
       "1     0.389588  0.195020  0.303050  ...      0      0      0      0      0   \n",
       "2     0.408599  0.224943  0.270521  ...      0      0      0      0      0   \n",
       "3     0.428450  0.233821  0.300104  ...      0      0      0      0      0   \n",
       "4     0.365488  0.215303  0.284628  ...      0      0      0      0      0   \n",
       "...        ...       ...       ...  ...    ...    ...    ...    ...    ...   \n",
       "1995 -0.031881  0.036827 -0.027848  ...      0      0      0      0      0   \n",
       "1996  0.413421  0.238387  0.328467  ...      0      0      0      0      0   \n",
       "1997  0.350512  0.221610  0.301734  ...      0      0      0      0      0   \n",
       "1998  0.142732  0.132263  0.115279  ...      0      0      0      0      0   \n",
       "1999  0.390035  0.240979  0.328833  ...      0      0      0      0      0   \n",
       "\n",
       "      16301  16302  16303  16304  16305  \n",
       "0         0      0      0      0      0  \n",
       "1         0      0      0      0      0  \n",
       "2         0      0      0      0      0  \n",
       "3         0      0      0      0      0  \n",
       "4         0      0      0      0      0  \n",
       "...     ...    ...    ...    ...    ...  \n",
       "1995      0      0      0      0      0  \n",
       "1996      0      0      0      0      0  \n",
       "1997      0      0      0      0      0  \n",
       "1998      0      0      0      0      0  \n",
       "1999      0      0      0      0      0  \n",
       "\n",
       "[2000 rows x 16318 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_plus_unibi_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_unibi_test = pd.concat([x_test_meta_data, pd.DataFrame(x_test_bi_uni)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Quant</th>\n",
       "      <th>Numbers</th>\n",
       "      <th>Humans</th>\n",
       "      <th>Affect</th>\n",
       "      <th>Posemo</th>\n",
       "      <th>Negemo</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Health</th>\n",
       "      <th>Money</th>\n",
       "      <th>...</th>\n",
       "      <th>16296</th>\n",
       "      <th>16297</th>\n",
       "      <th>16298</th>\n",
       "      <th>16299</th>\n",
       "      <th>16300</th>\n",
       "      <th>16301</th>\n",
       "      <th>16302</th>\n",
       "      <th>16303</th>\n",
       "      <th>16304</th>\n",
       "      <th>16305</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0.438788</td>\n",
       "      <td>0.442058</td>\n",
       "      <td>0.376120</td>\n",
       "      <td>0.323262</td>\n",
       "      <td>0.372743</td>\n",
       "      <td>0.295448</td>\n",
       "      <td>0.408184</td>\n",
       "      <td>0.189394</td>\n",
       "      <td>0.323355</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.359726</td>\n",
       "      <td>0.311153</td>\n",
       "      <td>0.349979</td>\n",
       "      <td>0.299698</td>\n",
       "      <td>0.322610</td>\n",
       "      <td>0.283746</td>\n",
       "      <td>0.362152</td>\n",
       "      <td>0.185080</td>\n",
       "      <td>0.255581</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0.381785</td>\n",
       "      <td>0.395566</td>\n",
       "      <td>0.379186</td>\n",
       "      <td>0.310019</td>\n",
       "      <td>0.363533</td>\n",
       "      <td>0.285346</td>\n",
       "      <td>0.347560</td>\n",
       "      <td>0.196813</td>\n",
       "      <td>0.264847</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>0.429036</td>\n",
       "      <td>0.422716</td>\n",
       "      <td>0.399536</td>\n",
       "      <td>0.317259</td>\n",
       "      <td>0.369049</td>\n",
       "      <td>0.291300</td>\n",
       "      <td>0.405328</td>\n",
       "      <td>0.214971</td>\n",
       "      <td>0.320778</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>0.447681</td>\n",
       "      <td>0.463328</td>\n",
       "      <td>0.394265</td>\n",
       "      <td>0.330171</td>\n",
       "      <td>0.363650</td>\n",
       "      <td>0.315782</td>\n",
       "      <td>0.420971</td>\n",
       "      <td>0.227874</td>\n",
       "      <td>0.312311</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>9</td>\n",
       "      <td>0.399572</td>\n",
       "      <td>0.389455</td>\n",
       "      <td>0.370972</td>\n",
       "      <td>0.296135</td>\n",
       "      <td>0.324928</td>\n",
       "      <td>0.280590</td>\n",
       "      <td>0.354530</td>\n",
       "      <td>0.205950</td>\n",
       "      <td>0.282203</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>9</td>\n",
       "      <td>0.411685</td>\n",
       "      <td>0.391345</td>\n",
       "      <td>0.380572</td>\n",
       "      <td>0.310538</td>\n",
       "      <td>0.340796</td>\n",
       "      <td>0.286594</td>\n",
       "      <td>0.409758</td>\n",
       "      <td>0.211444</td>\n",
       "      <td>0.305251</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>6</td>\n",
       "      <td>0.390539</td>\n",
       "      <td>0.352376</td>\n",
       "      <td>0.343733</td>\n",
       "      <td>0.288593</td>\n",
       "      <td>0.309180</td>\n",
       "      <td>0.267916</td>\n",
       "      <td>0.381026</td>\n",
       "      <td>0.213367</td>\n",
       "      <td>0.286757</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>8</td>\n",
       "      <td>0.376050</td>\n",
       "      <td>0.361656</td>\n",
       "      <td>0.398328</td>\n",
       "      <td>0.326424</td>\n",
       "      <td>0.362487</td>\n",
       "      <td>0.306439</td>\n",
       "      <td>0.392490</td>\n",
       "      <td>0.219705</td>\n",
       "      <td>0.291424</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>11</td>\n",
       "      <td>0.418783</td>\n",
       "      <td>0.389066</td>\n",
       "      <td>0.370773</td>\n",
       "      <td>0.322638</td>\n",
       "      <td>0.361223</td>\n",
       "      <td>0.300260</td>\n",
       "      <td>0.426405</td>\n",
       "      <td>0.220188</td>\n",
       "      <td>0.292126</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 16318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Length     Quant   Numbers    Humans    Affect    Posemo    Negemo  \\\n",
       "0       18  0.438788  0.442058  0.376120  0.323262  0.372743  0.295448   \n",
       "1        9  0.359726  0.311153  0.349979  0.299698  0.322610  0.283746   \n",
       "2       13  0.381785  0.395566  0.379186  0.310019  0.363533  0.285346   \n",
       "3       28  0.429036  0.422716  0.399536  0.317259  0.369049  0.291300   \n",
       "4       32  0.447681  0.463328  0.394265  0.330171  0.363650  0.315782   \n",
       "..     ...       ...       ...       ...       ...       ...       ...   \n",
       "495      9  0.399572  0.389455  0.370972  0.296135  0.324928  0.280590   \n",
       "496      9  0.411685  0.391345  0.380572  0.310538  0.340796  0.286594   \n",
       "497      6  0.390539  0.352376  0.343733  0.288593  0.309180  0.267916   \n",
       "498      8  0.376050  0.361656  0.398328  0.326424  0.362487  0.306439   \n",
       "499     11  0.418783  0.389066  0.370773  0.322638  0.361223  0.300260   \n",
       "\n",
       "        Cause    Health     Money  ...  16296  16297  16298  16299  16300  \\\n",
       "0    0.408184  0.189394  0.323355  ...      0      0      0      0      0   \n",
       "1    0.362152  0.185080  0.255581  ...      0      0      0      0      0   \n",
       "2    0.347560  0.196813  0.264847  ...      0      0      0      0      0   \n",
       "3    0.405328  0.214971  0.320778  ...      0      0      0      0      0   \n",
       "4    0.420971  0.227874  0.312311  ...      0      0      0      0      0   \n",
       "..        ...       ...       ...  ...    ...    ...    ...    ...    ...   \n",
       "495  0.354530  0.205950  0.282203  ...      0      0      0      0      0   \n",
       "496  0.409758  0.211444  0.305251  ...      0      0      0      0      0   \n",
       "497  0.381026  0.213367  0.286757  ...      0      0      0      0      0   \n",
       "498  0.392490  0.219705  0.291424  ...      0      0      0      0      0   \n",
       "499  0.426405  0.220188  0.292126  ...      0      0      0      0      0   \n",
       "\n",
       "     16301  16302  16303  16304  16305  \n",
       "0        0      0      0      0      0  \n",
       "1        0      0      0      0      0  \n",
       "2        0      0      0      0      0  \n",
       "3        0      0      0      0      0  \n",
       "4        0      0      0      0      0  \n",
       "..     ...    ...    ...    ...    ...  \n",
       "495      0      0      0      0      0  \n",
       "496      0      0      0      0      0  \n",
       "497      0      0      0      0      0  \n",
       "498      0      0      0      0      0  \n",
       "499      0      0      0      0      0  \n",
       "\n",
       "[500 rows x 16318 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_plus_unibi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_uni_train = pd.concat([x_train_meta_data, pd.DataFrame(x_train_uni)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_plus_uni_test = pd.concat([x_test_meta_data, pd.DataFrame(x_test_uni)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logreg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegressionCV(cv = 5, max_iter = 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliabarnett/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/juliabarnett/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_train_scaled = min_max_scaler.fit_transform(meta_plus_unibi_train)\n",
    "df_test_scaled = min_max_scaler.fit_transform(meta_plus_unibi_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=10000,\n",
       "           multi_class='warn', n_jobs=None, penalty='l2',\n",
       "           random_state=None, refit=True, scoring=None, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(df_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fit:  0.9815\n",
      "Test fit:  0.792\n",
      "Precision:  0.8360655737704918\n",
      "Recall:  0.5483870967741935\n",
      "F1:  0.6623376623376623\n",
      "F0.5:  0.7566765578635014\n"
     ]
    }
   ],
   "source": [
    "all_eval_metrics(logreg, df_train_scaled, y_train, df_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run models on combined set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "rf_model = RandomForestClassifier(criterion = 'entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliabarnett/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/juliabarnett/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/juliabarnett/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/juliabarnett/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/juliabarnett/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7870376721104506"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-Fold Cross validation\n",
    "np.mean(cross_val_score(rf_model, meta_plus_unibi_train, y_train, cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_param_grid = {\n",
    "                    'bootstrap': [True],\n",
    "                    'criterion': ['entropy'],\n",
    "                    'max_depth': [20, 25],\n",
    "                    'max_features': ['sqrt'],\n",
    "                    'min_samples_leaf': [1, 2, 3],\n",
    "                    'min_samples_split': [3, 5],\n",
    "                    'n_estimators': [1000]\n",
    "                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'bootstrap': [True], 'criterion': ['entropy'], 'max_depth': [20, 25], 'max_features': ['sqrt'], 'min_samples_leaf': [1, 2, 3], 'min_samples_split': [3, 5], 'n_estimators': [1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_model = GridSearchCV(rf_model, intro_param_grid, cv=5)\n",
    "grid_rf_model.fit(meta_plus_unibi_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Quant</th>\n",
       "      <th>Numbers</th>\n",
       "      <th>Humans</th>\n",
       "      <th>Affect</th>\n",
       "      <th>Posemo</th>\n",
       "      <th>Negemo</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Health</th>\n",
       "      <th>Money</th>\n",
       "      <th>...</th>\n",
       "      <th>16296</th>\n",
       "      <th>16297</th>\n",
       "      <th>16298</th>\n",
       "      <th>16299</th>\n",
       "      <th>16300</th>\n",
       "      <th>16301</th>\n",
       "      <th>16302</th>\n",
       "      <th>16303</th>\n",
       "      <th>16304</th>\n",
       "      <th>16305</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0.353801</td>\n",
       "      <td>0.337185</td>\n",
       "      <td>0.335885</td>\n",
       "      <td>0.306786</td>\n",
       "      <td>0.323733</td>\n",
       "      <td>0.293186</td>\n",
       "      <td>0.340358</td>\n",
       "      <td>0.216565</td>\n",
       "      <td>0.245024</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.445641</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.403198</td>\n",
       "      <td>0.318483</td>\n",
       "      <td>0.366868</td>\n",
       "      <td>0.291880</td>\n",
       "      <td>0.389588</td>\n",
       "      <td>0.195020</td>\n",
       "      <td>0.303050</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.400896</td>\n",
       "      <td>0.405256</td>\n",
       "      <td>0.334174</td>\n",
       "      <td>0.286815</td>\n",
       "      <td>0.304097</td>\n",
       "      <td>0.271695</td>\n",
       "      <td>0.408599</td>\n",
       "      <td>0.224943</td>\n",
       "      <td>0.270521</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.434727</td>\n",
       "      <td>0.427285</td>\n",
       "      <td>0.365813</td>\n",
       "      <td>0.312239</td>\n",
       "      <td>0.347695</td>\n",
       "      <td>0.294671</td>\n",
       "      <td>0.428450</td>\n",
       "      <td>0.233821</td>\n",
       "      <td>0.300104</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0.380094</td>\n",
       "      <td>0.381828</td>\n",
       "      <td>0.398283</td>\n",
       "      <td>0.322352</td>\n",
       "      <td>0.358448</td>\n",
       "      <td>0.301454</td>\n",
       "      <td>0.365488</td>\n",
       "      <td>0.215303</td>\n",
       "      <td>0.284628</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.066280</td>\n",
       "      <td>-0.107174</td>\n",
       "      <td>-0.056800</td>\n",
       "      <td>0.009591</td>\n",
       "      <td>-0.005325</td>\n",
       "      <td>0.019552</td>\n",
       "      <td>-0.031881</td>\n",
       "      <td>0.036827</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>14</td>\n",
       "      <td>0.439474</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>0.409544</td>\n",
       "      <td>0.326273</td>\n",
       "      <td>0.360026</td>\n",
       "      <td>0.302010</td>\n",
       "      <td>0.413421</td>\n",
       "      <td>0.238387</td>\n",
       "      <td>0.328467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>15</td>\n",
       "      <td>0.395877</td>\n",
       "      <td>0.398751</td>\n",
       "      <td>0.386112</td>\n",
       "      <td>0.309341</td>\n",
       "      <td>0.343397</td>\n",
       "      <td>0.291269</td>\n",
       "      <td>0.350512</td>\n",
       "      <td>0.221610</td>\n",
       "      <td>0.301734</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>2</td>\n",
       "      <td>0.167023</td>\n",
       "      <td>0.118744</td>\n",
       "      <td>0.165098</td>\n",
       "      <td>0.197201</td>\n",
       "      <td>0.225408</td>\n",
       "      <td>0.174447</td>\n",
       "      <td>0.142732</td>\n",
       "      <td>0.132263</td>\n",
       "      <td>0.115279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>24</td>\n",
       "      <td>0.438327</td>\n",
       "      <td>0.431567</td>\n",
       "      <td>0.407259</td>\n",
       "      <td>0.322881</td>\n",
       "      <td>0.377415</td>\n",
       "      <td>0.293954</td>\n",
       "      <td>0.390035</td>\n",
       "      <td>0.240979</td>\n",
       "      <td>0.328833</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 16318 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Length     Quant   Numbers    Humans    Affect    Posemo    Negemo  \\\n",
       "0        12  0.353801  0.337185  0.335885  0.306786  0.323733  0.293186   \n",
       "1        13  0.445641  0.472066  0.403198  0.318483  0.366868  0.291880   \n",
       "2         7  0.400896  0.405256  0.334174  0.286815  0.304097  0.271695   \n",
       "3        12  0.434727  0.427285  0.365813  0.312239  0.347695  0.294671   \n",
       "4        18  0.380094  0.381828  0.398283  0.322352  0.358448  0.301454   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "1995      2 -0.066280 -0.107174 -0.056800  0.009591 -0.005325  0.019552   \n",
       "1996     14  0.439474  0.441441  0.409544  0.326273  0.360026  0.302010   \n",
       "1997     15  0.395877  0.398751  0.386112  0.309341  0.343397  0.291269   \n",
       "1998      2  0.167023  0.118744  0.165098  0.197201  0.225408  0.174447   \n",
       "1999     24  0.438327  0.431567  0.407259  0.322881  0.377415  0.293954   \n",
       "\n",
       "         Cause    Health     Money  ...  16296  16297  16298  16299  16300  \\\n",
       "0     0.340358  0.216565  0.245024  ...      0      0      0      0      0   \n",
       "1     0.389588  0.195020  0.303050  ...      0      0      0      0      0   \n",
       "2     0.408599  0.224943  0.270521  ...      0      0      0      0      0   \n",
       "3     0.428450  0.233821  0.300104  ...      0      0      0      0      0   \n",
       "4     0.365488  0.215303  0.284628  ...      0      0      0      0      0   \n",
       "...        ...       ...       ...  ...    ...    ...    ...    ...    ...   \n",
       "1995 -0.031881  0.036827 -0.027848  ...      0      0      0      0      0   \n",
       "1996  0.413421  0.238387  0.328467  ...      0      0      0      0      0   \n",
       "1997  0.350512  0.221610  0.301734  ...      0      0      0      0      0   \n",
       "1998  0.142732  0.132263  0.115279  ...      0      0      0      0      0   \n",
       "1999  0.390035  0.240979  0.328833  ...      0      0      0      0      0   \n",
       "\n",
       "      16301  16302  16303  16304  16305  \n",
       "0         0      0      0      0      0  \n",
       "1         0      0      0      0      0  \n",
       "2         0      0      0      0      0  \n",
       "3         0      0      0      0      0  \n",
       "4         0      0      0      0      0  \n",
       "...     ...    ...    ...    ...    ...  \n",
       "1995      0      0      0      0      0  \n",
       "1996      0      0      0      0      0  \n",
       "1997      0      0      0      0      0  \n",
       "1998      0      0      0      0      0  \n",
       "1999      0      0      0      0      0  \n",
       "\n",
       "[2000 rows x 16318 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_plus_unibi_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_rf_model.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_rf_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fit:  0.8075\n",
      "Test fit:  0.756\n",
      "Precision:  0.7622950819672131\n",
      "Recall:  0.5\n",
      "F1:  0.6038961038961039\n",
      "F0.5:  0.6899109792284867\n"
     ]
    }
   ],
   "source": [
    "all_eval_metrics(grid_rf_model, meta_plus_unibi_train, y_train, meta_plus_unibi_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE METRIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_scorer(fbeta_score, beta=0.5)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fone_scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "fone_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_f1 = RandomForestClassifier(criterion = 'entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "                 'n_estimators': [1000, 1100],\n",
    "                 'max_depth': [15, 20, 25],\n",
    "                 'criterion': [\"entropy\"],\n",
    "                 'max_features': ['auto', 'sqrt'],\n",
    "                 'min_samples_split': [2, 3, 4],\n",
    "                 'min_samples_leaf': [1, 2],\n",
    "                 'bootstrap': [True, False]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [1000, 1100], 'max_depth': [15, 20, 25], 'criterion': ['entropy'], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 3, 4], 'min_samples_leaf': [1, 2], 'bootstrap': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(fbeta_score, beta=0.5), verbose=0)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_model_f1 = GridSearchCV(rf_model_f1, param_grid1, cv=5, scoring=fone_scorer)\n",
    "grid_rf_model_f1.fit(meta_plus_uni_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None,\n",
       "            criterion='entropy', max_depth=25, max_features='sqrt',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=None, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_model_f1.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 25,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 1000}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_model_f1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fit:  0.9976507439310885\n",
      "Test fit:  0.8108866442199775\n",
      "Precision:  0.8120300751879699\n",
      "Recall:  0.5806451612903226\n",
      "F1:  0.677115987460815\n",
      "F0.5:  0.7520891364902506\n"
     ]
    }
   ],
   "source": [
    "all_eval_metrics(grid_rf_model_f1, meta_plus_uni_train, y_train, meta_plus_uni_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = grid_rf_model_f1.predict(meta_plus_uni_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 25,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 1000}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_model_f1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'bootstrap': [False],\n",
    " 'criterion': ['entropy'],\n",
    " 'max_depth': [25],\n",
    " 'max_features': ['sqrt'],\n",
    " 'min_samples_leaf': [1],\n",
    " 'min_samples_split': [2],\n",
    " 'n_estimators': [1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_best = RandomForestClassifier(criterion = 'entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'bootstrap': [False], 'criterion': ['entropy'], 'max_depth': [25], 'max_features': ['sqrt'], 'min_samples_leaf': [1], 'min_samples_split': [2], 'n_estimators': [1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(fbeta_score, beta=0.5), verbose=0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_model_best = GridSearchCV(rf_model_best, best_params, cv=5, scoring=fone_scorer)\n",
    "grid_rf_model_best.fit(meta_plus_uni_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Quant</th>\n",
       "      <th>Numbers</th>\n",
       "      <th>Humans</th>\n",
       "      <th>Affect</th>\n",
       "      <th>Posemo</th>\n",
       "      <th>Negemo</th>\n",
       "      <th>Cause</th>\n",
       "      <th>Health</th>\n",
       "      <th>Money</th>\n",
       "      <th>...</th>\n",
       "      <th>2230</th>\n",
       "      <th>2231</th>\n",
       "      <th>2232</th>\n",
       "      <th>2233</th>\n",
       "      <th>2234</th>\n",
       "      <th>2235</th>\n",
       "      <th>2236</th>\n",
       "      <th>2237</th>\n",
       "      <th>2238</th>\n",
       "      <th>2239</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0.353801</td>\n",
       "      <td>0.337185</td>\n",
       "      <td>0.335885</td>\n",
       "      <td>0.306786</td>\n",
       "      <td>0.323733</td>\n",
       "      <td>0.293186</td>\n",
       "      <td>0.340358</td>\n",
       "      <td>0.216565</td>\n",
       "      <td>0.245024</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>0.445641</td>\n",
       "      <td>0.472066</td>\n",
       "      <td>0.403198</td>\n",
       "      <td>0.318483</td>\n",
       "      <td>0.366868</td>\n",
       "      <td>0.291880</td>\n",
       "      <td>0.389588</td>\n",
       "      <td>0.195020</td>\n",
       "      <td>0.303050</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.400896</td>\n",
       "      <td>0.405256</td>\n",
       "      <td>0.334174</td>\n",
       "      <td>0.286815</td>\n",
       "      <td>0.304097</td>\n",
       "      <td>0.271695</td>\n",
       "      <td>0.408599</td>\n",
       "      <td>0.224943</td>\n",
       "      <td>0.270521</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.434727</td>\n",
       "      <td>0.427285</td>\n",
       "      <td>0.365813</td>\n",
       "      <td>0.312239</td>\n",
       "      <td>0.347695</td>\n",
       "      <td>0.294671</td>\n",
       "      <td>0.428450</td>\n",
       "      <td>0.233821</td>\n",
       "      <td>0.300104</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>0.380094</td>\n",
       "      <td>0.381828</td>\n",
       "      <td>0.398283</td>\n",
       "      <td>0.322352</td>\n",
       "      <td>0.358448</td>\n",
       "      <td>0.301454</td>\n",
       "      <td>0.365488</td>\n",
       "      <td>0.215303</td>\n",
       "      <td>0.284628</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.066280</td>\n",
       "      <td>-0.107174</td>\n",
       "      <td>-0.056800</td>\n",
       "      <td>0.009591</td>\n",
       "      <td>-0.005325</td>\n",
       "      <td>0.019552</td>\n",
       "      <td>-0.031881</td>\n",
       "      <td>0.036827</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>14</td>\n",
       "      <td>0.439474</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>0.409544</td>\n",
       "      <td>0.326273</td>\n",
       "      <td>0.360026</td>\n",
       "      <td>0.302010</td>\n",
       "      <td>0.413421</td>\n",
       "      <td>0.238387</td>\n",
       "      <td>0.328467</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>15</td>\n",
       "      <td>0.395877</td>\n",
       "      <td>0.398751</td>\n",
       "      <td>0.386112</td>\n",
       "      <td>0.309341</td>\n",
       "      <td>0.343397</td>\n",
       "      <td>0.291269</td>\n",
       "      <td>0.350512</td>\n",
       "      <td>0.221610</td>\n",
       "      <td>0.301734</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>2</td>\n",
       "      <td>0.167023</td>\n",
       "      <td>0.118744</td>\n",
       "      <td>0.165098</td>\n",
       "      <td>0.197201</td>\n",
       "      <td>0.225408</td>\n",
       "      <td>0.174447</td>\n",
       "      <td>0.142732</td>\n",
       "      <td>0.132263</td>\n",
       "      <td>0.115279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>24</td>\n",
       "      <td>0.438327</td>\n",
       "      <td>0.431567</td>\n",
       "      <td>0.407259</td>\n",
       "      <td>0.322881</td>\n",
       "      <td>0.377415</td>\n",
       "      <td>0.293954</td>\n",
       "      <td>0.390035</td>\n",
       "      <td>0.240979</td>\n",
       "      <td>0.328833</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Length     Quant   Numbers    Humans    Affect    Posemo    Negemo  \\\n",
       "0        12  0.353801  0.337185  0.335885  0.306786  0.323733  0.293186   \n",
       "1        13  0.445641  0.472066  0.403198  0.318483  0.366868  0.291880   \n",
       "2         7  0.400896  0.405256  0.334174  0.286815  0.304097  0.271695   \n",
       "3        12  0.434727  0.427285  0.365813  0.312239  0.347695  0.294671   \n",
       "4        18  0.380094  0.381828  0.398283  0.322352  0.358448  0.301454   \n",
       "...     ...       ...       ...       ...       ...       ...       ...   \n",
       "1995      2 -0.066280 -0.107174 -0.056800  0.009591 -0.005325  0.019552   \n",
       "1996     14  0.439474  0.441441  0.409544  0.326273  0.360026  0.302010   \n",
       "1997     15  0.395877  0.398751  0.386112  0.309341  0.343397  0.291269   \n",
       "1998      2  0.167023  0.118744  0.165098  0.197201  0.225408  0.174447   \n",
       "1999     24  0.438327  0.431567  0.407259  0.322881  0.377415  0.293954   \n",
       "\n",
       "         Cause    Health     Money  ...  2230  2231  2232  2233  2234  2235  \\\n",
       "0     0.340358  0.216565  0.245024  ...     0     0     0     0     0     0   \n",
       "1     0.389588  0.195020  0.303050  ...     0     0     0     0     0     0   \n",
       "2     0.408599  0.224943  0.270521  ...     0     0     0     0     0     0   \n",
       "3     0.428450  0.233821  0.300104  ...     0     0     0     0     0     0   \n",
       "4     0.365488  0.215303  0.284628  ...     0     0     0     0     0     0   \n",
       "...        ...       ...       ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "1995 -0.031881  0.036827 -0.027848  ...     0     0     0     0     0     0   \n",
       "1996  0.413421  0.238387  0.328467  ...     0     0     0     0     0     0   \n",
       "1997  0.350512  0.221610  0.301734  ...     0     0     0     0     0     0   \n",
       "1998  0.142732  0.132263  0.115279  ...     0     0     0     0     0     0   \n",
       "1999  0.390035  0.240979  0.328833  ...     0     0     0     0     0     0   \n",
       "\n",
       "      2236  2237  2238  2239  \n",
       "0        0     0     0     0  \n",
       "1        0     0     0     0  \n",
       "2        0     0     0     0  \n",
       "3        0     0     0     0  \n",
       "4        0     0     0     0  \n",
       "...    ...   ...   ...   ...  \n",
       "1995     0     0     0     0  \n",
       "1996     0     0     0     0  \n",
       "1997     0     0     0     0  \n",
       "1998     0     0     0     0  \n",
       "1999     0     0     0     0  \n",
       "\n",
       "[2000 rows x 2252 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_plus_uni_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fit:  0.9976507439310885\n",
      "Test fit:  0.8044692737430168\n",
      "Precision:  0.8015267175572519\n",
      "Recall:  0.5645161290322581\n",
      "F1:  0.662460567823344\n",
      "F0.5:  0.7394366197183099\n"
     ]
    }
   ],
   "source": [
    "all_eval_metrics(grid_rf_model_best, meta_plus_uni_train, y_train, meta_plus_uni_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_f1_bi = RandomForestClassifier(criterion = 'entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {\n",
    "                 'n_estimators': [1000, 1100],\n",
    "                 'max_depth': [15, 20, 25],\n",
    "                 'criterion': [\"entropy\"],\n",
    "                 'max_features': ['auto', 'sqrt'],\n",
    "                 'min_samples_split': [2, 3, 4],\n",
    "                 'min_samples_leaf': [1, 2],\n",
    "                 'bootstrap': [True, False]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [1000, 1100], 'max_depth': [15, 20, 25], 'criterion': ['entropy'], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 3, 4], 'min_samples_leaf': [1, 2], 'bootstrap': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(fbeta_score, beta=0.5), verbose=0)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_model_f1_bi = GridSearchCV(rf_model_f1_bi, param_grid1, cv=5, scoring=fone_scorer)\n",
    "grid_rf_model_f1_bi.fit(meta_plus_unibi_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, class_weight=None,\n",
       "            criterion='entropy', max_depth=25, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=None, oob_score=False,\n",
       "            random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_model_f1_bi.best_estimator_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 25,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 1000}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_model_f1_bi.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fit:  0.915523465703971\n",
      "Test fit:  0.7860962566844921\n",
      "Precision:  0.8198198198198198\n",
      "Recall:  0.489247311827957\n",
      "F1:  0.6127946127946127\n",
      "F0.5:  0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "all_eval_metrics(grid_rf_model_f1_bi, meta_plus_unibi_train, y_train, meta_plus_unibi_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = grid_rf_model_f1_bi.predict(meta_plus_unibi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_bi = {'bootstrap': [False],\n",
    " 'criterion': ['entropy'],\n",
    " 'max_depth': [25],\n",
    " 'max_features': ['sqrt'],\n",
    " 'min_samples_leaf': [1],\n",
    " 'min_samples_split': [2],\n",
    " 'n_estimators': [1000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_best_bi = RandomForestClassifier(criterion = 'entropy') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'bootstrap': [False], 'criterion': ['entropy'], 'max_depth': [25], 'max_features': ['sqrt'], 'min_samples_leaf': [1], 'min_samples_split': [2], 'n_estimators': [1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(fbeta_score, beta=0.5), verbose=0)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_model_best_bi = GridSearchCV(rf_model_best_bi, best_params_bi, cv=5, scoring=fone_scorer)\n",
    "grid_rf_model_best_bi.fit(meta_plus_unibi_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fit:  0.9976507439310885\n",
      "Test fit:  0.8044692737430168\n",
      "Precision:  0.8015267175572519\n",
      "Recall:  0.5645161290322581\n",
      "F1:  0.662460567823344\n",
      "F0.5:  0.7394366197183099\n"
     ]
    }
   ],
   "source": [
    "all_eval_metrics(grid_rf_model_best, meta_plus_unibi_train, y_train, meta_plus_unibi_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegressionCV(cv = 5, max_iter = 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliabarnett/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/juliabarnett/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df_train_scaled = min_max_scaler.fit_transform(meta_plus_unibi_train)\n",
    "df_test_scaled = min_max_scaler.fit_transform(meta_plus_unibi_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_logr = {    \n",
    "                     'fit_intercept': [True, False],\n",
    "                     'max_iter': [50000],\n",
    "                     'penalty': ['l2'],\n",
    "                     'refit': [True, False                     \n",
    "                     'scoring': [fone_scorer],\n",
    "                     'solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,\n",
       "           fit_intercept=True, intercept_scaling=1.0, max_iter=10000,\n",
       "           multi_class='warn', n_jobs=None, penalty='l2',\n",
       "           random_state=None, refit=True, scoring=None, solver='lbfgs',\n",
       "           tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'fit_intercept': [True, False], 'max_iter': [50000], 'penalty': ['l2'], 'refit': [True, False], 'scoring': [make_scorer(fbeta_score, beta=0.5)], 'solver': ['newton-cg', 'lbfgs', 'sag', 'saga']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(fbeta_score, beta=0.5), verbose=0)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_model_regr = GridSearchCV(logreg, param_grid_logr, cv=5, scoring=fone_scorer)\n",
    "grid_rf_model_regr.fit(df_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fit:  0.9954609485052435\n",
      "Test fit:  0.8333333333333334\n",
      "Precision:  0.8380281690140845\n",
      "Recall:  0.6397849462365591\n",
      "F1:  0.725609756097561\n",
      "F0.5:  0.7891246684350132\n"
     ]
    }
   ],
   "source": [
    "all_eval_metrics(grid_rf_model_regr, df_train_scaled, y_train, df_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_matrix(pred_val, actual_val):\n",
    "    if pred_val == 1:\n",
    "        if actual_val == 1:\n",
    "            return \"TP\"\n",
    "        elif actual_val == 0:\n",
    "            return \"FP\"\n",
    "    elif pred_val == 0:\n",
    "        if actual_val == 1:\n",
    "            return \"FN\"\n",
    "        elif actual_val == 0:\n",
    "            return \"TN\"\n",
    "    else:\n",
    "        return \"ERROR IN DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conf_df(model, x_df, y_df):\n",
    "    compared_df = pd.DataFrame()\n",
    "    compared_df[\"preds\"] = model.predict(x_df)\n",
    "    compared_df[\"actuals\"] = y_df\n",
    "    compared_df[\"error_type\"] = compared_df.apply(lambda x: conf_matrix(x.preds, x.actuals), axis=1)\n",
    "    compared_df[\"count\"] = 1\n",
    "    return compared_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_cost_matrix(model, x_df, y_df):\n",
    "    conf_df = create_conf_df(model, x_df, y_df)\n",
    "    summary_df = conf_df[[\"error_type\", \"count\"]].groupby(by=[\"error_type\"]).sum()\n",
    "    fn = summary_df.iloc[0][0]\n",
    "    fp = summary_df.iloc[1][0]\n",
    "    tn = summary_df.iloc[2][0]\n",
    "    tp = summary_df.iloc[3][0]\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1_score = (2*precision*recall)/(precision+recall)\n",
    "    F0_5score = ((1 + 0.5**2) * precision * recall) / (0.5**2 * precision + recall)\n",
    "    print(summary_df)\n",
    "    print(\"FN:\", fn)\n",
    "    print(\"FP:\", fp)\n",
    "    print(\"TN:\", tn)\n",
    "    print(\"TP:\", tp)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1:\", f1_score)\n",
    "    print(\"F0.5:\", F0_5score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_check(model, x_train, y_train, x_test, y_test):\n",
    "    train_fit = model.score(x_train, y_train)\n",
    "    print(\"Train fit: \", train_fit)\n",
    "    test_fit = model.score(x_test, y_test)\n",
    "    print(\"Test fit: \", test_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fit:  0.9954609485052435\n",
      "Test fit:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "fitness_check(grid_rf_model_regr, df_train_scaled, y_train, df_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            count\n",
      "error_type       \n",
      "FN             23\n",
      "FP             67\n",
      "TN            119\n",
      "TP            291\n",
      "FN: 23\n",
      "FP: 67\n",
      "TN: 119\n",
      "TP: 291\n",
      "Precision: 0.8128491620111732\n",
      "Recall: 0.9267515923566879\n",
      "F1: 0.8660714285714286\n",
      "F0.5: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "summary_cost_matrix(grid_rf_model_regr, df_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
